{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lecture 12: Visual and Large Multimodal Models\n", "\n", "This notebook focuses on how to utilize Visual or Large Multimodal Models (LMM) for some inference tasks and model fine-tuning.\n", "\n", "We primarily use three models: \n", "\n", "1. Stable-diffusion (Separate Images, ComfyUI-tutorial.md and sd-WebUI-tutorial.md )\n", "    A latent text-to-image diffusion model.(https://github.com/CompVis/stable-diffusion)\n", "    2.1 ComfyUI (for Inference)\n", "    2.2 WebUI (for Training)\n", "\n", "2. Segment anything model (SAM 2):  \n", "    SAM2 is a foundation model towards solving promptable visual segmentation in images and videos. \n", "    (https://huggingface.co/facebook/sam2-hiera-large)\n", "    \n", "3. Qwen2-VL:  \n", "    Qwen2-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud. It can answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.\n", "    The model is available on HuggingFace: (https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct) \n", "\n", "\n", "\n", "Note: The weights of these several models in this notebook are sourced from HuggingFace and have already been downloaded to **`/ssdshare/share/lab5`**, you can use them directly. \n", "You can also pre-download the model weights and store them locally on your own. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 1 Image segmentation with SAM-2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Preparing the input/output/configuration directoreis"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["!mkdir -p sam2\n", "!ln -s /ssdshare/share/lab5/sam2/checkpoints/ sam2/checkpoints\n", "\n", "import os\n", "data_path = \"/ssdshare/share/lab5/sam2data\"\n", "image_path = os.path.join(data_path, \"images\")\n", "video_path = os.path.join(data_path, \"videos\")\n", "sam2_path = \"sam2\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Image Segmentation with SAM-2\n", "SAM-2 is a foundation model towards solving promptable visual segmentation in images and videos. In this task, we will use SAM-2 to segment objects in an image and a video."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# configure the GPU (or CPU), here we use the GPU version\n", "\n", "import os\n", "import torch\n", "\n", "\n", "if torch.cuda.is_available():\n", "    device = torch.device(\"cuda\")\n", "else:\n", "    device = torch.device(\"cpu\")\n", "print(f\"using device: {device}\")\n", "\n", "if device.type == \"cuda\":\n", "    # use bfloat16 for the entire notebook\n", "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n", "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n", "    if torch.cuda.get_device_properties(0).major >= 8:\n", "        torch.backends.cuda.matmul.allow_tf32 = True\n", "        torch.backends.cudnn.allow_tf32 = True\n", "\n"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# import SAM and other image processing dependencies\n", "from sam2.build_sam import build_sam2, build_sam2_video_predictor\n", "from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator, SAM2ImagePredictor\n", "import matplotlib.pyplot as plt\n", "\n"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "np.random.seed(3)  # optional, for reproducibility"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# helper functions to display the generated masks\n", "\n", "def show_mask(mask, ax, random_color=False, borders=True, obj_id=None):\n", "    # Display one of the generated masks\n", "    if random_color:\n", "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n", "    elif obj_id is None:\n", "        color = np.array([30/255, 144/255, 255/255, 0.6])\n", "    else:\n", "        cmap = plt.get_cmap(\"tab10\")\n", "        cmap_idx = 0 if obj_id is None else obj_id\n", "        color = np.array([*cmap(cmap_idx)[:3], 0.6])\n", "    h, w = mask.shape[-2:]\n", "    mask = mask.astype(np.uint8)\n", "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n", "    if borders:\n", "        import cv2\n", "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n", "        # Try to smooth contours\n", "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n", "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n", "    ax.imshow(mask_image)\n", "\n", "def show_points(coords, labels, ax, marker_size=375):\n", "    # condition points\n", "    pos_points = coords[labels==1]\n", "    neg_points = coords[labels==0]\n", "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n", "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n", "\n", "def show_box(box, ax):\n", "    # condition box\n", "    x0, y0 = box[0], box[1]\n", "    w, h = box[2] - box[0], box[3] - box[1]\n", "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n", "\n", "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n", "    # sam2 generate several possible masks\n", "    # iterate over masks and display them\n", "    for i, (mask, score) in enumerate(zip(masks, scores)):\n", "        plt.figure(figsize=(10, 10))\n", "        plt.imshow(image)\n", "        show_mask(mask, plt.gca(), borders=borders)\n", "        if point_coords is not None:\n", "            assert input_labels is not None\n", "            show_points(point_coords, input_labels, plt.gca())\n", "        if box_coords is not None:\n", "            # boxes\n", "            show_box(box_coords, plt.gca())\n", "        if len(scores) > 1:\n", "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n", "        plt.axis('off')\n", "        plt.show()"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# here is how to read an input image\n", "from PIL import Image\n", "truck_image = Image.open(os.path.join(image_path, \"truck.jpg\"))\n", "truck_image"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# load configurations of SAM-2\n", "\n", "import hydra  # hydra is a configuration management library\n", "hydra.core.global_hydra.GlobalHydra.instance().clear()\n", "\n", "hydra.initialize(config_path=sam2_path)\n", "\n", "# build sam2 model\n", "sam2_checkpoint = \"sam2/checkpoints/sam2.1_hiera_large.pt\"\n", "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n", "\n", "sam2 = build_sam2(model_cfg,\n", "                    sam2_checkpoint, \n", "                    device=device, \n", "                    apply_postprocessing=False # When set to True, applies post-processing steps to refine masks:\n", "                                              # - Removes small disconnected regions\n", "                                              # - Fills small holes\n", "                                              # - Applies boundary smoothing\n", "                                              # These steps improve mask quality but add computational overhead\n", "                    )\n", "# create automatic mask generator and image predictor\n", "mask_generator = SAM2AutomaticMaskGenerator(\n", "    model=sam2, # the model built above\n", "    points_per_side=64, # Grid size for sampling points\n", "    points_per_batch=128, # Number of points to process at once\n", "    pred_iou_thresh=0.7, # IoU threshold for prediction confidence\n", "    stability_score_thresh=0.92, # Minimum stability score for a mask to be considered\n", "    stability_score_offset=0.7, # Offset for stability score threshold\n", "    crop_n_layers=1, # Number of layers to crop the image\n", "    box_nms_thresh=0.7, # IoU threshold for non-maximum suppression\n", "    crop_n_points_downscale_factor=2, # Downscale factor for cropping points\n", "    min_mask_region_area=25.0,  # Minimum area for a mask to be considered\n", "    use_m2m=True, # Use M2M (Mask-to-Mask) for mask refinement - a technique that refines initial mask predictions by using them as input for subsequent iterations, improving segmentation quality\n", ")\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# generate masks\n", "# the first run loads the model weights, which takes a while\n", "image = np.array(truck_image.convert(\"RGB\"))\n", "masks = mask_generator.generate(image)\n", "plt.figure(figsize=(20, 20))\n", "plt.imshow(image)\n", "for mask in masks:\n", "    show_mask(mask['segmentation'], plt.gca(), random_color=True, borders=False)\n", "plt.axis('off')\n", "plt.show() "]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# check condition point\n", "# 1 in the label means that the point is inside the object\n", "input_point = np.array([[500, 375]]) # locate the point on the object\n", "input_label = np.array([1]) # the point is inside the object\n", "plt.figure(figsize=(10, 10))\n", "plt.imshow(image)\n", "show_points(input_point, input_label, plt.gca())\n", "plt.axis('on')\n", "plt.show()  "]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# step 1: predict masks\n", "predictor = SAM2ImagePredictor(sam2)\n", "predictor.set_image(image)\n", "masks, scores, logits = predictor.predict(\n", "    point_coords=input_point,\n", "    point_labels=input_label,\n", "    multimask_output=True,  # output all the masks\n", ")\n", "# step 2: sort by probability score\n", "sorted_ind = np.argsort(scores)[::-1] # [::-1]reverses the order\n", "masks = masks[sorted_ind]\n", "scores = scores[sorted_ind]\n", "logits = logits[sorted_ind]\n", "\n", "# show all the masks\n", "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# we can use more condition points to specify a more concrete area\n", "input_point = np.array([[500, 375], [1125, 625]])\n", "input_label = np.array([1, 1])\n", "\n", "# \n", "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n", "                                              # :, : means to keep all dimensions in the height and width of the mask\n", "masks, scores, _ = predictor.predict(\n", "    point_coords=input_point,\n", "    point_labels=input_label,\n", "    mask_input=mask_input[None, :, :],  # first dimension is the batch size, here we only have one\n", "    multimask_output=False,\n", ")\n", "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["# red star is negative point, which means the area should not be included in the mask\n", "input_point = np.array([[500, 375], [1125, 625]])\n", "input_label = np.array([1, 0])   ## 0 is a negative point\n", "\n", "mask_input = logits[np.argmax(scores), :, :]  # Choose the model's best mask\n", "masks, scores, _ = predictor.predict(\n", "    point_coords=input_point,\n", "    point_labels=input_label,\n", "    mask_input=mask_input[None, :, :],\n", "    multimask_output=False,\n", ")\n", "show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["# we can use a box to specify an area\n", "input_box = np.array([425, 600, 700, 875]) # a box is a tuple of (x1, y1, x2, y2)\n", "masks, scores, _ = predictor.predict(\n", "    point_coords=None,\n", "    point_labels=None,\n", "    box=input_box[None, :],  # the box\n", "    multimask_output=False,\n", ")\n", "show_masks(image, masks, scores, box_coords=input_box)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# we can combine box and point to specify an area\n", "input_box = np.array([425, 600, 700, 875])\n", "input_point = np.array([[575, 750]])\n", "input_label = np.array([0])\n", "masks, scores, logits = predictor.predict(\n", "    point_coords=input_point,\n", "    point_labels=input_label,\n", "    box=input_box,\n", "    multimask_output=False,\n", ")\n", "show_masks(image, masks, scores, box_coords=input_box, point_coords=input_point, input_labels=input_label)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Choose from *one of* the following two tasks.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# segment all the groceries in the image '/ssdshare/share/lab5/sam2data/images/groceries.jpg'\n", "# the mask should contain all the groceries (in one mask), including the bags and the visible items, but nothing else\n", "# display the segmentation results as musks\n", "# you can either use points or a box to help the segmentation\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# segment out the license plate in the image '/ssdshare/share/lab5/sam2data/images/cars.jpg'\n"]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["# clean up\n", "\n", "del sam2\n", "del predictor\n", "del mask_generator\n", "del logits\n", "del mask_input\n", "for i in range(3):\n", "    # clear cuda memory\n", "    torch.cuda.empty_cache()\n", "    # rubish collection\n", "    import gc\n", "    gc.collect()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. Using SAM2 for Video Segmentation"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# create sam2 video predictor\n", "predictor = build_sam2_video_predictor(model_cfg, sam2_checkpoint, device=device)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Step 0: Extracting all frames of video into images (Already done for you)\n", "\n", "A video is a sequence of images. \n", "\n", "First, we will extract all frames from the video.\n", "\n", "For example, in ./videos folder, we have a video named bedroom.mp4. We can extract the frames from the video using the following command:\n", "```\n", "mkdir bedroom\n", "ffmpeg -i ./videos/bedroom.mp4 -q:v 2 -start_number 0 ./bedroom/'%05d.jpg'\n", "```\n", "where `-q:v` generates high-quality JPEG frames and `-start_number 0` asks ffmpeg to start the JPEG file from `00000.jpg`.\n", "\n", "If you want to try this task with your own video, you can upload it to the ./videos folder and extract the frames using the above command.\n", "\n", "!!!!Before running the above command, make sure you have the ffmpeg package installed. If not, you can install it using the following command:\n", "```\n", "apt install ffmpeg\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then we can examine the extracted images."]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["# `video_dir` a directory of JPEG frames with filenames like `<frame_index>.jpg`\n", "video_dir = os.path.join(video_path, \"bedroom\")\n", "\n", "# scan all the JPEG frame names in this directory\n", "frame_names = [\n", "    p for p in os.listdir(video_dir)\n", "    if os.path.splitext(p)[-1] in [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\"]\n", "]\n", "frame_names.sort(key=lambda p: int(os.path.splitext(p)[0]))\n", "\n", "# take a look the first video frame\n", "frame_idx = 0\n", "plt.figure(figsize=(9, 6))\n", "plt.title(f\"frame {frame_idx}\")\n", "plt.imshow(Image.open(os.path.join(video_dir, frame_names[frame_idx])))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Step 1: Segment out of a single frame"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We show an example of segmenting the boy out of video.  First, let's choose frame 0 and add a first click on it. "]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# SAM 2 requires stateful inference for interactive video segmentation, so we need to initialize an **inference state** on this video.\n", "# During initialization, it loads all the JPEG frames in `video_path` and stores their pixels in `inference_state` (as shown in the progress bar below).\n", "inference_state = predictor.init_state(video_path=video_dir)\n", "\n", "# Chose a frame index to interact with\n", "ann_frame_idx = 0  # frame index\n", "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n", "\n", "# Let's add a positive click at (x, y) = (210, 350) to get started\n", "points = np.array([[210, 350]], dtype=np.float32)  # the boy\n", "labels = np.array([1], np.int32)  # positive click\n", "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n", "    inference_state=inference_state,\n", "    frame_idx=ann_frame_idx,\n", "    obj_id=ann_obj_id,\n", "    points=points,\n", "    labels=labels,\n", ")\n", "\n", "# show the results on the current (interacted) frame\n", "plt.figure(figsize=(9, 6))\n", "plt.title(f\"frame {ann_frame_idx}\")\n", "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n", "show_points(points, labels, plt.gca())\n", "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0], borders=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This not what we want, let's try to choose the entire boy.  We can add a second click to refine the prediction."]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["ann_frame_idx = 0  # the frame index we interact with\n", "ann_obj_id = 1  # give a unique id to each object we interact with (it can be any integers)\n", "\n", "# Let's add a 2nd positive click at (x, y) = (250, 220) to refine the mask\n", "# sending all clicks (and their labels) to `add_new_points_or_box`\n", "points = np.array([[210, 350], [250, 220]], dtype=np.float32)  # a second click on the boy\n", "labels = np.array([1, 1], np.int32)  # both are positive clicks\n", "_, out_obj_ids, out_mask_logits = predictor.add_new_points_or_box(\n", "    inference_state=inference_state,\n", "    frame_idx=ann_frame_idx,\n", "    obj_id=ann_obj_id,\n", "    points=points,\n", "    labels=labels,\n", ")\n", "\n", "# show the results on the current (interacted) frame\n", "plt.figure(figsize=(9, 6))\n", "plt.title(f\"frame {ann_frame_idx}\")\n", "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n", "show_points(points, labels, plt.gca())\n", "show_mask((out_mask_logits[0] > 0.0).cpu().numpy(), plt.gca(), obj_id=out_obj_ids[0], borders=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With this 2nd refinement click, now we get a segmentation mask of the entire child on frame 0.  Now let's expand the result into the entire video."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Step 2: Propagate the prompts to get the masklet across the video"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To get the masklet throughout the entire video, we propagate the prompts using the `propagate_in_video` API."]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# run propagation throughout the video and collect the results in a dict\n", "video_segments = {}  # video_segments contains the per-frame segmentation results\n", "# iterate over the frames of the video\n", "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n", "    video_segments[out_frame_idx] = {\n", "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n", "        for i, out_obj_id in enumerate(out_obj_ids)\n", "    }\n", "\n", "# render the segmentation results every few frames\n", "vis_frame_stride = 30\n", "plt.close(\"all\")\n", "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n", "    plt.figure(figsize=(6, 4))\n", "    plt.title(f\"frame {out_frame_idx}\")\n", "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n", "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n", "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id, borders=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Step 4: Add new prompts to further refine the masklet"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It appears that in the output masklet above, there are some small imperfections in boundary details on frame 150.\n", "\n", "We can tell the model not to include the imperfections by adding a new prompt on frame 150."]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["ann_frame_idx = 150  # further refine some details on this frame\n", "ann_obj_id = 1  # give a unique id to the object we interact with (it can be any integers)\n", "\n", "# show the segment before further refinement\n", "plt.figure(figsize=(9, 6))\n", "plt.title(f\"frame {ann_frame_idx} -- before refinement\")\n", "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n", "show_mask(video_segments[ann_frame_idx][ann_obj_id], plt.gca(), obj_id=ann_obj_id, borders=False)\n", "\n", "# Let's add a negative click on this frame at (x, y) = (82, 415) to refine the segment\n", "points = np.array([[82, 410]], dtype=np.float32)\n", "# for labels, `1` means positive click and `0` means negative click\n", "labels = np.array([0], np.int32)\n", "_, _, out_mask_logits = predictor.add_new_points_or_box(\n", "    inference_state=inference_state,\n", "    frame_idx=ann_frame_idx,\n", "    obj_id=ann_obj_id,\n", "    points=points,\n", "    labels=labels,\n", ")\n", "\n", "# show the segment after the further refinement\n", "plt.figure(figsize=(9, 6))\n", "plt.title(f\"frame {ann_frame_idx} -- after refinement\")\n", "plt.imshow(Image.open(os.path.join(video_dir, frame_names[ann_frame_idx])))\n", "show_points(points, labels, plt.gca())\n", "show_mask((out_mask_logits > 0.0).cpu().numpy(), plt.gca(), obj_id=ann_obj_id, borders=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Step 5: Propagate the prompts (again) to get the masklet across the video"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's get an updated masklet for the entire video. Here we call `propagate_in_video` again to propagate all the prompts after adding the new refinement click above."]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["# run propagation throughout the video and collect the results in a dict\n", "video_segments = {}  # video_segments contains the per-frame segmentation results\n", "for out_frame_idx, out_obj_ids, out_mask_logits in predictor.propagate_in_video(inference_state):\n", "    video_segments[out_frame_idx] = {\n", "        out_obj_id: (out_mask_logits[i] > 0.0).cpu().numpy()\n", "        for i, out_obj_id in enumerate(out_obj_ids)\n", "    }\n", "\n", "# render the segmentation results every few frames\n", "vis_frame_stride = 30\n", "plt.close(\"all\")\n", "for out_frame_idx in range(0, len(frame_names), vis_frame_stride):\n", "    plt.figure(figsize=(6, 4))\n", "    plt.title(f\"frame {out_frame_idx}\")\n", "    plt.imshow(Image.open(os.path.join(video_dir, frame_names[out_frame_idx])))\n", "    for out_obj_id, out_mask in video_segments[out_frame_idx].items():\n", "        show_mask(out_mask, plt.gca(), obj_id=out_obj_id, borders=False)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# segment the `dog` in video file(already processed into jpgs) '/ssdshare/share/lab5/sam2data/videos/outdoor/'\n", "# and show the segmentation results (frame 1, 20, 50, 80, 100, 140)"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["# clean up\n", "\n", "del predictor\n", "del out_mask_logits\n", "del video_segments\n", "del inference_state\n", "del masks\n", "del mask\n", "for i in range(3):\n", "    torch.cuda.empty_cache()\n", "    import gc\n", "    gc.collect()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Part 3. ImageQA with Qwen2-VL\n", "Qwen2-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud. It can answer questions about graphs and diagrams, answer UI-based questions, and do fine-grained localization on screen images.\n", "\n", "Use the pre-downloaded model weights: /ssdshare/share/lab5/Qwen2-VL-7B-Instruct, or make sure you have downloaded the weights of the model from HuggingFace."]}, {"cell_type": "code", "execution_count": 58, "metadata": {}, "outputs": [], "source": ["from PIL import Image\n", "import requests\n", "import torch\n", "from torchvision import io\n", "from typing import Dict\n", "from transformers import Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Load the model in half-precision (bf16) on the available device(s) \n"]}, {"cell_type": "code", "execution_count": 63, "metadata": {}, "outputs": [], "source": ["#model = Qwen2VLForConditionalGeneration.from_pretrained(\n", "#    \"/ssdshare/share/lab5/Qwen2-VL-7B-Instruct\",  # the model weights\n", "#    torch_dtype=torch.bfloat16,  # more about these settings later in this course\n", "#    device_map=\"cuda:0\", # use GPU 0\n", "#    attn_implementation=\"flash_attention_2\" # more about these settings later in this course\n", "#)\n", "processor = AutoProcessor.from_pretrained(\"/ssdshare/share/lab5/Qwen2-VL-7B-Instruct\")"]}, {"cell_type": "code", "execution_count": 64, "metadata": {}, "outputs": [], "source": ["# Check the special tokens\n", "# Prepare for activating the model's ability to localize objects in the image. \n", "#  This is useful to create the prompts in the later steps. \n", "#  (i.e., so you know where to put the tokens representing the image.)\n", "\n", "processor.tokenizer.special_tokens_map"]}, {"cell_type": "code", "execution_count": 74, "metadata": {}, "outputs": [], "source": ["# Clean up\n", "del processor\n", "for i in range(3):\n", "    torch.cuda.empty_cache()\n", "    import gc\n", "    gc.collect()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Wrap the model and processor in a class, so that we can use it more easily\n"]}, {"cell_type": "code", "execution_count": 78, "metadata": {}, "outputs": [], "source": ["class Qwen2VL:\n", "    def __init__(self, model_name: str = \"/ssdshare/share/lab5/Qwen2-VL-7B-Instruct\"):\n", "        if torch.cuda.is_available():\n", "            print(\"You are running the model on GPU.\")\n", "            self.device = torch.device(\"cuda:0\")\n", "        else:\n", "            print(\"You are running the model on CPU.\")\n", "            self.device = torch.device(\"cpu\")\n", "        self.dtype = torch.bfloat16\n", "        print(\"Loading model and processor...\")\n", "        self.model = Qwen2VLForConditionalGeneration.from_pretrained(\n", "            model_name, torch_dtype=self.dtype, device_map=self.device, attn_implementation=\"flash_attention_2\"\n", "        )\n", "        self.processor = AutoProcessor.from_pretrained(model_name)\n", "        print(\"Model and processor loaded.\")\n", "\n", "    def generate(self, text: str, image: Image, max_new_tokens: int = 128):\n", "        # Same as the previous code\n", "        conversation = [\n", "            {\n", "                \"role\": \"user\",\n", "                \"content\": [\n", "                    {\"type\": \"image\",},\n", "                    {\"type\": \"text\", \"text\": text},\n", "                ],\n", "            }\n", "        ]\n", "        text_prompt = self.processor.apply_chat_template(conversation, add_generation_prompt=True)\n", "        inputs = self.processor(\n", "            text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n", "        )\n", "        inputs = inputs.to(self.device)\n", "        output_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens)\n", "        generated_ids = [\n", "            output_ids[len(input_ids) :]\n", "            for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n", "        ]\n", "        output_text = self.processor.batch_decode(\n", "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n", "        )\n", "        return output_text[0]\n", "    \n", "    def grounding(self, text: str, image: Image):\n", "        # text is your description of the object\n", "        # We can't easily ask the model to localize objects through natural language (You can try it by yourself, use the generate method)\n", "        # According to https://arxiv.org/pdf/2409.12191 and the special tokens checked above, we can use the following template\n", "        # <|vision_start|>Picture1.jpg<|vision_end|>\n", "        # <|object_ref_start|>the eyes on a giraffe<|object_ref_end|><|box_start|>(176,106),(232,160)<|box_end|>\n", "        def get_pos_1000(text):\n", "            import re\n", "            # left, top, right, bottom\n", "            return list(map(int, re.findall(r\"\\d+\", text)))\n", "        \n", "        def drawbox_on_image(image_, pos):\n", "            # use PIL to draw box on image\n", "            from PIL import ImageDraw\n", "            image = image_.copy()\n", "            draw = ImageDraw.Draw(image)\n", "            # map pos from 0 ~ 1000 to image size\n", "            pos[0] = pos[0] * image.width // 1000\n", "            pos[1] = pos[1] * image.height // 1000\n", "            pos[2] = pos[2] * image.width // 1000\n", "            pos[3] = pos[3] * image.height // 1000\n", "            draw.rectangle(pos, outline=\"red\", width=3)\n", "            return image\n", "        \n", "        ## the prompt should match the special tokens above\n", "        text_prompt = f\"\"\"<|vision_start|><|image_pad|><|vision_end|>\\n<|object_ref_start|>{text}<|object_ref_end|><|box_start|>\"\"\"\n", "        inputs = self.processor(\n", "            text=[text_prompt], images=[image], padding=True, return_tensors=\"pt\"\n", "        )\n", "        inputs = inputs.to(self.device)\n", "        output_ids = self.model.generate(**inputs, max_new_tokens=64, eos_token_id=self.processor.tokenizer.encode(\"<|box_end|>\"))\n", "        generated_ids = [\n", "            output_ids[len(input_ids) :]\n", "            for input_ids, output_ids in zip(inputs.input_ids, output_ids)\n", "        ]\n", "        output_text = self.processor.batch_decode(\n", "            generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True\n", "        )\n", "        # return output_text[0]\n", "        pos = get_pos_1000(output_text[0])\n", "        return drawbox_on_image(image, pos)\n", "    \n", "    def design(self, text: str, image: Image):\n", "        output_text = self.generate(text, image, max_new_tokens=768)\n", "        def get_html(text):\n", "            import re\n", "            return re.findall(r\"```html(.*?)```\", text, re.DOTALL)\n", "        html = get_html(output_text)[0]\n", "        if len(html) == 0:\n", "            print(\"Invalid output text.\")\n", "            print(output_text)\n", "            return None\n", "        return html\n", "        "]}, {"cell_type": "code", "execution_count": 79, "metadata": {}, "outputs": [], "source": ["llm = Qwen2VL()\n"]}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [], "source": ["import os\n", "imgpath = \"/ssdshare/share/lab5/vqadata\""]}, {"cell_type": "code", "execution_count": 82, "metadata": {}, "outputs": [], "source": ["prompt = 'How many bottles of [Magna] beer are there? Please note that several types of beer might be on the table.'\n", "img = Image.open(os.path.join(imgpath, \"test0.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 83, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 84, "metadata": {}, "outputs": [], "source": ["prompt =' Describe what is Object 1 and object 2. Tell me what is in the circled glass.'\n", "img = Image.open(os.path.join(imgpath, \"test1.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 85, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 86, "metadata": {}, "outputs": [], "source": ["prompt= ' Please read the text in this image and return the information in the following JSON format (note xxx is placeholder, if the information is not available in the image, put \"N/A\" instead). {\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\": xxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx, \"SEX\": xxx, \"HGT\": xxx, \"WGT\": xxx, \"EYES\": xxx, \"HAIR\": xxx, \"DONOR\": xxx}'\n", "img = Image.open(os.path.join(imgpath, \"test2.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 87, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 93, "metadata": {}, "outputs": [], "source": ["prompt ='Count the number of apples in the image.'\n", "img = Image.open(os.path.join(imgpath, \"test3.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 94, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 95, "metadata": {}, "outputs": [], "source": ["prompt = 'Describe the landmark in the image.'\n", "img = Image.open(os.path.join(imgpath, \"test6.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 96, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [], "source": ["prompt = 'Describe the name of the dish.'\n", "img = Image.open(os.path.join(imgpath, \"test7.jpeg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 100, "metadata": {}, "outputs": [], "source": ["prompt = 'What is wrong with the foot in this figure??'\n", "img = Image.open(os.path.join(imgpath, \"test8.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 101, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 102, "metadata": {}, "outputs": [], "source": ["prompt ='What is the spatial relation between the frisbee and the man?'\n", "img = Image.open(os.path.join(imgpath, \"test9.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 103, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 104, "metadata": {}, "outputs": [], "source": ["prompt = 'Which oceans surround Africa?  both to the east and to the west.'\n", "img = Image.open(os.path.join(imgpath, \"test13.jpg\"))\n", "img"]}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [], "source": ["llm.generate(prompt, img)"]}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [], "source": ["grounding_img = Image.open(os.path.join(imgpath, \"grounding.jpg\"))\n", "grounding_img = grounding_img.resize((grounding_img.width // 2, grounding_img.height // 2))\n", "llm.grounding(\"A squirrel\", grounding_img)"]}, {"cell_type": "code", "execution_count": 50, "metadata": {}, "outputs": [], "source": ["llm.grounding(\"Tail of the squirrel\", grounding_img)"]}, {"cell_type": "code", "execution_count": 77, "metadata": {}, "outputs": [], "source": ["# clean up\n", "\n", "del llm\n", "\n", "for i in range(3):\n", "    torch.cuda.empty_cache()\n", "    import gc\n", "    gc.collect()"]}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [], "source": ["### Your Task\n", "# 1. run one different vlm, and ask two questions.\n", "# 2. refer to lab5/3-qwen2vl_api.ipynb, use different vlms to generate text.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Text-to-Image with Stable Diffusion"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 Comfy-UI\n", "\n", "ComfyUI is a powerful modular diffusion model GUI that allows users to easily create and run diffusion models.\n", "\n", "\n", "### Step 1. Running the ComfyUI on the cluster\n", "We have built a Docker image that contains all the necessary dependencies to run ComfyUI. To run ComfyUI, follow the steps below:\n", "\n", "1. Copy and create a new configuation file based on https://github.com/iiisthu/ailab/blob/main/user/comfyui-template.yaml .  You should set your namespace in the configure file, and use harbor-local.ai.iiis.co/llm-course/comfyui:v1 as the Docker image (already set in the template).\n", "\n", "2. Helm install, just as other labs.\n", "\n", "3. In order to access the website your started in the cluster, you should run the following command to forward the port to your local computer.  (you can also do it with the Kubernetes plugin in VSCode).  \n", "```bash\n", "# on your local PC\n", "kubectl port-forward pod/<pod_name> 8188:8188\n", "```\n", "\n", "4. Opne a brower and visit https://127.0.0.1:8188 , you should see your UI.  \n", "\n", "\n", "### Step 2. Playing with the comfy UI.\n", "![ComfyUI Interface](assets/preview.png)\n", "1. Import workflow configuration file. Click the `Workflow` button and select the workflow configuration file stored in **your PC**. We provide some example configs in `LLM-applications-course/lec5/comfy_example_workflows`. You can download to your PC and import them.\n", "![Example Workflow Configuration File](assets/select.png)\n", "\n", "2. Select models. You can enlarge the page to see the workflow more clearly. Here we can see in this workflow, the `Load Checkpoint` module requires the `ckpt_name` should be one of sdv3/2b_1024/sd3_medium.safetensors. For example, we select sd3_medium.safetensors.\n", "\n", "3. Write your text prompt in `CLIP Text Encode` module. `Prompt` is what we want, `Negative Prompt` is what we don't want. \n", "\n", "4. (Optional) Change the `Seed` to `randomize` generate different results.\n", "\n", "5. (Optional) Customize your workflow, e.g., add a lora module.\n", "\n", "6. Click `Queue` to run the workflow.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# write down your prompts here\n", "# and copy the generated image below\n", "# if you have a different workflow from the above (either you write your own, or you find on the Internet)\n", "# please submit it together with this notebook"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.2 Training Stable Diffusion with WebUI\n", "\n", "stable-diffusion-webui is a web interface for Stable Diffusion. Here we provide a tutorial on how to train Stable Diffusion using the web interface.\n", "\n", "#### Step 1. Running the Stable Diffusion WebUI on the cluster\n", "We have built a Docker image that contains all the necessary dependencies to run sd-WebUI. To run sd-WebUI, follow the steps below:\n", "\n", "Follow a similar procedure to start the webUI, using webui-template.yaml. \n", "This time, please forward port 7860 instead. \n", "\n", "You should be able to access the GUI at https://127.0.0.1:7860  (from your pc)\n", "\n", "#### Step 2. Playing with SD-WebUI\n", "Pay attention to all paths in pictures below. If a path starts with \"/share\", you should replace it with \"/ssdshare/share\", e.g., \"/share/lab5/clip-vit-l-14\" -> \"/ssdshare/share/lab5/clip-vit-l-14\".\n", "\n", "We want to teach the model a `concept` of `headless_statue`.\n", "![Headless Statue](assets/sd-hdst.jpeg)\n", "\n", "1. Select a sd model from the dropdown list.\n", "![Select Model](assets/sd-ckpt.png)\n", "\n", "2. Try to generate images using prompt: \"An oil painting of headless_statue\". We are not satisfied with the generated images.\n", "![Generate Images](assets/sd-base.png)\n", "\n", "3. Preprocess our `headless_statue` images. Set parameters following images below, and click `Generate` button.\n", "![Preprocess Images](assets/sd-preprocess.png)\n", "\n", "4. Create embedding for our concept `headless_statue`. \n", "\n", "- Name: filename for the created embedding. You will also use this text in prompts when referring to the embedding.\n", "\n", "- Initialization text: the embedding you create will initially be filled with vectors of this text. If you create a one vector embedding named \"zzzz1234\" with \"tree\" as initialization text, and use it in prompt without training, then prompt \"a zzzz1234 by monet\" will produce same pictures as \"a tree by monet\".\n", "\n", "- Number of vectors per token: the size of embedding. The larger this value, the more information about subject you can fit into the embedding, but also the more words it will take away from your prompt allowance. With stable diffusion, you have a limit of 75 tokens in the prompt. If you use an embedding with 16 vectors in a prompt, that will leave you with space for 75 - 16 = 59. Also from my experience, the larger the number of vectors, the more pictures you need to obtain good results.\n", "\n", "![Create Embedding](assets/sd-create-emb.png)\n", "\n", "5. Train the embedding. Set parameters following images below, and click `Train Embedding` button.\n", "![Train Embedding](assets/sd-train.png)\n", "![Sample](assets/sd-sample.png)\n", "\n", "6. Generate images using prompt: \"An oil painting of `Name`\". We can see the generated images are more related to our concept `headless_statue`.\n", "![Result](assets/sd-result.png)\n", "\n", "\n", "### Textual Inversion\n", "Textual Inversion is a parameter efficient method to train Stable Diffusion. This method can be used to represent a wide array of concepts. Trained on this method, Stable Diffusion can learn a pseudo-word that represents a specific artist or a new concept.\n", "![Textual Inversion](assets/teaser.JPG)\n", "\n", "#### Why we need Textual Inversion Algorithm?\n", "Suppose we have a sd model (which is trained on a specific dataset without the image of `The Thinker`). When we want to use it to generate `A cat in the pose of The Thinker`, we need rewrite our prompt to `A cat with its hand on its chin, sitting on a rock, its eyes looking down thoughtfully`. This is because the model doesn't know what the exact pose of `The Thinker` is. Textual Inversion can help us to find the pseudo-word that represents the concept of `The Thinker` with 3-5 `The Thinker` images.\n", "\n", "#### How does it work?\n", "The essence of Textual Inversion is to map the object in the image to a pseudo-word(A high dimension vector actually. Not necessarily a natural language word, we barely use natural language word to tag it)\n", "![Principal](assets/training.JPG)\n", "\n", "#### Why not train embedding directly?\n", "The scale of the embedding is $\\frac{vocab\\_ size}{token\\_ vectors\\_ num}$ times of the pseudo-word embedding in Textual Inversion, which requires far more data to train (Where vocab_size is the size of the vocabulary used in stable diffusion, and token_vectors_num is the number of token vectors we have to train in textual inversion)."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}, "orig_nbformat": 4}, "nbformat": 4, "nbformat_minor": 2}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lec3. LangChain\n",
    "\n",
    "\n",
    "**LangChain** is a framework for developing applications powered by language models. It provides abundant abstractions about langage models and sources of context (prompt instructions, few shot examples, content to ground its response in, etc.), which enable the user to easily **chain** these components together for developing awesome applications.\n",
    "\n",
    "In this lab, we will learn several key abstractions in LangChain and build an input-output customized AI-powered web-search application.\n",
    "\n",
    "### Reference \n",
    "1. [Langchain document](https://python.langchain.com/docs/introduction/)\n",
    "\n",
    "\n",
    "## 0. First thing first\n",
    "\n",
    "### 0.1 Dependencies and Keys\n",
    "  \n",
    "In addition to the Open AI keys, add the following keys to your .env file.\n",
    "\n",
    "- Serp api key:\n",
    "    ```\n",
    "    SERPAPI_API_KEY=\"YOURKEY\"\n",
    "    ```\n",
    "    The `SERPAPI_API_KEY` is for invoking the search engine, first register through this [web site](https://serpapi.com/).\n",
    "\n",
    "    After getting these two keys, set your keys as environment variables.\n",
    "- Langchain API key (for tracing)\n",
    "    ```\n",
    "    LANGCHAIN_TRACING_V2=\"true\"\n",
    "    LANGCHAIN_API_KEY=ls_xxxxxxxx\n",
    "    ```\n",
    "    To create a `LANGCHAIN_API_KEY`, you can register through the [LANGSMITH](https://docs.smith.langchain.com/)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have installed these dependencies in your image\n",
    "# %pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "# os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL = \"qwen2.5-72b-instruct\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.environ.get(\n",
    "    \"INFINI_API_KEY\"\n",
    ")  # langchain use this environment variable to find the OpenAI API key\n",
    "OPENAI_BASE = os.environ.get(\n",
    "    \"INFINI_BASE_URL\"\n",
    ")  # will be used to pass the OpenAI base URL to langchain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Key abstractions in LangChain\n",
    "\n",
    "| Abstracted Components | Input Type                                | Output Type           |\n",
    "|-----------------------|-------------------------------------------|-----------------------|\n",
    "| Prompt                | Dictionary                                | PromptValue           |\n",
    "| ChatModel             | string, list of messages or a PromptValue | string, ChatMessage   |\n",
    "| OutputParser          | The output of an LLM or ChatModel         | Depends on the parser |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 The ChatModel\n",
    "\n",
    "`ChatModels` is a language model which takes a list of messages or a string as input and returns a message or a string.\n",
    "\n",
    "`ChatModel` provides two methods to interact with the user:\n",
    "\n",
    "- `predict`: takes in a string, returns a string.\n",
    "- `predict_messages`: takes in a list of messages, returns a message.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0, model=CHAT_MODEL, base_url=OPENAI_BASE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an output utility to show the type and the content of the result\n",
    "def print_with_type(res):\n",
    "    print(f\"%s\" % (type(res)))\n",
    "    print(f\"%s\" % res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are four roles in LangChain, and you can define your own custom roles.\n",
    "\n",
    "- `HumanMessage`: A ChatMessage coming from a human/user.\n",
    "- `AIMessage`: A ChatMessage coming from an AI/assistant.\n",
    "- `SystemMessage`: A ChatMessage coming from the system.\n",
    "- `FunctionMessage`: A ChatMessage coming from a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='Hello Xu Wei! Nice to meet you too! I\\'d be happy to tell you about LangChain.\\n\\nLangChain is a framework designed to help developers build applications that leverage large language models (LLMs) like me. It provides a set of tools and abstractions that make it easier to integrate LLMs into various applications, from chatbots and content generators to more complex systems that require natural language understanding and generation.\\n\\n### Key Features of LangChain:\\n1. **Modular Design**: LangChain is built with a modular architecture, allowing developers to mix and match different components to suit their specific needs. This includes different language models, data sources, and output formats.\\n\\n2. **Chain of Thought**: One of the core concepts in LangChain is the \"Chain of Thought\" (CoT) pattern. This involves breaking down complex tasks into a series of simpler steps, each of which can be handled by a language model. This approach helps in improving the accuracy and reliability of the model\\'s outputs.\\n\\n3. **Data Integration**: LangChain supports integration with various data sources, including databases, APIs, and file systems. This allows the language model to access and use external data to generate more informed and contextually relevant responses.\\n\\n4. **Customization**: Developers can customize the behavior of the language model by defining custom prompts, templates, and logic. This flexibility is crucial for tailoring the model to specific use cases and domains.\\n\\n5. **Security and Privacy**: LangChain includes features to help ensure the security and privacy of data. This is particularly important when dealing with sensitive information.\\n\\n6. **Community and Ecosystem**: LangChain has a growing community of developers and contributors. This community provides support, shares best practices, and develops additional tools and plugins that extend the functionality of the framework.\\n\\n### Use Cases:\\n- **Chatbots**: Building conversational agents that can engage in natural language conversations with users.\\n- **Content Generation**: Generating articles, blog posts, and other types of content.\\n- **Code Generation**: Assisting developers by generating code snippets or entire functions.\\n- **Data Analysis**: Analyzing and summarizing large datasets.\\n- **Personal Assistants**: Creating personal assistants that can help with tasks like scheduling, reminders, and information retrieval.\\n\\n### Getting Started:\\nIf you\\'re interested in using LangChain, you can start by checking out the official documentation and tutorials. The documentation provides detailed guides on how to set up the framework, integrate different language models, and build your first application.\\n\\nI hope this gives you a good overview of LangChain! If you have any more questions or need further details, feel free to ask!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 530, 'prompt_tokens': 50, 'total_tokens': 580, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-dbfb29fb-d0f3-40b7-8dd7-a0ac2b761dc1-0' usage_metadata={'input_tokens': 50, 'output_tokens': 530, 'total_tokens': 580, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "qtext = \"hello! my name is xu wei, nice to meet you! could you tell me something about langchain\"\n",
    "\n",
    "messages = []\n",
    "messages.append(\n",
    "    HumanMessage(content=qtext)  # construct a human message\n",
    ")\n",
    "res = chat_model.invoke(messages)  # invoke the chat model\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "messages.append(res)  # append the result to the chat history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructors are tedious to use, and you can use the following more friendly API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content=\"I'm just here to help you out! What can I assist you with today?\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 50, 'total_tokens': 68, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-659035d4-6b36-4764-b57d-85d749fee485-0' usage_metadata={'input_tokens': 50, 'output_tokens': 18, 'total_tokens': 68, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# a simpler way to manage messages\n",
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "history.add_user_message(\"hi!\")  # avoid using the constructor directly\n",
    "history.add_ai_message(\"whats up?\")\n",
    "history.add_user_message(\"nothing much, you?\")\n",
    "\n",
    "res = chat_model.invoke(history.messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangChain has a wide range of applications across various industries and use cases. Here are some of the key areas where LangChain can be particularly useful:\\n\\n### 1. **Chatbots and Virtual Assistants**\\n- **Customer Support**: Building chatbots that can handle customer inquiries, provide product information, and assist with troubleshooting.\\n- **Personal Assistants**: Creating virtual assistants that can manage schedules, set reminders, and perform tasks on behalf of users.\\n\\n### 2. **Content Generation**\\n- **Article Writing**: Generating articles, blog posts, and news updates.\\n- **Social Media Posts**: Creating engaging social media content.\\n- **Marketing Copy**: Writing marketing materials, product descriptions, and ad copy.\\n\\n### 3. **Code Generation and Assistance**\\n- **Code Snippets**: Generating code snippets for specific tasks or problems.\\n- **Documentation**: Writing documentation for code and APIs.\\n- **Code Review**: Providing suggestions and feedback on code quality.\\n\\n### 4. **Data Analysis and Reporting**\\n- **Data Summarization**: Summarizing large datasets and providing insights.\\n- **Report Generation**: Creating detailed reports and visualizations.\\n- **Predictive Analytics**: Using language models to make predictions based on historical data.\\n\\n### 5. **Educational Tools**\\n- **Tutoring Systems**: Building systems that can provide personalized tutoring and feedback.\\n- **Content Creation**: Generating educational content, quizzes, and exercises.\\n- **Language Learning**: Creating tools to help users learn new languages.\\n\\n### 6. **E-commerce and Retail**\\n- **Product Recommendations**: Providing personalized product recommendations based on user preferences and behavior.\\n- **Customer Reviews**: Generating and summarizing customer reviews.\\n- **Inventory Management**: Assisting with inventory management and stock updates.\\n\\n### 7. **Healthcare**\\n- **Patient Engagement**: Building chatbots that can provide health information, answer patient questions, and offer support.\\n- **Medical Documentation**: Assisting healthcare professionals in creating and updating patient records.\\n- **Research Assistance**: Helping researchers by summarizing studies and generating hypotheses.\\n\\n### 8. **Legal and Compliance**\\n- **Contract Review**: Assisting in the review and summarization of legal documents.\\n- **Compliance Checking**: Ensuring that documents and processes comply with regulations.\\n- **Legal Research**: Providing assistance in legal research and case analysis.\\n\\n### 9. **Financial Services**\\n- **Financial Advice**: Offering personalized financial advice and investment recommendations.\\n- **Risk Assessment**: Analyzing and assessing financial risks.\\n- **Report Generation**: Creating financial reports and summaries.\\n\\n### 10. **Gaming and Entertainment**\\n- **NPC Dialogue**: Generating dialogue for non-player characters (NPCs) in video games.\\n- **Story Generation**: Creating dynamic and engaging storylines for games and interactive media.\\n- **Content Moderation**: Assisting in the moderation of user-generated content.\\n\\n### 11. **Research and Development**\\n- **Literature Review**: Summarizing and synthesizing research papers.\\n- **Hypothesis Generation**: Generating hypotheses for scientific experiments.\\n- **Data Analysis**: Assisting in the analysis of experimental data.\\n\\n### 12. **Customer Experience**\\n- **Feedback Analysis**: Analyzing customer feedback and reviews to identify trends and areas for improvement.\\n- **Survey Creation**: Generating and analyzing surveys to gather customer insights.\\n\\n### 13. **Human Resources**\\n- **Resume Screening**: Assisting in the screening of job applications and resumes.\\n- **Interview Preparation**: Providing interview questions and feedback.\\n- **Employee Onboarding**: Creating onboarding materials and assisting new employees.\\n\\n### 14. **Content Moderation**\\n- **Toxicity Detection**: Identifying and flagging toxic or inappropriate content.\\n- **Content Categorization**: Categorizing content into different types (e.g., news, opinion, entertainment).\\n\\n### 15. **Translation and Localization**\\n- **Language Translation**: Translating content into multiple languages.\\n- **Cultural Adaptation**: Adapting content to be culturally appropriate for different regions.\\n\\n### 16. **Creative Writing**\\n- **Storytelling**: Generating creative stories and narratives.\\n- **Poetry and Lyrics**: Writing poetry and song lyrics.\\n- **Scriptwriting**: Assisting in the creation of scripts for films, TV shows, and plays.\\n\\n### 17. **Marketing and Sales**\\n- **Lead Generation**: Generating leads and qualifying potential customers.\\n- **Sales Scripts**: Creating sales scripts and pitches.\\n- **Customer Segmentation**: Segmenting customers based on behavior and preferences.\\n\\n### 18. **Customer Insights**\\n- **Sentiment Analysis**: Analyzing customer sentiment from reviews and social media.\\n- **Trend Analysis**: Identifying trends and patterns in customer behavior.\\n\\n### 19. **Personalized Recommendations**\\n- **Product Recommendations**: Providing personalized product recommendations based on user preferences.\\n- **Content Recommendations**: Recommending articles, videos, and other content based on user interests.\\n\\n### 20. **Automation and Workflow**\\n- **Task Automation**: Automating repetitive tasks and workflows.\\n- **Process Optimization**: Identifying and optimizing business processes.\\n\\nThese applications demonstrate the versatility and power of LangChain in leveraging large language models to solve a wide range of problems and enhance various aspects of business and personal life. If you have a specific use case in mind, feel free to share it, and I can provide more detailed guidance on how LangChain can be applied!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1097, 'prompt_tokens': 594, 'total_tokens': 1691, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-449d7d44-317f-4f18-a238-63172cc8cd02-0' usage_metadata={'input_tokens': 594, 'output_tokens': 1097, 'total_tokens': 1691, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# remembering the chat history and context\n",
    "\n",
    "qtext = \"what is its application?\"\n",
    "messages.append(HumanMessage(content=qtext))  ## providing context of chat histroy\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n",
    "messages.append(res)  ## remembers the histroy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prompt templates\n",
    "\n",
    "LangChain provides ``PromptTemplate`` to help with formatting the prompts. \n",
    "A ``PromptTemplate`` allows you to define a template with placeholders that can be filled in with specific values at runtime. \n",
    "This helps in creating dynamic and reusable prompts for different contexts and inputs.\n",
    "\n",
    "The most plain prompt is in the type of a ``string``. Usually, the prompt includes several different type of `Messages`, which contains the `role` and the plain prompt as `content`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Simple template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "What is a good name for a company that makes candies?\n"
     ]
    }
   ],
   "source": [
    "# Prompt Template\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"What is a good name for a company that makes {product}?\"\n",
    ")\n",
    "input_prompt = prompt.format(product=\"candies\")\n",
    "\n",
    "print_with_type(input_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I love programming.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# Chat Template (a list of temlates in a chat prompt template)\n",
    "\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "\n",
    "# format chat message prompt\n",
    "sys_template = (\n",
    "    \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    ")\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", sys_template),\n",
    "        (\"human\", human_template),\n",
    "    ]\n",
    ")\n",
    "chat_input = chat_prompt.format_messages(\n",
    "    input_language=\"English\", output_language=\"Chinese\", text=\"I love programming.\"\n",
    ")\n",
    "\n",
    "print_with_type(chat_input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Using template in the chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-5af2dad3-1871-49f1-a535-f9a9b4edb0bb-0' usage_metadata={'input_tokens': 29, 'output_tokens': 5, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# format messages with PromptTemplate with translator as an example\n",
    "messages = []\n",
    "chat_input = chat_prompt.format_messages(\n",
    "    input_language=\"English\", output_language=\"Chinese\", text=qtext\n",
    ")\n",
    "print_with_type(chat_input)\n",
    "print_with_type(chat_model.invoke(chat_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Chinese.', additional_kwargs={}, response_metadata={}), HumanMessage(content='what is its application?', additional_kwargs={}, response_metadata={})]\n",
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='它的应用是什么？' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 5, 'prompt_tokens': 29, 'total_tokens': 34, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-c3400939-6a47-4147-97ad-26fa0c76cfca-0' usage_metadata={'input_tokens': 29, 'output_tokens': 5, 'total_tokens': 34, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "messages = chat_input + messages  ## the system message must be at the beginning\n",
    "print_with_type(messages)\n",
    "\n",
    "res = chat_model.invoke(messages)\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Chaining Components together\n",
    "\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components. \n",
    "In LangChain, most of the above key abstraction components are `Runnable` objects, and we can **chain** them together to build awesome applications. \n",
    "\n",
    "LangChain makes the chainning powerful through **LangChain Expression Language (LCEL)**, which can support chainning in manners of:\n",
    "\n",
    "- Async, Batch, and Streaming Support: any chain constructed in LCEL can automatically have full synv, async, batch and streaming support. \n",
    "- Fallbacks: due to many factors like network connection or non-deterministic properties, your LLM applications need to handle errors gracefully. With LCEL, your can easily attach fallbacks any chain.\n",
    "- Parallelism: since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.\n",
    "- LangSmith Tracing Integration: (for debugging, see below).\n",
    "\n",
    "In this lab, we only demonstrate the simplest functional chainning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='{\"这是输入到聊天模型链和聊天提示的内容。\"}' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 39, 'total_tokens': 53, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}} id='run-e28af50a-23ac-4787-956d-10a5f85297f6-0' usage_metadata={'input_tokens': 39, 'output_tokens': 14, 'total_tokens': 53, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "# More abstractions: bundling prompt and the chat_model into a chain\n",
    "\n",
    "translate_chain = chat_prompt | chat_model\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = translate_chain.invoke(\n",
    "    {\"input_language\": \"English\", \"output_language\": \"Chinese\", \"text\": {qtext}}\n",
    ")\n",
    "print_with_type(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Output parser\n",
    "\n",
    "Language models output text, which is often unstructured and free-form. However, in many applications, you may need more structured information to work with, such as JSON, XML, or other formats. This is where output parsers come in.\n",
    "\n",
    "Output parsers are tools that transform the raw text output of language models into structured data formats. The motivation for using output parsers is to facilitate easier data manipulation, integration, and analysis. By converting text into structured formats, you can more effectively utilize the information in downstream applications, automate workflows, and ensure consistency in data handling.\n",
    "\n",
    "LangChain provides several commonly-used output parsers, including:\n",
    "- [JSONparser](https://python.langchain.com/docs/how_to/output_parser_json/): Converts text output into JSON format.\n",
    "- [XMLparser](https://python.langchain.com/docs/how_to/output_parser_xml/): Converts text output into XML format.\n",
    "- [YAMLparser](https://python.langchain.com/docs/how_to/output_parser_yaml/): Converts text output into YAML format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.1 Simple output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "{\"这是输入到聊天模型链和聊天提示的内容。\"}\n"
     ]
    }
   ],
   "source": [
    "# a simple output parser\n",
    "# StdOutParser converts the chat message to a string.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "stdoutchain = chat_prompt | chat_model | output_parser\n",
    "\n",
    "qtext = \"this is input to a chain of chat model and chat prompt.\"\n",
    "out = stdoutchain.invoke(\n",
    "    {\"input_language\": \"English\", \"output_language\": \"Chinese\", \"text\": {qtext}}\n",
    ")\n",
    "print_with_type(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4.2 Advanced output parsers: from Results to a Python Object\n",
    "Here we demonstrate the powerful Json Outputparser as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': 'A Survey of Deep Learning System Optimization, Deep Learning-based Text Representation: A Survey, Efficient and Effective Algorithms for Training Quantized Neural Networks, and more'}\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "class Professor(BaseModel):\n",
    "    name: str = Field(description=\"name of the Professor\")\n",
    "    publications: str = Field(\n",
    "        description=\"the string of the professor's publications, separated by comma\"\n",
    "    )\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query.\\n{format_instructions}\\n{query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "professor_chain = prompt | chat_model | parser\n",
    "query = \"tell me about professor Wei Xu including his publications.\"\n",
    "output = professor_chain.invoke({\"query\": {query}})\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.prompt_values.StringPromptValue'>\n",
      "text='Answer the user query.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"name of the Professor\", \"title\": \"Name\", \"type\": \"string\"}, \"publications\": {\"description\": \"the string of the professor\\'s publications, separated by comma\", \"title\": \"Publications\", \"type\": \"string\"}}, \"required\": [\"name\", \"publications\"]}\\n```\\n{\\'tell me about professor Wei Xu including his publications.\\'}'\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# see how langchain organizes the input to construct the result.\n",
    "# you can do so by printing the input of the chat_model.\n",
    "output = prompt.invoke({\"query\": {query}})\n",
    "print_with_type(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice that the list of papers lacks substantial information and contains many inaccuracies. \n",
    "\n",
    "This is because the model has no knowlege about Prof. Wei Xu.  \n",
    "\n",
    "We will now demonstrate how to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Adding more contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Allowing the model to search the web: Retrievers\n",
    "\n",
    "Many LLM applications require user-specific data that is not part of the model's training set, like the above example : )\n",
    "The primary way of accomplishing this is through **Retrieval Augmented Generation (RAG)**. In this process, external data is retrieved and then passed to the LLM when doing the generation step. `Retriever` is an interface that returns documents given an unstructured query, which is used to provide the related contents to LLMs\n",
    "\n",
    "LangChain provides all the building blocks for RAG applications - from simple to complex, including document loaders, text embedding models and web searches.  We will introduce these models in Lab 4.  Here, we introduce three very basic retrievers that does web search and some local file access.  \n",
    "\n",
    "- [web page](https://python.langchain.com/docs/how_to/document_loader_web/)\n",
    "- [load Json](https://python.langchain.com/docs/how_to/document_loader_json/)\n",
    "- [load PDF files](https://python.langchain.com/docs/how_to/document_loader_pdf/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "SerpAPI is a widely used API to access search engine results. It allows you to scrape and parse search engine data, providing a way to retrieve up-to-date information from the web.\n",
    "\n",
    "In LangChain, SerpAPI can be integrated as a retriever to enhance the capabilities of language models by providing them with the latest information from the web. This is particularly useful for applications that require current data that is not part of the model's training set.\n",
    "\n",
    "By using SerpAPI with LangChain, you can perform web searches and feed the retrieved data into the language model to generate more accurate and contextually relevant responses.\n",
    "\n",
    "In the following sections, we will demonstrate how to set up and use SerpAPI within LangChain to perform web searches and integrate the results into your language model workflows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'title': \"Nvidia GTC 2025: What to expect from Nvidia's biggest event of the year\", 'link': 'https://finance.yahoo.com/news/nvidia-gtc-2025-what-to-expect-from-nvidias-biggest-event-of-the-year-123039333.html', 'source': 'Yahoo Finance', 'date': '20 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24b43c2f81805cf6b5b3be5815be8f76320.jpeg'}, {'title': 'Nvidia GTC 2025 live updates: Blackwell Ultra, next-gen Rubin chip architecture, and much more', 'link': 'https://techcrunch.com/storyline/nvidia-gtc-2025-live-updates-blackwell-ultra-next-gen-rubin-chip-architecture-and-much-more/', 'source': 'TechCrunch', 'date': '9 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24b4c7b0aaf8781d047769a910f60ccdbb5.jpeg'}, {'title': \"GTC 2025: On-site preview of Nvidia's biggest AI announcements\", 'link': 'https://www.digitimes.com/news/a20250318VL205/gtc-tsmc-lite-on-technology-jensen-huang-supply-chain.html', 'source': 'DigiTimes', 'date': '47 minutes ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24bc3f521adf0ee9f31107994e50c988a68.jpeg'}, {'title': \"92% of Nvidia users turn on DLSS... if they've been lucky enough to bag an RTX 50-series card at launch AND have the Nvidia App installed\", 'link': 'https://www.pcgamer.com/hardware/graphics-cards/92-percent-of-nvidia-users-turn-on-dlss-if-theyve-been-lucky-enough-to-bag-an-rtx-50-series-card-at-launch-and-have-the-nvidia-app-installed/', 'source': 'PC Gamer', 'date': '23 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24bfae7f5d60d411131c3c58ec69e5546a1.jpeg'}, {'title': 'NVIDIA Unveils Playable Half-Life 2 RTX With Glorious Ray Tracing', 'link': 'https://hothardware.com/news/nvidia-unveils-playable-half-life-2-rtx', 'source': 'HotHardware', 'date': '14 hours ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24babd727336341ea3db8d3f4b7e073af98.jpeg'}, {'title': 'The Ultimate Guide to Nvidia: Strategies to Buy, Hold, and Profit', 'link': 'https://www.fool.com/investing/2025/03/18/ultimate-guide-to-nvidia-strategies-to-buy-profit/', 'source': 'The Motley Fool', 'date': '26 minutes ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24b66fb3dbf7d1dd2bb5aaa3616b9d0b852.jpeg'}, {'title': 'Trump-Putin phone call expected, Nvidia’s Huang to speak - what’s moving markets', 'link': 'https://www.investing.com/news/economy-news/trumpputin-phone-call-expected-nvidias-huang-to-speak--whats-moving-markets-3933269', 'source': 'Investing.com', 'date': '6 minutes ago', 'thumbnail': 'https://serpapi.com/searches/67d9307ec24d6098b461f425/images/1facaa3a5b60e24b8138a12753d5821b3cc4677d8b5018de.jpeg'}]\n"
     ]
    }
   ],
   "source": [
    "# Using the search API\n",
    "\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "results = search.run(\"Nvidia\")\n",
    "print_with_type(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put the search and LLM together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "\n",
    "\n",
    "class News(BaseModel):\n",
    "    title: str = Field(description=\"title of the news\")\n",
    "    brief_desc: str = Field(description=\"brief descrption of the corresponding news\")\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "    \"context\": search.run,  # passing a retriever\n",
    "    \"query\": RunnablePassthrough(),\n",
    "}\n",
    "websearch_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_chain.invoke(\n",
    "    \"tell me about the nvidia companies, and write a brief summary for it\"\n",
    ")\n",
    "\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Debugging and Logging\n",
    "\n",
    "#### 2.2.1 The debug mode\n",
    "\n",
    "LangChain provides a ``debug`` mode that allows you to see detailed information about the execution of your chains and agents. When debug mode is enabled, LangChain will print verbose output showing:\n",
    "\n",
    "1. The exact prompts being sent to the LLM\n",
    "2. The raw responses received from the LLM\n",
    "3. The execution flow of chains and agents\n",
    "4. Any intermediate steps and tool calls\n",
    "\n",
    "This is extremely useful for:\n",
    "- Troubleshooting unexpected outputs\n",
    "- Understanding how your chains are processing data\n",
    "- Optimizing prompts\n",
    "- Identifying errors in your chain's logic\n",
    "\n",
    "You can enable debug mode using `set_debug(True)` and disable it with `set_debug(False)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [3.41s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [3.42s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"NVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\",\n",
      "  \"query\": \"tell me about the company nvidia, and write a brief summary for it\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\nNVIDIA is the engine of the world's AI infrastructure. Companies and countries around the world are building NVIDIA-powered AI factories to process, refine, and manufacture intelligence from data, creating new revenue opportunities for the world's $100 trillion of industries.\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"title\\\": {\\\"description\\\": \\\"title of the news\\\", \\\"title\\\": \\\"Title\\\", \\\"type\\\": \\\"string\\\"}, \\\"brief_desc\\\": {\\\"description\\\": \\\"brief descrption of the corresponding news\\\", \\\"title\\\": \\\"Brief Desc\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"title\\\", \\\"brief_desc\\\"]}\\n```\\nQuery: tell me about the company nvidia, and write a brief summary for it\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.36s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"title\\\": \\\"NVIDIA: The Engine of Global AI Infrastructure\\\",\\n  \\\"brief_desc\\\": \\\"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 70,\n",
      "                \"prompt_tokens\": 294,\n",
      "                \"total_tokens\": 364,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-d3428c66-a048-4c8b-a0a4-741ed840120d-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 294,\n",
      "              \"output_tokens\": 70,\n",
      "              \"total_tokens\": 364,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 70,\n",
      "      \"prompt_tokens\": 294,\n",
      "      \"total_tokens\": 364,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [11ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [5.81s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"title\": \"NVIDIA: The Engine of Global AI Infrastructure\",\n",
      "  \"brief_desc\": \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"}\n"
     ]
    }
   ],
   "source": [
    "# Debugging and logging: debug mode\n",
    "from langchain.globals import set_debug\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\n",
    "    \"tell me about the company nvidia, and write a brief summary for it\"\n",
    ")\n",
    "\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'title': 'NVIDIA: The Engine of Global AI Infrastructure', 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries.\"}\n"
     ]
    }
   ],
   "source": [
    "set_debug(False)\n",
    "# Try rerun the previous example to see the verbose output.\n",
    "res = websearch_chain.invoke(\n",
    "    \"tell me about the company nvidia, and write a brief summary for it\"\n",
    ")\n",
    "\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Tracing with LangSmith\n",
    "\n",
    "LangSmith is a developer platform that helps you debug, test, evaluate, and monitor LLM applications. It provides:\n",
    "\n",
    "1. **Tracing**: Visualize the execution flow of your chains and agents\n",
    "2. **Debugging**: Inspect inputs, outputs, and intermediate steps\n",
    "3. **Evaluation**: Measure and compare model performance\n",
    "4. **Monitoring**: Track usage and performance in production\n",
    " \n",
    "To use LangSmith tracing:\n",
    "Set environment variables (in your .env file):\n",
    "   - `LANGCHAIN_TRACING_V2=\"true\"` to enable tracing\n",
    "   - `LANGCHAIN_API_KEY` with your LangSmith API key\n",
    "\n",
    "After running your code, you can view detailed traces at https://docs.smith.langchain.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'NVIDIA: The Engine of Global AI Infrastructure',\n",
       " 'brief_desc': \"NVIDIA is a leading technology company that powers the world's AI infrastructure. It enables companies and countries to build AI factories that process and refine data to generate intelligence, opening up new revenue opportunities across various industries estimated at $100 trillion.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "from langsmith import traceable\n",
    "\n",
    "# Auto-trace LLM calls in-context\n",
    "client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "@traceable  # Auto-trace this function\n",
    "def pipeline(user_input: str):\n",
    "    return websearch_chain.invoke(user_input)\n",
    "\n",
    "\n",
    "pipeline(\"tell me about the nvidia companies, and write a brief summary for it\")\n",
    "# Out:  Hello there! How can I assist you today?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### your task ####\n",
    "# go to the langsmith webpage and observe the traces.\n",
    "\n",
    "# my traces: https://smith.langchain.com/o/e69ffa17-9bd4-4be5-824f-d82ed88eb809/projects/p/0d9ced39-546b-4f10-8684-14e4243f5958\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': \"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\"}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# retrieve the information and fix the query results about Prof. Xu,\n",
    "# generating the correct Professor object.\n",
    "# Note that you do not have to get a perfect answer from the LLM in this lab.\n",
    "# (if the answer is not perfect, please analyze and debug it in the next cell.)\n",
    "parser = JsonOutputParser(pydantic_object=Professor)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"Answer the user query based on the following context: \\n{context}\\n{format_instructions}\\nQuery: {query}\",\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
    ")\n",
    "\n",
    "chat_model.temperature = 0\n",
    "\n",
    "search = SerpAPIWrapper()\n",
    "setup_and_retrieval = {\n",
    "    \"context\": search.run,  # passing a retriever\n",
    "    \"query\": RunnablePassthrough(),\n",
    "}\n",
    "websearch_professor_chain = setup_and_retrieval | prompt | chat_model | parser\n",
    "\n",
    "res = websearch_professor_chain.invoke(\n",
    "    \"tell me about professor Wei Xu including his publications.\"\n",
    ")\n",
    "\n",
    "print_with_type(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query> > chain:run] [616ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"[' entity_type: explore_universal.', 'My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ...', 'Dr. Xu is interested in the neuronal mechanisms underlying the basic cognitive processes. He focuses on the circuit level with the goal of bridging cellular ...', 'I have a broad research interest in distributed system design, big data and financial technology. My current projects include privacy-preserving computation, ...', 'Wei Xu is an assistant professor in the School of Interactive Computing at the Georgia Institute of Technology. Xu received her Ph.D. in Computer Science ...', 'My research addresses fundamental questions on the roles of protein arginine methyltransferases (PRMTs) in governing essential cellular processes.', 'Full Professor at Huazhong University of Science and Technology · Prof. Dr Xu received the double B.E. degree from Tianjin University (TJU), China, ...', 'Prof. Xu is IET Fellow and IEEE Senior Member. He has served as an Associate Editor for several Journals, such as IEEE TRANS. ON INDUSTRIAL ELECTRONICS, IEEE ...', 'Since 2013, he has been Full Professor with State Key Laboratory of Advanced Electromagnetic Engineering in Huazhong University of Science and Technology, ...', 'My laboratory is focused on the transcriptional regulation of estrogen receptor (ER) signaling pathways by nuclear receptor co-factors.', \\\"We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"]\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > chain:RunnableParallel<context,query>] [624ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"context\": \"[' entity_type: explore_universal.', 'My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ...', 'Dr. Xu is interested in the neuronal mechanisms underlying the basic cognitive processes. He focuses on the circuit level with the goal of bridging cellular ...', 'I have a broad research interest in distributed system design, big data and financial technology. My current projects include privacy-preserving computation, ...', 'Wei Xu is an assistant professor in the School of Interactive Computing at the Georgia Institute of Technology. Xu received her Ph.D. in Computer Science ...', 'My research addresses fundamental questions on the roles of protein arginine methyltransferases (PRMTs) in governing essential cellular processes.', 'Full Professor at Huazhong University of Science and Technology · Prof. Dr Xu received the double B.E. degree from Tianjin University (TJU), China, ...', 'Prof. Xu is IET Fellow and IEEE Senior Member. He has served as an Associate Editor for several Journals, such as IEEE TRANS. ON INDUSTRIAL ELECTRONICS, IEEE ...', 'Since 2013, he has been Full Professor with State Key Laboratory of Advanced Electromagnetic Engineering in Huazhong University of Science and Technology, ...', 'My laboratory is focused on the transcriptional regulation of estrogen receptor (ER) signaling pathways by nuclear receptor co-factors.', \\\"We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"]\",\n",
      "  \"query\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"context\": \"[' entity_type: explore_universal.', 'My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ...', 'Dr. Xu is interested in the neuronal mechanisms underlying the basic cognitive processes. He focuses on the circuit level with the goal of bridging cellular ...', 'I have a broad research interest in distributed system design, big data and financial technology. My current projects include privacy-preserving computation, ...', 'Wei Xu is an assistant professor in the School of Interactive Computing at the Georgia Institute of Technology. Xu received her Ph.D. in Computer Science ...', 'My research addresses fundamental questions on the roles of protein arginine methyltransferases (PRMTs) in governing essential cellular processes.', 'Full Professor at Huazhong University of Science and Technology · Prof. Dr Xu received the double B.E. degree from Tianjin University (TJU), China, ...', 'Prof. Xu is IET Fellow and IEEE Senior Member. He has served as an Associate Editor for several Journals, such as IEEE TRANS. ON INDUSTRIAL ELECTRONICS, IEEE ...', 'Since 2013, he has been Full Professor with State Key Laboratory of Advanced Electromagnetic Engineering in Huazhong University of Science and Technology, ...', 'My laboratory is focused on the transcriptional regulation of estrogen receptor (ER) signaling pathways by nuclear receptor co-factors.', \\\"We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"]\",\n",
      "  \"query\": \"tell me about professor Wei Xu including his publications.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > prompt:PromptTemplate] [2ms] Exiting Prompt run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the user query based on the following context: \\n[' entity_type: explore_universal.', 'My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ...', 'Dr. Xu is interested in the neuronal mechanisms underlying the basic cognitive processes. He focuses on the circuit level with the goal of bridging cellular ...', 'I have a broad research interest in distributed system design, big data and financial technology. My current projects include privacy-preserving computation, ...', 'Wei Xu is an assistant professor in the School of Interactive Computing at the Georgia Institute of Technology. Xu received her Ph.D. in Computer Science ...', 'My research addresses fundamental questions on the roles of protein arginine methyltransferases (PRMTs) in governing essential cellular processes.', 'Full Professor at Huazhong University of Science and Technology · Prof. Dr Xu received the double B.E. degree from Tianjin University (TJU), China, ...', 'Prof. Xu is IET Fellow and IEEE Senior Member. He has served as an Associate Editor for several Journals, such as IEEE TRANS. ON INDUSTRIAL ELECTRONICS, IEEE ...', 'Since 2013, he has been Full Professor with State Key Laboratory of Advanced Electromagnetic Engineering in Huazhong University of Science and Technology, ...', 'My laboratory is focused on the transcriptional regulation of estrogen receptor (ER) signaling pathways by nuclear receptor co-factors.', \\\"We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"]\\nThe output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\\\"properties\\\": {\\\"foo\\\": {\\\"title\\\": \\\"Foo\\\", \\\"description\\\": \\\"a list of strings\\\", \\\"type\\\": \\\"array\\\", \\\"items\\\": {\\\"type\\\": \\\"string\\\"}}}, \\\"required\\\": [\\\"foo\\\"]}\\nthe object {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]} is a well-formatted instance of the schema. The object {\\\"properties\\\": {\\\"foo\\\": [\\\"bar\\\", \\\"baz\\\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\\\"properties\\\": {\\\"name\\\": {\\\"description\\\": \\\"name of the Professor\\\", \\\"title\\\": \\\"Name\\\", \\\"type\\\": \\\"string\\\"}, \\\"publications\\\": {\\\"description\\\": \\\"the string of the professor's publications, separated by comma\\\", \\\"title\\\": \\\"Publications\\\", \\\"type\\\": \\\"string\\\"}}, \\\"required\\\": [\\\"name\\\", \\\"publications\\\"]}\\n```\\nQuery: tell me about professor Wei Xu including his publications.\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:RunnableSequence > llm:ChatOpenAI] [2.58s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"```json\\n{\\n  \\\"name\\\": \\\"Wei Xu\\\",\\n  \\\"publications\\\": \\\"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"\\n}\\n```\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"```json\\n{\\n  \\\"name\\\": \\\"Wei Xu\\\",\\n  \\\"publications\\\": \\\"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\\\"\\n}\\n```\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 78,\n",
      "                \"prompt_tokens\": 570,\n",
      "                \"total_tokens\": 648,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-4cc39450-3862-4dce-9757-48bb65c190e8-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 570,\n",
      "              \"output_tokens\": 78,\n",
      "              \"total_tokens\": 648,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 78,\n",
      "      \"prompt_tokens\": 570,\n",
      "      \"total_tokens\": 648,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence > parser:JsonOutputParser] [11ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"name\": \"Wei Xu\",\n",
      "  \"publications\": \"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:RunnableSequence] [3.23s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"name\": \"Wei Xu\",\n",
      "  \"publications\": \"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\"\n",
      "}\n",
      "<class 'dict'>\n",
      "{'name': 'Wei Xu', 'publications': \"My research lies at the intersections of machine learning, natural language processing, and social media. ... I received the NSF CAREER Award, Faculty Research ..., We spoke with Wei Xu, an associate professor at Georgia Tech's School of Interactive Computing, whose research focuses on natural language processing and ...\"}\n"
     ]
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# analyze the answer, if the answer is not correct, write down some comments\n",
    "# about why the answer is not correct.\n",
    "set_debug(True)\n",
    "\n",
    "res = websearch_professor_chain.invoke(\n",
    "    \"tell me about professor Wei Xu including his publications.\"\n",
    ")\n",
    "\n",
    "set_debug(False)\n",
    "\n",
    "print_with_type(res)\n",
    "\n",
    "# First, the query does not specify the person's detailed information, and the output from the search API may be about other people named Wei Xu.\n",
    "# Second, the query is not instructive enough to guide the search API and model to provide the publications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Smarter workflow: Agents\n",
    "\n",
    "\n",
    "An AI agent is an autonomous system that perceives its environment, makes decisions, and takes actions to achieve specific goals. In the context of LLMs, an agent uses a language model as its reasoning engine to determine what actions to take and in what order, unlike chains where the sequence is predefined.\n",
    "\n",
    "LangChain provides several frameworks for building agents:\n",
    "1. **Tool integration**: LangChain allows agents to use external tools and APIs to gather information or perform actions\n",
    "2. **Agent types**: Supports various agent architectures like ReAct (Reasoning and Acting), Plan-and-Execute, and others\n",
    "3. **Memory systems**: Enables agents to maintain context across interactions (next lab)\n",
    "4. **Structured output**: Helps parse and validate agent responses\n",
    "\n",
    "For more advanced agent capabilities, LangGraph (an extension of LangChain) offers enhanced features for creating highly controllable and customizable agents with better state management and complex workflows (next lab).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Function-calling: Letter r's in straberry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the following very simple example, and see if LLM can get it correct. (the correct answer is 3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='There are two \\'r\\'s in the word \"strawberry.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 40, 'total_tokens': 55, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5-72b-instruct', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': {'content': [], 'refusal': None}}, id='run-e530ca7b-8af5-4702-b59f-f2909d210033-0', usage_metadata={'input_tokens': 40, 'output_tokens': 15, 'total_tokens': 55, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model.invoke(\"how many r's are there in the word strawberry?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the AI answered incorrectly by stating there are two 'r's in the word \"strawberry\" when there are actually three 'r's (st**r**awbe**rr**y). This demonstrates a limitation of LLMs in performing simple counting tasks. Even advanced models can make these basic errors because they process text holistically rather than character-by-character like humans do. This is why tools like function calling are useful - they allow us to delegate specific tasks (like counting) to dedicated functions that can perform them accurately.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[StructuredTool(name='get_letter_count', description='Returns the number of a specific letter in a word.\\n    The input should be in the format: word,letter (e.g., strawberry,r)', args_schema=<class 'langchain_core.utils.pydantic.get_letter_count'>, func=<function get_letter_count at 0x7f74a6dd3ce0>)]\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "\n",
    "\n",
    "# here is an example of a tool that can be used to count the number of a specific letter in a word.\n",
    "# note that we only use a single string parameter, because the simple agent only accept tools with a single parameter.\n",
    "# to fit two parameters, we need a clear format instruction for the input.\n",
    "@tool\n",
    "def get_letter_count(query: str) -> int:\n",
    "    \"\"\"Returns the number of a specific letter in a word.\n",
    "    The input should be in the format: word,letter (e.g., strawberry,r)\"\"\"\n",
    "    word, letter = query.split(\",\")\n",
    "    return word.count(letter)\n",
    "\n",
    "\n",
    "print(get_letter_count.invoke(\"strawberry,r\"))\n",
    "\n",
    "tools = [get_letter_count]\n",
    "\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant who can use tools, but bad at counting letters in words.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\"\n",
    "        ),  # used to store the previous agent tool invocations and the corresponding tool outputs.\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word.\\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23564/2737035873.py:5: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent_chain = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.36s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 37,\n",
      "                \"prompt_tokens\": 200,\n",
      "                \"total_tokens\": 237,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-e841caf8-df7f-46b8-92cb-cb07d9a3ab00-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 200,\n",
      "              \"output_tokens\": 37,\n",
      "              \"total_tokens\": 237,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 37,\n",
      "      \"prompt_tokens\": 200,\n",
      "      \"total_tokens\": 237,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.36s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] Entering Tool run with input:\n",
      "\u001b[0m\"strawberry,r\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:get_letter_count] [1ms] Exiting Tool run with output:\n",
      "\u001b[0m\"3\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"how many r's are there in the word strawberry?\",\n",
      "  \"agent_scratchpad\": \"I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nget_letter_count(query: str) -> int - Returns the number of a specific letter in a word.\\n    The input should be in the format: word,letter (e.g., strawberry,r)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [get_letter_count]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: how many r's are there in the word strawberry?\\nThought:I need to find out how many times the letter 'r' appears in the word 'strawberry'.\\nAction: get_letter_count\\nAction Input: strawberry,r\\nObservation: 3\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.27s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 23,\n",
      "                \"prompt_tokens\": 241,\n",
      "                \"total_tokens\": 264,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-cca60702-a6bf-4efa-9f8c-a6712acb9a86-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 241,\n",
      "              \"output_tokens\": 23,\n",
      "              \"total_tokens\": 264,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 23,\n",
      "      \"prompt_tokens\": 241,\n",
      "      \"total_tokens\": 264,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.28s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: There are 3 'r's in the word strawberry.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.64s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"There are 3 'r's in the word strawberry.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    chat_model,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    prompt_template=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "agent_chain.invoke({\"input\": \"how many r's are there in the word strawberry?\"})\n",
    "\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort the numbers 3,1,4,1,5,9,2,6,5,3,5\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort the numbers 3,1,4,1,5,9,2,6,5,3,5\",\n",
      "  \"agent_scratchpad\": \"\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nsort_numbers(numbers: str) -> str - Returns the sorted numbers in a comma separated string.\\n    The input should be a comma separated string of numbers (e.g., 3,2,1)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [sort_numbers]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: sort the numbers 3,1,4,1,5,9,2,6,5,3,5\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.40s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I need to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 3,1,4,1,5,9,2,6,5,3,5\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I need to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 3,1,4,1,5,9,2,6,5,3,5\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 42,\n",
      "                \"prompt_tokens\": 215,\n",
      "                \"total_tokens\": 257,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-f043166f-bb69-4b3e-a22a-b0a109420847-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 215,\n",
      "              \"output_tokens\": 42,\n",
      "              \"total_tokens\": 257,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 42,\n",
      "      \"prompt_tokens\": 215,\n",
      "      \"total_tokens\": 257,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.41s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I need to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 3,1,4,1,5,9,2,6,5,3,5\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:sort_numbers] Entering Tool run with input:\n",
      "\u001b[0m\"3,1,4,1,5,9,2,6,5,3,5\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[chain:AgentExecutor > tool:sort_numbers] [2ms] Exiting Tool run with output:\n",
      "\u001b[0m\"1,1,2,3,3,4,5,5,5,6,9\"\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"sort the numbers 3,1,4,1,5,9,2,6,5,3,5\",\n",
      "  \"agent_scratchpad\": \"I need to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 3,1,4,1,5,9,2,6,5,3,5\\nObservation: 1,1,2,3,3,4,5,5,5,6,9\\nThought:\",\n",
      "  \"stop\": [\n",
      "    \"\\nObservation:\",\n",
      "    \"\\n\\tObservation:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: Answer the following questions as best you can. You have access to the following tools:\\n\\nsort_numbers(numbers: str) -> str - Returns the sorted numbers in a comma separated string.\\n    The input should be a comma separated string of numbers (e.g., 3,2,1)\\n\\nUse the following format:\\n\\nQuestion: the input question you must answer\\nThought: you should always think about what to do\\nAction: the action to take, should be one of [sort_numbers]\\nAction Input: the input to the action\\nObservation: the result of the action\\n... (this Thought/Action/Action Input/Observation can repeat N times)\\nThought: I now know the final answer\\nFinal Answer: the final answer to the original input question\\n\\nBegin!\\n\\nQuestion: sort the numbers 3,1,4,1,5,9,2,6,5,3,5\\nThought:I need to sort the given numbers.\\nAction: sort_numbers\\nAction Input: 3,1,4,1,5,9,2,6,5,3,5\\nObservation: 1,1,2,3,3,4,5,5,5,6,9\\nThought:\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain > llm:ChatOpenAI] [1.35s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"I now know the final answer\\nFinal Answer: 1,1,2,3,3,4,5,5,5,6,9\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"stop\",\n",
      "          \"logprobs\": {\n",
      "            \"content\": [],\n",
      "            \"refusal\": null\n",
      "          }\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"I now know the final answer\\nFinal Answer: 1,1,2,3,3,4,5,5,5,6,9\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"refusal\": null\n",
      "            },\n",
      "            \"response_metadata\": {\n",
      "              \"token_usage\": {\n",
      "                \"completion_tokens\": 33,\n",
      "                \"prompt_tokens\": 281,\n",
      "                \"total_tokens\": 314,\n",
      "                \"completion_tokens_details\": null,\n",
      "                \"prompt_tokens_details\": null\n",
      "              },\n",
      "              \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "              \"system_fingerprint\": null,\n",
      "              \"finish_reason\": \"stop\",\n",
      "              \"logprobs\": {\n",
      "                \"content\": [],\n",
      "                \"refusal\": null\n",
      "              }\n",
      "            },\n",
      "            \"type\": \"ai\",\n",
      "            \"id\": \"run-3504ae3c-7eed-4404-b597-6621e1abcaf3-0\",\n",
      "            \"usage_metadata\": {\n",
      "              \"input_tokens\": 281,\n",
      "              \"output_tokens\": 33,\n",
      "              \"total_tokens\": 314,\n",
      "              \"input_token_details\": {},\n",
      "              \"output_token_details\": {}\n",
      "            },\n",
      "            \"tool_calls\": [],\n",
      "            \"invalid_tool_calls\": []\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 33,\n",
      "      \"prompt_tokens\": 281,\n",
      "      \"total_tokens\": 314,\n",
      "      \"completion_tokens_details\": null,\n",
      "      \"prompt_tokens_details\": null\n",
      "    },\n",
      "    \"model_name\": \"qwen2.5-72b-instruct\",\n",
      "    \"system_fingerprint\": null\n",
      "  },\n",
      "  \"run\": null,\n",
      "  \"type\": \"LLMResult\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor > chain:LLMChain] [1.36s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"text\": \"I now know the final answer\\nFinal Answer: 1,1,2,3,3,4,5,5,5,6,9\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[chain:AgentExecutor] [2.77s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"1,1,2,3,3,4,5,5,5,6,9\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "#### your task ####\n",
    "# implement a tool that sort an array of numbers (packed as a comma separated string)\n",
    "# then ask the agent to use the tool to sort an input array.\n",
    "@tool\n",
    "def sort_numbers(numbers: str) -> str:\n",
    "    \"\"\"Returns the sorted numbers in a comma separated string.\n",
    "    The input should be a comma separated string of numbers (e.g., 3,2,1)\"\"\"\n",
    "    return \",\".join(sorted(numbers.split(\",\")))\n",
    "\n",
    "\n",
    "tools = [sort_numbers]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant who can use tools, but bad at sorting numbers.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\"\n",
    "        ),  # used to store the previous agent tool invocations and the corresponding tool outputs.\n",
    "    ]\n",
    ")\n",
    "\n",
    "set_debug(True)\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    chat_model,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    prompt_template=prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "agent_chain.invoke({\"input\": \"sort the numbers 3,1,4,1,5,9,2,6,5,3,5\"})\n",
    "\n",
    "set_debug(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Create an auto-web-search AI Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the following example about the built-in web-search tool and understand the code.\n",
    "\n",
    "\n",
    "from langchain.agents import load_tools\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=News)\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant, helping the users search the web and write summary for the user's interested topic: {keyword}\",\n",
    "        ),\n",
    "        (\"user\", \"{keyword}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\"\n",
    "        ),  # used to store the previous agent tool invocations and the corresponding tool outputs.\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "tools = [load_tools([\"serpapi\"], chat_model)[0]]\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    chat_model,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    prompt_template=prompt,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for recent news from Tsinghua University to provide an updated answer.\n",
      "Action: Search\n",
      "Action Input: \"Tsinghua University news last week\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[' entity_type: related_questions.', 'Vietnamese Prime Minister Pham Minh Chinh meets with Secretary of the CPC Tsinghua University Committee Qiu Yong and witnesses signing of MoUs with Vietnames...', 'LATEST NEWS · Spring Snow in Tsinghua · Your guide to upcoming exhibitions and performances at Tsinghua · \\u200bResearch team from Department of Computer Science and ...', 'The latest news, analysis and opinion on Tsinghua University. In-depth analysis, industry insights and expert opinion.', 'LATEST NEWS · Mar25. Spring in Tsinghua · Mar18. Exhibition of Indonesian modern, contemporary art opens in Beijing · Mar17. Tsinghua hosts job fair for 2021 ...', 'LATEST NEWS · May26. Pritzker-Tsinghua Fellowship Program Inaugurated · May20. Pennsylvania State University President Visits Tsinghua · May16. Tsinghua Ranks ...', \"Why is Tsinghua University ranked number 1 for US News' best global universities for Computer Science? Is their program actually better than ...\", 'LATEST NEWS · May06. Premier Wen Jiabao, Delegation Visit Students on Youth Day · May06. Vice President Hu meets with Hong Kong, Macao and Taiwan Students · May06.', 'Get information on Tsinghua University at US News. Find out where the university is ranked globally based on its academic reputation and research.', 'HEADLINES · Multi-stakeholders from 6 Continents Co-initiate the Global Youth Climate Week · HEADLINES · The 2nd World Health Forum Opens · HEADLINES · \\u200bWhere ...', 'Khronos Group Welcomes Tsinghua University as Academic Member ... Tsinghua is one of the most prestigious universities in China. Through teaching, research and ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results include a variety of news items, but the most relevant and recent one is the meeting between the Vietnamese Prime Minister and the Secretary of the CPC Tsinghua University Committee. There are also mentions of spring events and research activities, but they are less specific.\n",
      "Final Answer: One of the significant pieces of news from Tsinghua University within the last week is that Vietnamese Prime Minister Pham Minh Chinh met with Secretary of the CPC Tsinghua University Committee Qiu Yong. During the meeting, they witnessed the signing of Memorandums of Understanding (MoUs) with Vietnamese partners. Additionally, there have been updates on spring events and ongoing research activities at the university.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me the news from tsinghua university within last week?',\n",
       " 'output': 'One of the significant pieces of news from Tsinghua University within the last week is that Vietnamese Prime Minister Pham Minh Chinh met with Secretary of the CPC Tsinghua University Committee Qiu Yong. During the meeting, they witnessed the signing of Memorandums of Understanding (MoUs) with Vietnamese partners. Additionally, there have been updates on spring events and ongoing research activities at the university.'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_chain.invoke(\"tell me the news from tsinghua university within last week?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for information about Professor Wei Xu from Tsinghua University and his publications.\n",
      "Action: Search\n",
      "Action Input: \"Professor Wei Xu Tsinghua University publications\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Wei Xu. Professor, IIIS and CollegeAI, Tsinghua University. Verified email at tsinghua.edu.cn - Homepage · Computer Science. ArticlesCited byPublic access ...', '(For a full list of publications, please see my Google Scholar Page.) Training Job Placement in Clusters with Statistical In-Network Aggregation. Bohan Zhao, ...', 'Wei XU | Cited by 722 | of Tsinghua University, Beijing (TH) | Read 29 publications | Contact Wei XU.', 'Publication Topics. Convolutional Layers,Convolutional Neural Network,Deep Learning,Deep Neural Network,Dice Similarity Coefficient,Feature Maps,Fully ...', 'List of computer science publications by Wei Xu.', \"Wei Xu's 6 research works with 114 citations, including: Multi-object events recognition from video sequences using extended finite state machine.\", \"Beijing Pioneer in Teacher's Ethics. 2018. Tsinghua University Lab Design Award (3rd class). 2018. Tsinghua University Excellent Performance Employee Award.\", 'View the Special Programme for Research & Training in Tropical Diseases profile of Jian-wei Xu. Including their publications and grants.', 'Xu Wei, research professor, received his PhD degree from University of Chinese Academy of Sciences in 2016. · Working Experience · 2020.3-present, Agricultural ...', 'Xu received her Ph.D. in Computer Science from New York University, and her B.S. and M.S. from Tsinghua University. Her research interests are in natural ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: Professor Wei Xu is a faculty member at the Institute for Interdisciplinary Information Sciences (IIIS) and CollegeAI at Tsinghua University. His research interests are in computer science, particularly in areas such as deep learning, convolutional neural networks, and feature maps. He has a significant number of publications and citations, and for a comprehensive list, one can refer to his Google Scholar page. Some of his notable works include research on training job placement in clusters with statistical in-network aggregation. Additionally, Professor Xu has received several awards, including the Beijing Pioneer in Teacher's Ethics and the Tsinghua University Lab Design Award.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'tell me about professor Wei Xu of Tsinghua University including his publications.',\n",
       " 'output': \"Professor Wei Xu is a faculty member at the Institute for Interdisciplinary Information Sciences (IIIS) and CollegeAI at Tsinghua University. His research interests are in computer science, particularly in areas such as deep learning, convolutional neural networks, and feature maps. He has a significant number of publications and citations, and for a comprehensive list, one can refer to his Google Scholar page. Some of his notable works include research on training job placement in clusters with statistical in-network aggregation. Additionally, Professor Xu has received several awards, including the Beijing Pioneer in Teacher's Ethics and the Tsinghua University Lab Design Award.\"}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### YOUR TASK ####\n",
    "# use the web search tool to find out about\n",
    "# prof. wei xu and his publication list.\n",
    "# Compare the results with the previous implementation. is it better or worse?\n",
    "agent_chain.invoke(\n",
    "    \"tell me about professor Wei Xu of Tsinghua University including his publications.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Use one of the built-in tool for langchain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to search for the latest papers on machine learning on arxiv.org.\n",
      "Action: arxiv\n",
      "Action Input: \"machine learning\" latest\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mSince the initial search did not yield any results, I should try a more specific or slightly different query to find relevant papers.\n",
      "Action: arxiv\n",
      "Action Input: \"machine learning\" recent\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt seems that the search is not yielding any specific results, possibly due to the broad nature of the query or the way the API is returning results. I should try a more specific aspect of machine learning or a different keyword.\n",
      "Action: arxiv\n",
      "Action Input: \"deep learning\" recent\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNo good Arxiv Result was found\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mIt appears that the API is not returning any specific results, which might be due to a limitation in the API or the way the search is being conducted. I will try a more general search to see if I can get any results.\n",
      "Action: arxiv\n",
      "Action Input: \"machine learning\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mPublished: 2019-09-08\n",
      "Title: Lecture Notes: Optimization for Machine Learning\n",
      "Authors: Elad Hazan\n",
      "Summary: Lecture notes on optimization for machine learning, derived from a course at\n",
      "Princeton University and tutorials given in MLSS, Buenos Aires, as well as\n",
      "Simons Foundation, Berkeley.\n",
      "\n",
      "Published: 2018-11-11\n",
      "Title: An Optimal Control View of Adversarial Machine Learning\n",
      "Authors: Xiaojin Zhu\n",
      "Summary: I describe an optimal control view of adversarial machine learning, where the\n",
      "dynamical system is the machine learner, the input are adversarial actions, and\n",
      "the control costs are defined by the adversary's goals to do harm and be hard\n",
      "to detect. This view encompasses many types of adversarial machine learning,\n",
      "including test-item attacks, training-data poisoning, and adversarial reward\n",
      "shaping. The view encourages adversarial machine learning researcher to utilize\n",
      "advances in control theory and reinforcement learning.\n",
      "\n",
      "Published: 2017-07-16\n",
      "Title: Minimax deviation strategies for machine learning and recognition with short learning samples\n",
      "Authors: Michail Schlesinger, Evgeniy Vodolazskiy\n",
      "Summary: The article is devoted to the problem of small learning samples in machine\n",
      "learning. The flaws of maximum likelihood learning and minimax learning are\n",
      "looked into and the concept of minimax deviation learning is introduced that is\n",
      "free of those flaws.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThe search results are not the latest papers, as they are from 2017 to 2019. However, these papers provide valuable insights into specific areas of machine learning, such as optimization, adversarial machine learning, and strategies for learning with small datasets. Since the API is not returning more recent results, I will provide the most recent paper from the available results.\n",
      "Final Answer: The most recent paper from the available results is \"Lecture Notes: Optimization for Machine Learning\" by Elad Hazan, published in 2019. This paper provides lecture notes on optimization techniques for machine learning, derived from a course at Princeton University and tutorials given at MLSS, Buenos Aires, and the Simons Foundation, Berkeley. For the latest research, you may want to check the arXiv website directly or use a more specific search term.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'find the latest papers on the topic of machine learning.',\n",
       " 'output': 'The most recent paper from the available results is \"Lecture Notes: Optimization for Machine Learning\" by Elad Hazan, published in 2019. This paper provides lecture notes on optimization techniques for machine learning, derived from a course at Princeton University and tutorials given at MLSS, Buenos Aires, and the Simons Foundation, Berkeley. For the latest research, you may want to check the arXiv website directly or use a more specific search term.'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### your task ####\n",
    "# use a built-in tool in langchain to complete a task of your choice.\n",
    "# in the comment, please describe the task and the tool you used.\n",
    "# optional: try to use more than one tool in the same agent\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are very powerful assistant who can use tools to help the user.\",\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(\n",
    "            variable_name=\"agent_scratchpad\"\n",
    "        ),  # used to store the previous agent tool invocations and the corresponding tool outputs.\n",
    "    ]\n",
    ")\n",
    "\n",
    "tools = load_tools([\"arxiv\"])\n",
    "\n",
    "agent_chain = initialize_agent(\n",
    "    tools,\n",
    "    chat_model,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    prompt_template=prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_chain.invoke(\"find the latest papers on the topic of machine learning.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Lab 8 Supervised Fine Tuning"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1 Prepare the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In this section, we will generate instruction data and use them to do the Supervised Fine-tuning of a pre-trained Llama3-8B-instruct model.  \n", "\n", "First, we will use THU Chinese Classical Poetry Corpus (THU-CCPC) as our resource to generate instruction data. THU-CCPC is a part of THUNLP-AIPoet, which is a long-term project for AI generated Chinese poetry.\n", "\n", "The data in THU-CCPC is just base information of poems, so the first step is to preprocess the data and extract the necessary information. We will use the following steps to preprocess the data:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, let's first set the working directory."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["%cd /gfshome"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# dataset downloaded from \n", "# https://github.com/THUNLP-AIPoet/Datasets.git\n", "\n", "# we have already downloaded the dataset and put it in /ssdshare/share/lab8/Datasets\n", "# let's link it to the working directory for convenience\n", "\n", "# create a directory for processed output\n", "!mkdir ccpc"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# let's examine the input file\n", "!head -20 /ssdshare/share/lab8/Datasets/CCPC/ccpc_train_v1.0.json\n", "# This code transforms the CCPC dataset to a more readable and usable format.  "]}, {"cell_type": "code", "execution_count": 31, "metadata": {}, "outputs": [], "source": ["# This code transforms the CCPC dataset to a more friendly JSON format, \n", "# keeping only fields we need.\n", "# also we need to distinguish \u4e94\u8a00\u8bd7 from \u4e03\u8a00\u8bd7\n", "import json\n", "\n", "# Define input and output files\n", "input_file = \"/ssdshare/share/lab8/Datasets/CCPC/ccpc_train_v1.0.json\"\n", "output_file = \"ccpc/ccpc_transformed_with_format.json\"\n", "\n", "transformed_data = []\n", "\n", "# Load and transform data\n", "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n", "    for line in f:\n", "        if line.strip():  # Skip empty lines\n", "            item = json.loads(line.strip())  # Load each line as a JSON object\n", "            \n", "            # Process the content of the poem\n", "            lines = item[\"content\"].split(\"|\")\n", "            formatted_lines = [f\"{line}\uff0c\" if i % 2 == 0 else f\"{line}\u3002\" for i, line in enumerate(lines)]\n", "            formatted_content = \"\\n\".join(formatted_lines)\n", "\n", "            # Test the type of the poem\n", "            line_lengths = [len(line.replace(\"\uff0c\", \"\").replace(\"\u3002\", \"\")) for line in lines]\n", "            if all(length == 5 for length in line_lengths):\n", "                poem_type = \"\u4e94\u8a00\u8bd7\"\n", "            elif all(length == 7 for length in line_lengths):\n", "                poem_type = \"\u4e03\u8a00\u8bd7\"\n", "            else:\n", "                poem_type = \"\u6742\u8a00\u8bd7\"\n", "\n", "            # Transform the data structure\n", "            transformed_item = {\n", "                \"title\": item[\"title\"],\n", "                \"author\": item[\"author\"],\n", "                \"content\": formatted_content,\n", "                \"keywords\": item[\"keywords\"].split(),\n", "                \"poem_type\": poem_type\n", "            }\n", "            transformed_data.append(transformed_item)\n", "\n", "# Save the transformed data to output file\n", "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n", "\n", "# check if there are mixed poems\n", "def check_for_mixed_poems(transformed_data):\n", "    has_mixed_poems = any(item[\"poem_type\"] == \"\u6742\u8a00\u8bd7\" for item in transformed_data)\n", "    return has_mixed_poems\n", "\n", "\n", "if check_for_mixed_poems(transformed_data):\n", "    print(\"There are mixed poems in the transformed data.\"  )\n", "else:\n", "    print(\"There are no mixed poems in the transformed data.\")\n", "\n", "print(\"Transformation complete! Transformed data saved to:\", output_file)"]}, {"cell_type": "code", "execution_count": 32, "metadata": {}, "outputs": [], "source": ["# take a look at the resulting data\n", "!tail -50 ccpc/ccpc_transformed_with_format.json"]}, {"cell_type": "code", "execution_count": 33, "metadata": {}, "outputs": [], "source": ["# turn the labelled dataset into a SFT format\n", "# i.e. becomes question-answer pairs. \n", "\n", "import json\n", "import random\n", "\n", "# Define input and output files\n", "input_file = \"ccpc/ccpc_transformed_with_format.json\"\n", "output_file = \"LLaMA-Factory/data/alpaca_sft_dataset_with_varied_instructions.json\" # Save in Llama-Factory/data for use\n", "\n", "#Only use 5-character and 7-character poems\n", "def filter_poems(poem):\n", "    return poem[\"poem_type\"] in [\"\u4e94\u8a00\u8bd7\", \"\u4e03\u8a00\u8bd7\"]\n", "\n", "# Define filtering function \n", "def generate_instruction(poem_type, theme):\n", "    poem_type_map = {\n", "        \"\u4e94\u8a00\u8bd7\": \"5-character\",\n", "        \"\u4e03\u8a00\u8bd7\": \"7-character\"\n", "    }\n", "    english_poem_type = poem_type_map.get(poem_type, \"unknown\")\n", "    themes = \", \".join(theme)\n", "    \n", "    # Define instruction templates in five different styles\n", "    instruction_templates = [\n", "        f\"Hi, you are a Chinese poet, can you write a {english_poem_type} 4-line poem about the themes: {themes}?\",\n", "        f\"Hi, you are a Chinese poet now, can you compose a {english_poem_type} 4-line poem reflecting on the ideas of {themes}?\",\n", "        f\"Hi, as a Chinese poet, can you help me to create a {english_poem_type} 4-line poem that incorporates the themes of {themes}?\",\n", "        f\"Hi, please draft a {english_poem_type} 4-line poem based on the themes: {themes}.\",\n", "        f\"Hi, you are a Chinese poet, please generate a {english_poem_type} 4-line poem exploring the themes of {themes}.\"\n", "    ]\n", "    return random.choice(instruction_templates)  # Choose a random instruction template\n", "\n", "# Transform poem data into Alpaca format\n", "def create_alpaca_data(poem):\n", "    theme = poem[\"keywords\"]\n", "    instruction = generate_instruction(poem[\"poem_type\"], theme)\n", "    return {\n", "        \"instruction\": instruction,\n", "        \"input\": \"\",  # Unnecessary\n", "        \"output\": poem[\"content\"] \n", "    }\n", "\n", "# Load poem data and filter out unsuitable poems\n", "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n", "    poems = json.load(f)\n", "filtered_poems = [poem for poem in poems if filter_poems(poem)]\n", "alpaca_data = [create_alpaca_data(poem) for poem in filtered_poems]\n", "\n", "# Saved as Alpaca format\n", "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(alpaca_data, f, ensure_ascii=False, indent=4)\n", "\n", "print(f\"Alpaca dataset with varied instructions created successfully! Saved to {output_file}\")\n"]}, {"cell_type": "code", "execution_count": 34, "metadata": {}, "outputs": [], "source": ["# take a look at the result data\n", "\n", "!head -20 LLaMA-Factory/data/alpaca_sft_dataset_with_varied_instructions.json"]}, {"cell_type": "code", "execution_count": 35, "metadata": {}, "outputs": [], "source": ["# Now we already got the data set for SFT, but in Llama-Factory, we also need to register it.\n", "# datasets needs to be registered in LLaMA-Factory/data/dataset_info.json\n", "# let's take a look at the dataset_info.json first\n", "!head -100 LLaMA-Factory/data/dataset_info.json"]}, {"cell_type": "code", "execution_count": 36, "metadata": {}, "outputs": [], "source": ["# Now we add our SFT data to the  LLaMA-Factory/data/dataset_info.json:\n", "# poet_instructions = {\n", "#     \"file_name\": \"alpaca_sft_dataset_with_varied_instructions.json\",\n", "#     \"formatting\": \"alpaca\"\n", "# }\n", "\n", "import os\n", "import json\n", "\n", "# Path to the dataset_info.json file\n", "dataset_info_path = os.path.join(\"LLaMA-Factory\", \"data\", \"dataset_info.json\")\n", "\n", "# Read the existing dataset_info.json\n", "with open(dataset_info_path, \"r\", encoding=\"utf-8\") as f:\n", "    dataset_info = json.load(f)\n", "\n", "# Add the poet_instructions entry\n", "dataset_info[\"poet_instructions\"] = {\n", "    \"file_name\": \"alpaca_sft_dataset_with_varied_instructions.json\",\n", "    \"formatting\": \"alpaca\"\n", "}\n", "\n", "# Write the updated dataset_info.json\n", "with open(dataset_info_path, \"w\", encoding=\"utf-8\") as f:\n", "    json.dump(dataset_info, f, ensure_ascii=False, indent=4)\n", "\n", "print(f\"Updated {dataset_info_path} with poet_instructions dataset information.\")\n"]}, {"cell_type": "code", "execution_count": 37, "metadata": {}, "outputs": [], "source": ["# ensure the dataset is registered\n", "!tail -10 LLaMA-Factory/data/dataset_info.json"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we have the dataset ready, lets go back to 01_llama factory.ipynb. :)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 8 Supervised Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will generate instruction data and use them to do the Supervised Fine-tuning of a pre-trained Llama3-8B-instruct model.  \n",
    "\n",
    "First, we will use THU Chinese Classical Poetry Corpus (THU-CCPC) as our resource to generate instruction data. THU-CCPC is a part of THUNLP-AIPoet, which is a long-term project for AI generated Chinese poetry.\n",
    "\n",
    "The data in THU-CCPC is just base information of poems, so the first step is to preprocess the data and extract the necessary information. We will use the following steps to preprocess the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's first set the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gfshome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gfshome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset downloaded from \n",
    "# https://github.com/THUNLP-AIPoet/Datasets.git\n",
    "\n",
    "# we have already downloaded the dataset and put it in /ssdshare/share/lab8/Datasets\n",
    "# let's link it to the working directory for convenience\n",
    "\n",
    "# create a directory for processed output\n",
    "!mkdir ccpc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"dynasty\": \"Ming\", \"author\": \"翁万达\", \"content\": \"崖悬百尺古|面削一屏开|晴日流丹草|春风长绿苔\", \"title\": \"锦屏岩\", \"keywords\": \"屏开 晴日 春风 绿苔\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"童冀\", \"content\": \"每忆宋夫子|终年坐北轩|著书良自苦|得意好忘言\", \"title\": \"次胡仲申先生斋居述怀韵十首兼简宋景濂先生 其八\", \"keywords\": \"著书 终年 好 忘言\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"管讷\", \"content\": \"劝酒重持杯|杯深喜不辞|愿将今日意|同保百年期\", \"title\": \"初度日复呈兄勉翁三首 其二\", \"keywords\": \"劝酒 杯深 持杯 愿将\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"汪应辰\", \"content\": \"仁心均动植|风化正邦家|福庆方骈集|灵符尚辟邪\", \"title\": \"太上皇后合端午帖子词 其二\", \"keywords\": \"均 风化 灵符 邦家\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"蒲寿宬\", \"content\": \"骤来惊辟易|久视益虚无|咫尺星堪摘|波摇又走珠\", \"title\": \"心泉二首 其二\", \"keywords\": \"波摇 星 辟易 咫尺\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"袁说友\", \"content\": \"红妆夸睡足|粉额趁颜开|惟有江梅样|蛾眉淡拂来\", \"title\": \"用杨诚斋韵再题欧阳长老墨梅 其一\", \"keywords\": \"红妆 江梅 蛾眉 睡足\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"毕自严\", \"content\": \"屈指归来日|竹桃正著花|还应识故主|烂漫吐红霞\", \"title\": \"屈指归来日效白体七首 其七\", \"keywords\": \"屈指 红霞 识 著花\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"王致\", \"content\": \"年近古稀有|不易升此堂|他日留泉下|须留姓氏香\", \"title\": \"辞本州教官\", \"keywords\": \"古稀 他日 姓氏 泉下\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"姜特立\", \"content\": \"滴水添壶箭|跳丸叠崦嵫|只夸颜似玉|不觉鬓成丝\", \"title\": \"有感\", \"keywords\": \"添 成丝 鬓 颜\"}\n",
      "{\"dynasty\": \"Tang\", \"author\": \"白居易\", \"content\": \"吟咏霜毛句|闲尝雪水茶|城中展眉处|只是有元家\", \"title\": \"吟元郎中白须诗兼饮雪水茶因题壁上\", \"keywords\": \"霜毛 吟咏 句 城中\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"叶茵\", \"content\": \"山下水一泓|山上云一朵|有尘飞不来|终年风月我\", \"title\": \"壶中林壑\", \"keywords\": \"终年 风月 山下 我\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"卢龙云\", \"content\": \"高田二麦青|低田水初绿|邻翁唤赛神|枝头鸣布谷\", \"title\": \"题小景八首 其六\", \"keywords\": \"唤 布谷 枝头 邻翁\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"刘崧\", \"content\": \"玉塞三千里|天闲十二重|惟应霜骨出|识得是飞龙\", \"title\": \"题瘦马图\", \"keywords\": \"天闲 玉塞 飞龙 重\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"刘崧\", \"content\": \"出门复入门|望君还忆君|可怜飞鸟尽|江上又斜曛\", \"title\": \"和郭庆守秋日相忆\", \"keywords\": \"入门 还忆 飞鸟 出门\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"徐熥\", \"content\": \"岘山奇境饶|古洞藏幽谷|云深不可寻|但借玄猿宿\", \"title\": \"锦溪八景为林熙吉题 其六 宿猿洞\", \"keywords\": \"岘山 云深 幽谷 古洞\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"刘子翚\", \"content\": \"虽无适口味|暖益功稀比|菜苦不登盘|言中多逆耳\", \"title\": \"园蔬十咏 其十 苦益\", \"keywords\": \"登盘 功 多 味\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"谢榛\", \"content\": \"海日上孤帆|山云交杂树|美人方晓眠|更在林深处\", \"title\": \"潞阳晓访冯员外汝言 其一\", \"keywords\": \"孤帆 山云 海日 美人\"}\n",
      "{\"dynasty\": \"Song\", \"author\": \"陆游\", \"content\": \"远客厌征路|流年逢素秋|不知今夜月|还照几人愁\", \"title\": \"对月二首 其一\", \"keywords\": \"素秋 还照 流年 远客\"}\n",
      "{\"dynasty\": \"Ming\", \"author\": \"蒋主孝\", \"content\": \"昨夜秋风至|庭前落叶多|遥怜未归客|相见欲如何\", \"title\": \"秋风引\", \"keywords\": \"秋风 落叶 昨夜 遥怜\"}\n",
      "{\"dynasty\": \"Yuan\", \"author\": \"黄镇成\", \"content\": \"趺坐傍云根|圆空慧性存|法华堆自展|难与世人论\", \"title\": \"老僧翻经\", \"keywords\": \"世人 论 云根 存\"}\n"
     ]
    }
   ],
   "source": [
    "# let's examine the input file\n",
    "!head -20 /ssdshare/share/lab8/Datasets/CCPC/ccpc_train_v1.0.json\n",
    "# This code transforms the CCPC dataset to a more readable and usable format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are no mixed poems in the transformed data.\n",
      "Transformation complete! Transformed data saved to: ccpc/ccpc_transformed_with_format.json\n"
     ]
    }
   ],
   "source": [
    "# This code transforms the CCPC dataset to a more friendly JSON format, \n",
    "# keeping only fields we need.\n",
    "# also we need to distinguish 五言诗 from 七言诗\n",
    "import json\n",
    "\n",
    "# Define input and output files\n",
    "input_file = \"/ssdshare/share/lab8/Datasets/CCPC/ccpc_train_v1.0.json\"\n",
    "output_file = \"ccpc/ccpc_transformed_with_format.json\"\n",
    "\n",
    "transformed_data = []\n",
    "\n",
    "# Load and transform data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if line.strip():  # Skip empty lines\n",
    "            item = json.loads(line.strip())  # Load each line as a JSON object\n",
    "            \n",
    "            # Process the content of the poem\n",
    "            lines = item[\"content\"].split(\"|\")\n",
    "            formatted_lines = [f\"{line}，\" if i % 2 == 0 else f\"{line}。\" for i, line in enumerate(lines)]\n",
    "            formatted_content = \"\\n\".join(formatted_lines)\n",
    "\n",
    "            # Test the type of the poem\n",
    "            line_lengths = [len(line.replace(\"，\", \"\").replace(\"。\", \"\")) for line in lines]\n",
    "            if all(length == 5 for length in line_lengths):\n",
    "                poem_type = \"五言诗\"\n",
    "            elif all(length == 7 for length in line_lengths):\n",
    "                poem_type = \"七言诗\"\n",
    "            else:\n",
    "                poem_type = \"杂言诗\"\n",
    "\n",
    "            # Transform the data structure\n",
    "            transformed_item = {\n",
    "                \"title\": item[\"title\"],\n",
    "                \"author\": item[\"author\"],\n",
    "                \"content\": formatted_content,\n",
    "                \"keywords\": item[\"keywords\"].split(),\n",
    "                \"poem_type\": poem_type\n",
    "            }\n",
    "            transformed_data.append(transformed_item)\n",
    "\n",
    "# Save the transformed data to output file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(transformed_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# check if there are mixed poems\n",
    "def check_for_mixed_poems(transformed_data):\n",
    "    has_mixed_poems = any(item[\"poem_type\"] == \"杂言诗\" for item in transformed_data)\n",
    "    return has_mixed_poems\n",
    "\n",
    "\n",
    "if check_for_mixed_poems(transformed_data):\n",
    "    print(\"There are mixed poems in the transformed data.\"  )\n",
    "else:\n",
    "    print(\"There are no mixed poems in the transformed data.\")\n",
    "\n",
    "print(\"Transformation complete! Transformed data saved to:\", output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    },\n",
      "    {\n",
      "        \"title\": \"和韵题雨竹并菊\",\n",
      "        \"author\": \"黄仲昭\",\n",
      "        \"content\": \"雨过琅玕湿未乾，\\n霜馀黄菊尚堪餐。\\n他年何处偏思尔，\\n紫陌鸡声旅梦残。\",\n",
      "        \"keywords\": [\n",
      "            \"他年\",\n",
      "            \"琅玕\",\n",
      "            \"黄菊\",\n",
      "            \"雨过\"\n",
      "        ],\n",
      "        \"poem_type\": \"七言诗\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"燕都杂题三首 其三\",\n",
      "        \"author\": \"何失\",\n",
      "        \"content\": \"花市东边柳市西，\\n矮堂一笑百金挥。\\n如今踪迹无寻处，\\n谷雨绵山燕子飞。\",\n",
      "        \"keywords\": [\n",
      "            \"东边\",\n",
      "            \"谷雨\",\n",
      "            \"燕子\",\n",
      "            \"踪迹\"\n",
      "        ],\n",
      "        \"poem_type\": \"七言诗\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"久雨\",\n",
      "        \"author\": \"陈高\",\n",
      "        \"content\": \"新年新雨连残腊，\\n一月浑无一日晴。\\n晓起莺声都寂寞，\\n寒深柳眼未分明。\",\n",
      "        \"keywords\": [\n",
      "            \"莺声\",\n",
      "            \"新年\",\n",
      "            \"分明\",\n",
      "            \"寂寞\"\n",
      "        ],\n",
      "        \"poem_type\": \"七言诗\"\n",
      "    },\n",
      "    {\n",
      "        \"title\": \"山谷草圣\",\n",
      "        \"author\": \"廖大圭\",\n",
      "        \"content\": \"涪翁醉墨动惊蛇，\\n流落人间几岁华。\\n长恐六丁起雷电，\\n为龙飞去玉皇家。\",\n",
      "        \"keywords\": [\n",
      "            \"玉皇\",\n",
      "            \"流落\",\n",
      "            \"岁华\",\n",
      "            \"飞去\"\n",
      "        ],\n",
      "        \"poem_type\": \"七言诗\"\n",
      "    }\n",
      "]"
     ]
    }
   ],
   "source": [
    "# take a look at the resulting data\n",
    "!tail -50 ccpc/ccpc_transformed_with_format.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpaca dataset with varied instructions created successfully! Saved to LLaMA-Factory/data/alpaca_sft_dataset_with_varied_instructions.json\n"
     ]
    }
   ],
   "source": [
    "# turn the labelled dataset into a SFT format\n",
    "# i.e. becomes question-answer pairs. \n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Define input and output files\n",
    "input_file = \"ccpc/ccpc_transformed_with_format.json\"\n",
    "output_file = \"LLaMA-Factory/data/alpaca_sft_dataset_with_varied_instructions.json\" # Save in Llama-Factory/data for use\n",
    "\n",
    "#Only use 5-character and 7-character poems\n",
    "def filter_poems(poem):\n",
    "    return poem[\"poem_type\"] in [\"五言诗\", \"七言诗\"]\n",
    "\n",
    "# Define filtering function \n",
    "def generate_instruction(poem_type, theme):\n",
    "    poem_type_map = {\n",
    "        \"五言诗\": \"5-character\",\n",
    "        \"七言诗\": \"7-character\"\n",
    "    }\n",
    "    english_poem_type = poem_type_map.get(poem_type, \"unknown\")\n",
    "    themes = \", \".join(theme)\n",
    "    \n",
    "    # Define instruction templates in five different styles\n",
    "    instruction_templates = [\n",
    "        f\"Hi, you are a Chinese poet, can you write a {english_poem_type} 4-line poem about the themes: {themes}?\",\n",
    "        f\"Hi, you are a Chinese poet now, can you compose a {english_poem_type} 4-line poem reflecting on the ideas of {themes}?\",\n",
    "        f\"Hi, as a Chinese poet, can you help me to create a {english_poem_type} 4-line poem that incorporates the themes of {themes}?\",\n",
    "        f\"Hi, please draft a {english_poem_type} 4-line poem based on the themes: {themes}.\",\n",
    "        f\"Hi, you are a Chinese poet, please generate a {english_poem_type} 4-line poem exploring the themes of {themes}.\"\n",
    "    ]\n",
    "    return random.choice(instruction_templates)  # Choose a random instruction template\n",
    "\n",
    "# Transform poem data into Alpaca format\n",
    "def create_alpaca_data(poem):\n",
    "    theme = poem[\"keywords\"]\n",
    "    instruction = generate_instruction(poem[\"poem_type\"], theme)\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": \"\",  # Unnecessary\n",
    "        \"output\": poem[\"content\"] \n",
    "    }\n",
    "\n",
    "# Load poem data and filter out unsuitable poems\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    poems = json.load(f)\n",
    "filtered_poems = [poem for poem in poems if filter_poems(poem)]\n",
    "alpaca_data = [create_alpaca_data(poem) for poem in filtered_poems]\n",
    "\n",
    "# Saved as Alpaca format\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(alpaca_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Alpaca dataset with varied instructions created successfully! Saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"instruction\": \"Hi, as a Chinese poet, can you help me to create a 5-character 4-line poem that incorporates the themes of 屏开, 晴日, 春风, 绿苔?\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"崖悬百尺古，\\n面削一屏开。\\n晴日流丹草，\\n春风长绿苔。\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Hi, please draft a 5-character 4-line poem based on the themes: 著书, 终年, 好, 忘言.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"每忆宋夫子，\\n终年坐北轩。\\n著书良自苦，\\n得意好忘言。\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Hi, please draft a 5-character 4-line poem based on the themes: 劝酒, 杯深, 持杯, 愿将.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"劝酒重持杯，\\n杯深喜不辞。\\n愿将今日意，\\n同保百年期。\"\n",
      "    },\n",
      "    {\n",
      "        \"instruction\": \"Hi, you are a Chinese poet, please generate a 5-character 4-line poem exploring the themes of 均, 风化, 灵符, 邦家.\",\n",
      "        \"input\": \"\",\n",
      "        \"output\": \"仁心均动植，\\n风化正邦家。\\n福庆方骈集，\\n灵符尚辟邪。\"\n"
     ]
    }
   ],
   "source": [
    "# take a look at the result data\n",
    "\n",
    "!head -20 LLaMA-Factory/data/alpaca_sft_dataset_with_varied_instructions.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"identity\": {\n",
      "    \"file_name\": \"identity.json\"\n",
      "  },\n",
      "  \"alpaca_en_demo\": {\n",
      "    \"file_name\": \"alpaca_en_demo.json\"\n",
      "  },\n",
      "  \"alpaca_zh_demo\": {\n",
      "    \"file_name\": \"alpaca_zh_demo.json\"\n",
      "  },\n",
      "  \"glaive_toolcall_en_demo\": {\n",
      "    \"file_name\": \"glaive_toolcall_en_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"conversations\",\n",
      "      \"tools\": \"tools\"\n",
      "    }\n",
      "  },\n",
      "  \"glaive_toolcall_zh_demo\": {\n",
      "    \"file_name\": \"glaive_toolcall_zh_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"conversations\",\n",
      "      \"tools\": \"tools\"\n",
      "    }\n",
      "  },\n",
      "  \"mllm_demo\": {\n",
      "    \"file_name\": \"mllm_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"messages\",\n",
      "      \"images\": \"images\"\n",
      "    },\n",
      "    \"tags\": {\n",
      "      \"role_tag\": \"role\",\n",
      "      \"content_tag\": \"content\",\n",
      "      \"user_tag\": \"user\",\n",
      "      \"assistant_tag\": \"assistant\"\n",
      "    }\n",
      "  },\n",
      "  \"mllm_audio_demo\": {\n",
      "    \"file_name\": \"mllm_audio_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"messages\",\n",
      "      \"audios\": \"audios\"\n",
      "    },\n",
      "    \"tags\": {\n",
      "      \"role_tag\": \"role\",\n",
      "      \"content_tag\": \"content\",\n",
      "      \"user_tag\": \"user\",\n",
      "      \"assistant_tag\": \"assistant\"\n",
      "    }\n",
      "  },\n",
      "  \"mllm_video_demo\": {\n",
      "    \"file_name\": \"mllm_video_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"messages\",\n",
      "      \"videos\": \"videos\"\n",
      "    },\n",
      "    \"tags\": {\n",
      "      \"role_tag\": \"role\",\n",
      "      \"content_tag\": \"content\",\n",
      "      \"user_tag\": \"user\",\n",
      "      \"assistant_tag\": \"assistant\"\n",
      "    }\n",
      "  },\n",
      "  \"mllm_video_audio_demo\": {\n",
      "    \"file_name\": \"mllm_video_audio_demo.json\",\n",
      "    \"formatting\": \"sharegpt\",\n",
      "    \"columns\": {\n",
      "      \"messages\": \"messages\",\n",
      "      \"videos\": \"videos\",\n",
      "      \"audios\": \"audios\"\n",
      "    },\n",
      "    \"tags\": {\n",
      "      \"role_tag\": \"role\",\n",
      "      \"content_tag\": \"content\",\n",
      "      \"user_tag\": \"user\",\n",
      "      \"assistant_tag\": \"assistant\"\n",
      "    }\n",
      "  },\n",
      "  \"alpaca_en\": {\n",
      "    \"hf_hub_url\": \"llamafactory/alpaca_en\",\n",
      "    \"ms_hub_url\": \"llamafactory/alpaca_en\",\n",
      "    \"om_hub_url\": \"HaM/alpaca_en\"\n",
      "  },\n",
      "  \"alpaca_zh\": {\n",
      "    \"hf_hub_url\": \"llamafactory/alpaca_zh\",\n",
      "    \"ms_hub_url\": \"llamafactory/alpaca_zh\"\n",
      "  },\n",
      "  \"alpaca_gpt4_en\": {\n",
      "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_en\",\n",
      "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_en\"\n",
      "  },\n",
      "  \"alpaca_gpt4_zh\": {\n",
      "    \"hf_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
      "    \"ms_hub_url\": \"llamafactory/alpaca_gpt4_zh\",\n",
      "    \"om_hub_url\": \"State_Cloud/alpaca-gpt4-data-zh\"\n"
     ]
    }
   ],
   "source": [
    "# Now we already got the data set for SFT, but in Llama-Factory, we also need to register it.\n",
    "# datasets needs to be registered in LLaMA-Factory/data/dataset_info.json\n",
    "# let's take a look at the dataset_info.json first\n",
    "!head -100 LLaMA-Factory/data/dataset_info.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated LLaMA-Factory/data/dataset_info.json with poet_instructions dataset information.\n"
     ]
    }
   ],
   "source": [
    "# Now we add our SFT data to the  LLaMA-Factory/data/dataset_info.json:\n",
    "# poet_instructions = {\n",
    "#     \"file_name\": \"alpaca_sft_dataset_with_varied_instructions.json\",\n",
    "#     \"formatting\": \"alpaca\"\n",
    "# }\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Path to the dataset_info.json file\n",
    "dataset_info_path = os.path.join(\"LLaMA-Factory\", \"data\", \"dataset_info.json\")\n",
    "\n",
    "# Read the existing dataset_info.json\n",
    "with open(dataset_info_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset_info = json.load(f)\n",
    "\n",
    "# Add the poet_instructions entry\n",
    "dataset_info[\"poet_instructions\"] = {\n",
    "    \"file_name\": \"alpaca_sft_dataset_with_varied_instructions.json\",\n",
    "    \"formatting\": \"alpaca\"\n",
    "}\n",
    "\n",
    "# Write the updated dataset_info.json\n",
    "with open(dataset_info_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_info, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Updated {dataset_info_path} with poet_instructions dataset information.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \"columns\": {\n",
      "            \"prompt\": \"content\"\n",
      "        },\n",
      "        \"folder\": \"python\"\n",
      "    },\n",
      "    \"poet_instructions\": {\n",
      "        \"file_name\": \"alpaca_sft_dataset_with_varied_instructions.json\",\n",
      "        \"formatting\": \"alpaca\"\n",
      "    }\n",
      "}"
     ]
    }
   ],
   "source": [
    "# ensure the dataset is registered\n",
    "!tail -10 LLaMA-Factory/data/dataset_info.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the dataset ready, lets go back to 01_llama factory.ipynb. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

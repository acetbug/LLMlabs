### Note: DO NOT use quantized model or quantization_bit when merging lora adapters

### model
model_name_or_path: /ssdshare/share/Meta-Llama-3-8B-Instruct #Base model path
adapter_name_or_path: /gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-10-59-45 #Lora adapter path ***Please fullfill your own path here***
template: llama3
finetuning_type: lora

### export
export_dir: /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet #Your export directory
export_size: 4
export_device: cpu
export_legacy_format: false
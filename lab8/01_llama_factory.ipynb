{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Lab 8 Supervised Fine Tuning\n", "\n", "In this lab, we will perform\u00a0parameter efficient finetuning (PEFT) to finetune\u00a0a llama-2\u00a0model, using the HuggingFace SFTTrainer tool from its trl library."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Install dependencies"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# add proxy to access openai ...\n", "import os\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["!pip install -r requirements.txt\n", "\n", "#!mkdir -p /root/LLM-applications-course/lab8/LLaMA-Factory\n", "#!cd /root/LLM-applications-course/lab8/LLaMA-Factory/ && pip install -r /root/LLM-applications-course/lab8/requirements.txt"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's first change the working directory to /gfshome, to avoid writing too much data to the home directory. (Ignore the warnings)"]}, {"cell_type": "code", "execution_count": 28, "metadata": {}, "outputs": [], "source": ["# copy the config files to /gfshome, the working directory (will need later)\n", "!ln -s *.yaml /gfshome/"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["%cd /gfshome"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["#download llama factory\n", "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n", "!cd LLaMA-Factory && pip install -e \".[torch,metrics]\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2 Supervised Fine Tuning Example\n", "### 2.1 Motivation\n", "Llama3 is a versatile large language model available in various parameter sizes. Given its significant improvements in text generation tasks compared to its predecessor, Llama2, we aim to use Llama3-8B-Instruct to generate Chinese poetry based on specific themes."]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["################################################################################\n", "# Shared parameters between inference and SFT training\n", "################################################################################\n", "\n", "import transformers\n", "import torch\n", "# The base model\n", "model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n", "# Use a single GPU\n", "# device_map = {'':0}\n", "# Use all GPUs\n", "device_map = \"auto\""]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["################################################################################\n", "# bitsandbytes parameters\n", "################################################################################\n", "from transformers import BitsAndBytesConfig\n", "\n", "bnb_config = BitsAndBytesConfig(\n", "    load_in_4bit= True,    # use 4-bit precision for base model loading\n", "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n", "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n", "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n", ")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import (\n", "    AutoModelForCausalLM,\n", "    AutoTokenizer,\n", "    TrainingArguments,\n", "    pipeline,\n", ")\n", "import os\n", "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n", "# Load base model with bnb config\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    quantization_config=bnb_config,\n", "    device_map=device_map\n", ")\n", "model.config.use_cache = False\n", "model.config.pretraining_tp = 1\n", "\n", "# Load LLaMA tokenizer\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"left\""]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of \u98ce\u96e8\uff0c\u65c5\u4eba?\" \n", "eos_ids = [\n", "    tokenizer.eos_token_id,\n", "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") \n", "]\n", "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, eos_token_id=eos_ids, num_return_sequences=1)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The output does not make any sense. Not only the number of characters in each line is not suffcient to our requirement, but also the tune and words used is not like ancient poet at all.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 Preparing the training dataset\n", "\n", "Let's use sft to improve Llama3-8B-Instruct's ablity in this field now!\n", "\n", "You should prepare for the data we need to use for SFT in `02_poet data` .\n", "\n", "Please complete the procedures in that notebook.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3 SFT with Llama-Factory\n", "\n", "For Processing SFT, we use llama factory, which is a highly modular, user-friendly platform with great ease of use, supporting distributed training and a variety of pre-trained models. Llama factory provide a WebUI to make it easy for using."]}, {"cell_type": "code", "execution_count": 22, "metadata": {}, "outputs": [], "source": ["import os\n", "os.environ['BNB_CUDA_VERSION'] = '125'\n", "!cd LLaMA-Factory && llamafactory-cli webui"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can find the training parameters we selected in the file `Llama3-8B-Instruct-sft.yaml`, or refer to the screenshot in the slides. After you fullfill the parameters, click `Start` and wait for the SFT process to complete.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After the training runs to complete, please paste your loss change chat below. \n"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["# You can now terminate the training process by stopping the previous cell.\n", "# The resulting LoRA is saved in LLaMA-Factory/saves/Llama-3-8B-Instruct/lora \n", "# (who is automatically named with a date as suffix)\n", "!ls LLaMA-Factory/saves/Llama-3-8B-Instruct/lora "]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Merging the LoRA into the new model."]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# Merge Lora_model with Base model and save the merged model\n", "# ***Update the Lora-Merge.yaml configuration file and fullfill the Lora Path***\n", "# For more options in export, please refer to the [Llama-Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/export.md)\n", "\n", "!llamafactory-cli export Lora_Merge.yaml"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.3 Testing the fine-tuned model"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["#Choose your Finetuneed model for test\n", "#Dont't forget to change the model name to your export_dir\n", "model_name = \"/gfshome/merged_model/Llama-3-8B-Instruct-sft-poet\"  # your new model \n", "device_map = \"auto\""]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["import os\n", "import transformers\n", "import torch\n", "from transformers import BitsAndBytesConfig\n", "\n", "bnb_config = BitsAndBytesConfig(\n", "    load_in_4bit= True,    # use 4-bit precision for base model loading\n", "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n", "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n", "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n", ")\n", "\n", "from transformers import (\n", "    AutoModelForCausalLM,\n", "    AutoTokenizer,\n", "    TrainingArguments,\n", "    pipeline,\n", ")\n", "\n", "os.environ['BNB_CUDA_VERSION'] = '125'\n", "\n", "# Load base model with bnb config\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_name,\n", "    quantization_config=bnb_config,\n", "    device_map=device_map\n", ")\n", "model.config.use_cache = False\n", "model.config.pretraining_tp = 1\n", "\n", "# Load LLaMA tokenizer\n", "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "tokenizer.pad_token = tokenizer.eos_token\n", "tokenizer.padding_side = \"left\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["And if you don't want to merge your lora to get a new model, you can just using the lora when inference:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# import os\n", "# import transformers\n", "# import torch\n", "# from transformers import BitsAndBytesConfig\n", "\n", "# model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n", "# device_map = \"auto\"\n", "# adapter_name_or_path = \"/gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-09-25-23\"\n", "\n", "# bnb_config = BitsAndBytesConfig(\n", "#     load_in_4bit= True,    # use 4-bit precision for base model loading\n", "#     bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n", "#     bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n", "#     bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n", "# )\n", "\n", "# from transformers import (\n", "#     AutoModelForCausalLM,\n", "#     AutoTokenizer,\n", "#     TrainingArguments,\n", "#     pipeline,\n", "# )\n", "# from peft import PeftModel\n", "\n", "# model = AutoModelForCausalLM.from_pretrained(\n", "#     model_name,\n", "#     quantization_config=bnb_config,\n", "#     device_map=device_map\n", "# )\n", "# model.config.use_cache = False\n", "# model.config.pretraining_tp = 1\n", "\n", "# model = PeftModel.from_pretrained(\n", "#     model,\n", "#     adapter_name_or_path, \n", "#     device_map=device_map\n", "# )\n", "\n", "# os.environ['BNB_CUDA_VERSION'] = '125'\n", "\n", "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n", "# tokenizer.pad_token = tokenizer.eos_token\n", "# tokenizer.padding_side = \"left\""]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of \u98ce\u96e8\uff0c\u65c5\u4eba?\" \n", "eos_ids = [\n", "    tokenizer.eos_token_id,\n", "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n", "]\n", "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: \u82b1\u5f00, \u6843\u6e90.\" \n", "eos_ids = [\n", "    tokenizer.eos_token_id,\n", "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n", "]\n", "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["# Run text generation pipeline with our next model\n", "prompt = \"Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of \u7f8e\u56fd\uff0c\u5173\u7a0e?\" \n", "eos_ids = [\n", "    tokenizer.eos_token_id,\n", "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # \u5982\u679c tokenizer \u652f\u6301\u8fd9\u4e2a token\n", "]\n", "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n", "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n", "print(result[0]['generated_text'])"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}
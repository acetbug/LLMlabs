{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 Supervised Fine Tuning\n",
    "\n",
    "In this lab, we will perform parameter efficient finetuning (PEFT) to finetune a llama-2 model, using the HuggingFace SFTTrainer tool from its trl library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add proxy to access openai ...\n",
    "import os\n",
    "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n",
    "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting trl (from -r requirements.txt (line 1))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/da/f2/6f47dd96314a281b45695da75e28ece3a9b55931f965587767fc374492a1/trl-0.17.0-py3-none-any.whl (348 kB)\n",
      "Collecting fire (from -r requirements.txt (line 2))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/6b/b6/82c7e601d6d3c3278c40b7bd35e17e82aa227f050aa9f66cb7b7fce29471/fire-0.7.0.tar.gz (87 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.45.5)\n",
      "Collecting deepspeed (from -r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4f/5c/7542904cddfaa50a9a7ae6770349d468773359f5af1718865452cea8729d/deepspeed-0.16.9.tar.gz (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m90.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (13.7.1)\n",
      "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.10/dist-packages (from trl->-r requirements.txt (line 1)) (4.51.0)\n",
      "Collecting termcolor (from fire->-r requirements.txt (line 2))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/4f/bd/de8d508070629b6d84a30d01d57e4a65c69aa7f5abe7560b8fad3b50ea59/termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (2.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from bitsandbytes->-r requirements.txt (line 3)) (1.26.4)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (0.8.0)\n",
      "Collecting hjson (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/1f/7f/13cd798d180af4bf4c0ceddeefba2b864a63c71645abc0308b768d67bb81/hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.0.8)\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (1.11.1.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (9.0.0)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (2.9.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed->-r requirements.txt (line 4)) (4.66.5)\n",
      "Collecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 4))\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/db/24/552ebea28f0570b9e65e62b50287a273804c9f997cc1c2dcd4e2d64b9e7d/nvidia_ml_py-12.575.51-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.34.0->trl->-r requirements.txt (line 1)) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.32.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.10.5)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0.0->deepspeed->-r requirements.txt (line 4)) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (2024.9.11)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.46.0->trl->-r requirements.txt (line 1)) (0.21.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl->-r requirements.txt (line 1)) (2.18.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (4.0.3)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl->-r requirements.txt (line 1)) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=3.0.0->trl->-r requirements.txt (line 1)) (1.16.0)\n",
      "Building wheels for collected packages: fire, deepspeed\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=13bc7ebf69c018214feb7a945a08921649818dbeda6e709245a6dc80e1bf35e2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vtnvp7qh/wheels/27/e3/ba/cb2b3c7af0f41c57b748dbdae18aabf0fd68f6469f7036eec3\n",
      "  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.16.9-py3-none-any.whl size=1644434 sha256=45c029ae3a94dd22abdb5d932a8dbe0f69f62170b6ba10312387710d00c4c630\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-vtnvp7qh/wheels/34/05/4e/c09f6f951ea7d7f76d7652878a23bd72f4dc5ef1afc3096f5b\n",
      "Successfully built fire deepspeed\n",
      "Installing collected packages: nvidia-ml-py, hjson, termcolor, fire, deepspeed, trl\n",
      "Successfully installed deepspeed-0.16.9 fire-0.7.0 hjson-3.1.0 nvidia-ml-py-12.575.51 termcolor-3.1.0 trl-0.17.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n",
    "\n",
    "#!mkdir -p /root/LLM-applications-course/lab8/LLaMA-Factory\n",
    "#!cd /root/LLM-applications-course/lab8/LLaMA-Factory/ && pip install -r /root/LLM-applications-course/lab8/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first change the working directory to /gfshome, to avoid writing too much data to the home directory. (Ignore the warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the config files to /gfshome, the working directory (will need later)\n",
    "!ln -s *.yaml /gfshome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gfshome\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd /gfshome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Factory'...\n",
      "warning: auto-detection of host provider took too long (>2000ms)\n",
      "warning: see https://aka.ms/gcm/autodetect for more information.\n",
      "warning: auto-detection of host provider took too long (>2000ms)\n",
      "warning: see https://aka.ms/gcm/autodetect for more information.\n",
      "remote: Enumerating objects: 23732, done.\u001b[K\n",
      "remote: Counting objects: 100% (50/50), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 23732 (delta 28), reused 17 (delta 17), pack-reused 23682 (from 2)\u001b[K\n",
      "Receiving objects: 100% (23732/23732), 48.60 MiB | 2.19 MiB/s, done.\n",
      "Resolving deltas: 100% (17171/17171), done.\n",
      "Updating files: 100% (290/290), done.\n",
      "Looking in indexes: https://mirrors.cernet.edu.cn/pypi/web/simple, https://pypi.ngc.nvidia.com\n",
      "Obtaining file:///gfshome/LLaMA-Factory\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Checking if build backend supports build_editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build editable ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing editable metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.3,>=4.45.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (4.51.0)\n",
      "Requirement already satisfied: datasets<=3.6.0,>=2.16.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: accelerate<=1.7.0,>=0.34.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.6.0)\n",
      "Requirement already satisfied: peft<=0.15.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.15.1)\n",
      "Collecting trl<=0.9.6,>=0.8.6 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a5/c3/6565c2c376a829f99da20d39c2912405195ec1fa6aae068dc45c46793e72/trl-0.9.6-py3-none-any.whl (245 kB)\n",
      "Requirement already satisfied: tokenizers<=0.21.1,>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.1)\n",
      "Requirement already satisfied: gradio<=5.31.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.23.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.14.0)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.8.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.2.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.9.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (5.29.4)\n",
      "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.34.0)\n",
      "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.115.9)\n",
      "Collecting sse-starlette (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c8/48/3e49cf0f64961656402c0023edbc51844fe17afe53ab50e958a6dbbbd499/sse_starlette-2.3.5-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: matplotlib>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.2)\n",
      "Requirement already satisfied: fire in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Collecting omegaconf (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/e3/94/1843518e420fa3ed6919835845df698c7e27e183cb997394e4a670973a65/omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (23.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (6.0.2)\n",
      "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (1.26.4)\n",
      "Requirement already satisfied: pydantic<=2.10.6 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.9.2)\n",
      "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.2.2)\n",
      "Collecting av (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/23/42/0eafe0de75de6a0db71add8e4ea51ebf090482bad3068f4a874c90fbd110/av-14.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.10.1)\n",
      "Collecting tyro<0.9.0 (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/60/ec/e34d546cfd9c5b906d1d534bb75557be9f2b179609d60bb9e97ec07e8ead/tyro-0.8.14-py3-none-any.whl (109 kB)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (2.6.0)\n",
      "Requirement already satisfied: torchvision>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (0.21.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from llamafactory==0.9.3.dev0) (3.9.1)\n",
      "Collecting jieba (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.2/19.2 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting rouge-chinese (from llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/03/0f/394cf877be7b903881020ef7217f7dc644dad158d52a9353fcab22e3464d/rouge_chinese-1.0.3-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (6.0.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.30.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate<=1.7.0,>=0.34.0->llamafactory==0.9.3.dev0) (0.4.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.10.5)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (23.2.1)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.6.0)\n",
      "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.8.0)\n",
      "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.27.2)\n",
      "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.1.5)\n",
      "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.10.16)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (10.4.0)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.0.20)\n",
      "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.11.4)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.45.3)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (0.12.5)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (4.12.2)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.8.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (15.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.7.0->llamafactory==0.9.3.dev0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->llamafactory==0.9.3.dev0) (2024.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<=2.10.6->llamafactory==0.9.3.dev0) (2.23.4)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->llamafactory==0.9.3.dev0) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->llamafactory==0.9.3.dev0) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,!=4.52.0,<=4.52.3,>=4.45.0->llamafactory==0.9.3.dev0) (2024.9.11)\n",
      "Collecting docstring-parser>=0.16 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/d5/7c/e9fcff7623954d86bdc17782036cbf715ecab1bec4847c008557affe1ca8/docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro<0.9.0->llamafactory==0.9.3.dev0) (13.7.1)\n",
      "Collecting shtab>=1.5.6 (from tyro<0.9.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/74/03/3271b7bb470fbab4adf5bd30b0d32143909d96f3608d815b447357f47f2b/shtab-1.7.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (8.1.7)\n",
      "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn->llamafactory==0.9.3.dev0) (0.14.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->llamafactory==0.9.3.dev0) (3.1.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.5.2)\n",
      "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.61.0)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.12.1)\n",
      "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->llamafactory==0.9.3.dev0) (1.0.8)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from omegaconf->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/3e/38/7859ff46355f76f8d19459005ca000b6e7012f2f1ca597746cbcd1fbfe5e/antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge-chinese->llamafactory==0.9.3.dev0) (1.16.0)\n",
      "Collecting anyio<5.0,>=3.0 (from gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0)\n",
      "  Downloading https://mirrors.tuna.tsinghua.edu.cn/pypi/web/packages/a1/ee/48ca1a7c89ffec8b6a0c5d02b89c305671d5ffd8d3c94acf8b8c408575bb/anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (4.0.3)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.0.6)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->llamafactory==0.9.3.dev0) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->llamafactory==0.9.3.dev0) (4.3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets<=3.6.0,>=2.16.0->llamafactory==0.9.3.dev0) (2.0.7)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (2.18.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa->llamafactory==0.9.3.dev0) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (1.17.0)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio<=5.31.0,>=4.38.0->llamafactory==0.9.3.dev0) (1.5.4)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->llamafactory==0.9.3.dev0) (2.22)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->llamafactory==0.9.3.dev0) (0.1.2)\n",
      "Building wheels for collected packages: llamafactory, jieba, antlr4-python3-runtime\n",
      "  Building editable for llamafactory (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llamafactory: filename=llamafactory-0.9.3.dev0-0.editable-py3-none-any.whl size=27145 sha256=8212e9831ec7a7d2ade12ac4d9f26e6b1e1da272d87e1849acea66156e8893e8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-11fsxcvz/wheels/87/26/82/8f4922c9e797dfc3e05b24c481d0e498ffae7c1e700eb2c667\n",
      "  Building wheel for jieba (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=801b442ec08dfb642d38c6f8caec16ed277e19b7ddea035003a596b2566b5066\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-11fsxcvz/wheels/39/b4/c0/d4cac07d2a3f979e02b19a6fb22bd67a4750ccf231e795ebef\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144551 sha256=404f7fc5055d0c2bcaf8b69c26befe9c32059daa58d10f5465477551d471ca27\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-11fsxcvz/wheels/34/dc/f0/df40cac9fad37e7d77517c8c2594bdc5514e88eb4260b4f146\n",
      "Successfully built llamafactory jieba antlr4-python3-runtime\n",
      "Installing collected packages: jieba, antlr4-python3-runtime, shtab, rouge-chinese, omegaconf, docstring-parser, av, anyio, tyro, sse-starlette, trl, llamafactory\n",
      "  Attempting uninstall: anyio\n",
      "    Found existing installation: anyio 4.6.0\n",
      "    Uninstalling anyio-4.6.0:\n",
      "      Successfully uninstalled anyio-4.6.0\n",
      "  Attempting uninstall: trl\n",
      "    Found existing installation: trl 0.17.0\n",
      "    Uninstalling trl-0.17.0:\n",
      "      Successfully uninstalled trl-0.17.0\n",
      "Successfully installed antlr4-python3-runtime-4.9.3 anyio-4.9.0 av-14.4.0 docstring-parser-0.16 jieba-0.42.1 llamafactory-0.9.3.dev0 omegaconf-2.3.0 rouge-chinese-1.0.3 shtab-1.7.2 sse-starlette-2.3.5 trl-0.9.6 tyro-0.8.14\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#download llama factory\n",
    "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
    "!cd LLaMA-Factory && pip install -e \".[torch,metrics]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Supervised Fine Tuning Example\n",
    "### 2.1 Motivation\n",
    "Llama3 is a versatile large language model available in various parameter sizes. Given its significant improvements in text generation tasks compared to its predecessor, Llama2, we aim to use Llama3-8B-Instruct to generate Chinese poetry based on specific themes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Shared parameters between inference and SFT training\n",
    "################################################################################\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "# The base model\n",
    "model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# Use a single GPU\n",
    "# device_map = {'':0}\n",
    "# Use all GPUs\n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 22:01:35,594] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e27d9e008c943b8b37067ac86cb8a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "import os\n",
    "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]>\n",
      "\n",
      "<s>Here is a poem:</s>\n",
      "\n",
      "<s>风雨兼程，</s>\n",
      "<s>旅人寄语。</s>\n",
      "\n",
      "<s>(Translation: \"The wind and rain accompany my journey, / The traveler sends a letter.\")</s>\n",
      "\n",
      "<s>[/INST]</s>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INST: Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\n",
      "\n",
      "Here is a poem:\n",
      "\n",
      "风雨兼程，\n",
      "旅人寄语。\n",
      "\n",
      "(Translation: \"The wind and rain accompany my journey, / The traveler sends a letter.\")\n",
      "\n",
      "[/INST]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INST: Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\n",
      "\n",
      "Here is a poem:\n",
      "\n",
      "\n",
      "\n",
      "Translation: \"The wind and rain accompany my journey, / The traveler sends a letter.\")\n",
      "\n",
      "[/INST]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "INST: Hi, you are a\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\") \n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=200, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output does not make any sense. Not only the number of characters in each line is not suffcient to our requirement, but also the tune and words used is not like ancient poet at all.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the training dataset\n",
    "\n",
    "Let's use sft to improve Llama3-8B-Instruct's ablity in this field now!\n",
    "\n",
    "You should prepare for the data we need to use for SFT in `02_poet data` .\n",
    "\n",
    "Please complete the procedures in that notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 SFT with Llama-Factory\n",
    "\n",
    "For Processing SFT, we use llama factory, which is a highly modular, user-friendly platform with great ease of use, supporting distributed training and a variety of pre-trained models. Llama factory provide a WebUI to make it easy for using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 22:02:41,582] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-27 22:02:51 [__init__.py:239] Automatically detected platform cuda.\n",
      "Visit http://ip:port for Web UI, e.g., http://127.0.0.1:7860\n",
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 1093, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/components/data.py\", line 50, in can_preview\n",
      "    if len(dataset) == 0 or \"file_name\" not in dataset_info[dataset[0]]:\n",
      "KeyError: 'poet_'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 1093, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/components/data.py\", line 50, in can_preview\n",
      "    if len(dataset) == 0 or \"file_name\" not in dataset_info[dataset[0]]:\n",
      "KeyError: 'poe'\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/routes.py\", line 1093, in predict\n",
      "    output = await route_utils.call_process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2137, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 1663, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/utils.py\", line 890, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/components/data.py\", line 50, in can_preview\n",
      "    if len(dataset) == 0 or \"file_name\" not in dataset_info[dataset[0]]:\n",
      "KeyError: 'poe'\n",
      "[2025-05-27 22:14:43,540] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-27 22:14:48 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|2025-05-27 22:14:53] llamafactory.cli:143 >> Initializing 2 distributed tasks at: 127.0.0.1:59349\n",
      "W0527 22:14:54.889000 11780 torch/distributed/run.py:792] \n",
      "W0527 22:14:54.889000 11780 torch/distributed/run.py:792] *****************************************\n",
      "W0527 22:14:54.889000 11780 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0527 22:14:54.889000 11780 torch/distributed/run.py:792] *****************************************\n",
      "[2025-05-27 22:14:59,976] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2025-05-27 22:14:59,999] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "[2025-05-27 22:15:05,845] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[2025-05-27 22:15:05,846] [INFO] [comm.py:700:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2025-05-27 22:15:05,856] [INFO] [comm.py:669:init_distributed] cdb=None\n",
      "[WARNING|2025-05-27 22:15:05] llamafactory.hparams.parser:148 >> We recommend enable `upcast_layernorm` in quantized training.\n",
      "[INFO|2025-05-27 22:15:05] llamafactory.hparams.parser:143 >> Set `ddp_find_unused_parameters` to False in DDP training since LoRA is enabled.\n",
      "[INFO|2025-05-27 22:15:06] llamafactory.hparams.parser:401 >> Process rank: 0, world size: 2, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,079 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,079 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,080 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,080 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,080 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,080 >> loading file chat_template.jinja\n",
      "[INFO|2025-05-27 22:15:06] llamafactory.hparams.parser:401 >> Process rank: 1, world size: 2, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-27 22:15:06,545 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:15:06,553 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:15:06,555 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 22:15:06,561 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-27 22:15:06,965 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-27 22:15:06] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-27 22:15:06] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-27 22:15:06] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-27 22:15:06] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[rank1]:[W527 22:15:06.860541034 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "[INFO|2025-05-27 22:15:07] llamafactory.data.loader:143 >> Loading dataset alpaca_sft_dataset_with_varied_instructions.json...\n",
      "Setting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\n",
      "Generating train split: 109727 examples [00:00, 119396.81 examples/s]\n",
      "Converting format of dataset (num_proc=16): 100%|█| 100000/100000 [00:01<00:00, \n",
      "[rank0]:[W527 22:15:10.323520618 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n",
      "Running tokenizer on dataset (num_proc=16): 100%|█| 100000/100000 [00:08<00:00, \n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 13347, 11, 439, 264, 8620, 40360, 11, 649, 499, 1520, 757, 311, 1893, 264, 220, 20, 80325, 220, 19, 8614, 33894, 430, 52924, 279, 22100, 315, 80721, 237, 30867, 11, 122438, 9080, 11, 123335, 103125, 11, 10447, 3299, 51043, 242, 30, 128009, 128006, 78191, 128007, 271, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Hi, as a Chinese poet, can you help me to create a 5-character 4-line poem that incorporates the themes of 屏开, 晴日, 春风, 绿苔?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 103553, 244, 162, 7644, 102718, 110261, 102491, 42553, 28190, 21403, 232, 15120, 114897, 30867, 9174, 116366, 9080, 89753, 110588, 105301, 42553, 104149, 103125, 46961, 114735, 51043, 242, 1811, 128009]\n",
      "labels:\n",
      "崖悬百尺古，\n",
      "面削一屏开。\n",
      "晴日流丹草，\n",
      "春风长绿苔。<|eot_id|>\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:15:19,713 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:15:19,716 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-27 22:15:19] llamafactory.model.model_utils.quantization:143 >> Quantizing model to 4 bit with bitsandbytes.\n",
      "[INFO|2025-05-27 22:15:19] llamafactory.model.model_utils.kv_cache:143 >> KV cache is disabled during training.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|modeling_utils.py:1121] 2025-05-27 22:15:19,916 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-27 22:15:19,917 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-27 22:15:19,920 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:12<00:00,  3.07s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:12<00:00,  3.11s/it]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-27 22:15:32,491 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-27 22:15:32,491 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-27 22:15:32,494 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-27 22:15:32,495 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-27 22:15:32] llamafactory.model.model_utils.checkpointing:143 >> Gradient checkpointing enabled.\n",
      "[INFO|2025-05-27 22:15:32] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "[INFO|2025-05-27 22:15:32] llamafactory.model.adapter:143 >> Upcasting trainable params to float32.\n",
      "[INFO|2025-05-27 22:15:32] llamafactory.model.adapter:143 >> Fine-tuning method: LoRA\n",
      "[INFO|2025-05-27 22:15:32] llamafactory.model.model_utils.misc:143 >> Found linear modules: o_proj,q_proj,v_proj,down_proj,k_proj,up_proj,gate_proj\n",
      "[INFO|2025-05-27 22:15:34] llamafactory.model.loader:143 >> trainable params: 167,772,160 || all params: 8,198,033,408 || trainable%: 2.0465\n",
      "[INFO|trainer.py:748] 2025-05-27 22:15:34,460 >> Using auto half precision backend\n",
      "[2025-05-27 22:15:34,832] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.16.9, git-hash=unknown, git-branch=unknown\n",
      "[2025-05-27 22:15:34,832] [INFO] [config.py:735:__init__] Config mesh_device None world_size = 2\n",
      "[2025-05-27 22:15:35,019] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-05-27 22:15:35,024] [INFO] [logging.py:107:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2025-05-27 22:15:35,024] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-05-27 22:15:35,065] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\n",
      "[2025-05-27 22:15:35,065] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'bitsandbytes.optim.adamw.AdamW'>\n",
      "[2025-05-27 22:15:35,065] [WARNING] [engine.py:1338:_do_optimizer_sanity_check] **** You are using ZeRO with an untested optimizer, proceed with caution *****\n",
      "[2025-05-27 22:15:35,065] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2025-05-27 22:15:35,065] [INFO] [stage_1_and_2.py:150:__init__] Reduce bucket size 500000000\n",
      "[2025-05-27 22:15:35,065] [INFO] [stage_1_and_2.py:151:__init__] Allgather bucket size 500000000\n",
      "[2025-05-27 22:15:35,065] [INFO] [stage_1_and_2.py:152:__init__] CPU Offload: False\n",
      "[2025-05-27 22:15:35,065] [INFO] [stage_1_and_2.py:153:__init__] Round robin gradient partitioning: True\n",
      "[2025-05-27 22:15:35,969] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-05-27 22:15:35,970] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 6.09 GB         CA 6.16 GB         Max_CA 6 GB \n",
      "[2025-05-27 22:15:35,970] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.42 GB, percent = 4.4%\n",
      "[2025-05-27 22:15:36,186] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-05-27 22:15:36,186] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 6.25 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2025-05-27 22:15:36,186] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.43 GB, percent = 4.4%\n",
      "[2025-05-27 22:15:36,187] [INFO] [stage_1_and_2.py:557:__init__] optimizer state initialized\n",
      "[2025-05-27 22:15:36,445] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-05-27 22:15:36,446] [INFO] [utils.py:782:see_memory_usage] MA 5.94 GB         Max_MA 5.94 GB         CA 6.47 GB         Max_CA 6 GB \n",
      "[2025-05-27 22:15:36,446] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 44.42 GB, percent = 4.4%\n",
      "[2025-05-27 22:15:36,452] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-05-27 22:15:36,452] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-05-27 22:15:36,452] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-05-27 22:15:36,452] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0], mom=[(0.9, 0.999)]\n",
      "[2025-05-27 22:15:36,459] [INFO] [config.py:1003:print] DeepSpeedEngine configuration:\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   amp_enabled .................. False\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   amp_params ................... False\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   bfloat16_enabled ............. True\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   bfloat16_immediate_grad_update  True\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fd1e0230a30>\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   communication_data_type ...... None\n",
      "[2025-05-27 22:15:36,460] [INFO] [config.py:1007:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   curriculum_enabled_legacy .... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   curriculum_params_legacy ..... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   data_efficiency_enabled ...... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   dataloader_drop_last ......... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   disable_allgather ............ False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   dump_state ................... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   dynamic_loss_scale_args ...... None\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_enabled ........... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   eigenvalue_verbose ........... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   elasticity_enabled ........... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   fp16_auto_cast ............... None\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   fp16_enabled ................. False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   global_rank .................. 0\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   grad_accum_dtype ............. None\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   gradient_accumulation_steps .. 1\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   gradient_clipping ............ 0.3\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   graph_harvesting ............. False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   initial_dynamic_scale ........ 1\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   load_universal_checkpoint .... False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   loss_scale ................... 1.0\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   memory_breakdown ............. False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   mics_hierarchial_params_gather  False\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   mics_shard_size .............. -1\n",
      "[2025-05-27 22:15:36,461] [INFO] [config.py:1007:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   optimizer_name ............... None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   optimizer_params ............. None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   pld_enabled .................. False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   pld_params ................... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   prescale_gradients ........... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   scheduler_name ............... None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   scheduler_params ............. None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   sparse_attention ............. None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   sparse_gradients_enabled ..... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   steps_per_print .............. inf\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   train_batch_size ............. 16\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   train_micro_batch_size_per_gpu  8\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   use_data_before_expert_parallel_  False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   use_node_local_storage ....... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   wall_clock_breakdown ......... False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   weight_quantization_config ... None\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   world_size ................... 2\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   zero_allow_untested_optimizer  True\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=True zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   zero_enabled ................. True\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:1007:print]   zero_optimization_stage ...... 2\n",
      "[2025-05-27 22:15:36,462] [INFO] [config.py:993:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 8, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"gradient_clipping\": 0.3, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 5.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"round_robin_gradients\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\n",
      "}\n",
      "[INFO|trainer.py:2414] 2025-05-27 22:15:36,464 >> ***** Running training *****\n",
      "[INFO|trainer.py:2415] 2025-05-27 22:15:36,464 >>   Num examples = 100,000\n",
      "[INFO|trainer.py:2416] 2025-05-27 22:15:36,464 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2417] 2025-05-27 22:15:36,464 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:2420] 2025-05-27 22:15:36,464 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "[INFO|trainer.py:2421] 2025-05-27 22:15:36,464 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2422] 2025-05-27 22:15:36,464 >>   Total optimization steps = 2,063\n",
      "[INFO|trainer.py:2423] 2025-05-27 22:15:36,469 >>   Number of trainable parameters = 167,772,160\n",
      "  5%|█▉                                      | 100/2063 [00:53<17:19,  1.89it/s][INFO|2025-05-27 22:16:30] llamafactory.train.callbacks:143 >> {'loss': 3.8321, 'learning_rate': 9.9000e-05, 'epoch': 0.02, 'throughput': 2823.76}\n",
      "{'loss': 3.8321, 'grad_norm': 0.8500438928604126, 'learning_rate': 9.900000000000001e-05, 'epoch': 0.02, 'num_input_tokens_seen': 151360}\n",
      " 10%|███▉                                    | 200/2063 [01:46<16:20,  1.90it/s][INFO|2025-05-27 22:17:22] llamafactory.train.callbacks:143 >> {'loss': 3.3458, 'learning_rate': 1.9900e-04, 'epoch': 0.03, 'throughput': 2838.08}\n",
      "{'loss': 3.3458, 'grad_norm': 0.7104527354240417, 'learning_rate': 0.000199, 'epoch': 0.03, 'num_input_tokens_seen': 302080}\n",
      " 12%|████▊                                   | 250/2063 [02:12<15:57,  1.89it/s][INFO|trainer.py:3984] 2025-05-27 22:17:53,572 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:17:53,621 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:17:53,622 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:17:56,539 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:17:56,547 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/special_tokens_map.json\n",
      "[2025-05-27 22:17:57,082] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step250 is about to be saved!\n",
      "[2025-05-27 22:17:57,190] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:17:57,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:18:01,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:18:01,570] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:18:11,672] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:18:11,714] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-250/global_step250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:18:11,714] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step250 is ready now!\n",
      " 15%|█████▊                                  | 300/2063 [03:02<15:33,  1.89it/s][INFO|2025-05-27 22:18:38] llamafactory.train.callbacks:143 >> {'loss': 3.2790, 'learning_rate': 1.9861e-04, 'epoch': 0.05, 'throughput': 2488.33}\n",
      "{'loss': 3.279, 'grad_norm': 0.6891076564788818, 'learning_rate': 0.0001986097095854347, 'epoch': 0.05, 'num_input_tokens_seen': 452992}\n",
      " 19%|███████▊                                | 400/2063 [03:55<14:45,  1.88it/s][INFO|2025-05-27 22:19:31] llamafactory.train.callbacks:143 >> {'loss': 3.2016, 'learning_rate': 1.9442e-04, 'epoch': 0.06, 'throughput': 2568.07}\n",
      "{'loss': 3.2016, 'grad_norm': 0.6832563877105713, 'learning_rate': 0.00019442209852030647, 'epoch': 0.06, 'num_input_tokens_seen': 603840}\n",
      " 24%|█████████▋                              | 500/2063 [04:48<14:00,  1.86it/s][INFO|2025-05-27 22:20:25] llamafactory.train.callbacks:143 >> {'loss': 3.1498, 'learning_rate': 1.8756e-04, 'epoch': 0.08, 'throughput': 2610.92}\n",
      "{'loss': 3.1498, 'grad_norm': 0.7101650238037109, 'learning_rate': 0.0001875558231302091, 'epoch': 0.08, 'num_input_tokens_seen': 754176}\n",
      " 24%|█████████▋                              | 500/2063 [04:48<14:00,  1.86it/s][INFO|trainer.py:3984] 2025-05-27 22:20:28,783 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:20:28,865 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:20:28,866 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:20:31,835 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:20:31,850 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/special_tokens_map.json\n",
      "[2025-05-27 22:20:32,354] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step500 is about to be saved!\n",
      "[2025-05-27 22:20:32,457] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:20:32,457] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:20:36,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:20:36,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:20:46,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:20:46,735] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-500/global_step500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:20:46,735] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step500 is ready now!\n",
      " 29%|███████████▋                            | 600/2063 [06:04<12:52,  1.90it/s][INFO|2025-05-27 22:21:41] llamafactory.train.callbacks:143 >> {'loss': 3.1149, 'learning_rate': 1.7821e-04, 'epoch': 0.10, 'throughput': 2483.89}\n",
      "{'loss': 3.1149, 'grad_norm': 0.6851624250411987, 'learning_rate': 0.00017820567305889228, 'epoch': 0.1, 'num_input_tokens_seen': 905600}\n",
      " 34%|█████████████▌                          | 700/2063 [06:57<12:01,  1.89it/s][INFO|2025-05-27 22:22:34] llamafactory.train.callbacks:143 >> {'loss': 3.0875, 'learning_rate': 1.6664e-04, 'epoch': 0.11, 'throughput': 2528.12}\n",
      "{'loss': 3.0875, 'grad_norm': 0.696304440498352, 'learning_rate': 0.00016663690309121708, 'epoch': 0.11, 'num_input_tokens_seen': 1056320}\n",
      " 36%|██████████████▌                         | 750/2063 [07:24<11:45,  1.86it/s][INFO|trainer.py:3984] 2025-05-27 22:23:05,017 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:23:05,073 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:23:05,074 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:23:08,102 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:23:08,126 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/special_tokens_map.json\n",
      "[2025-05-27 22:23:08,603] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step750 is about to be saved!\n",
      "[2025-05-27 22:23:08,739] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:23:08,739] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:23:12,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:23:12,939] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:23:22,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:23:22,951] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-750/global_step750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:23:22,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step750 is ready now!\n",
      " 39%|███████████████▌                        | 800/2063 [08:13<11:12,  1.88it/s][INFO|2025-05-27 22:23:49] llamafactory.train.callbacks:143 >> {'loss': 3.0309, 'learning_rate': 1.5318e-04, 'epoch': 0.13, 'throughput': 2448.16}\n",
      "{'loss': 3.0309, 'grad_norm': 0.7106566429138184, 'learning_rate': 0.00015317770812945563, 'epoch': 0.13, 'num_input_tokens_seen': 1207936}\n",
      " 44%|█████████████████▍                      | 900/2063 [09:06<10:16,  1.89it/s][INFO|2025-05-27 22:24:43] llamafactory.train.callbacks:143 >> {'loss': 3.0003, 'learning_rate': 1.3821e-04, 'epoch': 0.14, 'throughput': 2484.30}\n",
      "{'loss': 3.0003, 'grad_norm': 0.7187826037406921, 'learning_rate': 0.00013820991261885798, 'epoch': 0.14, 'num_input_tokens_seen': 1358336}\n",
      " 48%|██████████████████▉                    | 1000/2063 [10:00<09:27,  1.87it/s][INFO|2025-05-27 22:25:36] llamafactory.train.callbacks:143 >> {'loss': 2.9857, 'learning_rate': 1.2216e-04, 'epoch': 0.16, 'throughput': 2515.81}\n",
      "{'loss': 2.9857, 'grad_norm': 0.6400742530822754, 'learning_rate': 0.0001221581385545502, 'epoch': 0.16, 'num_input_tokens_seen': 1509760}\n",
      " 48%|██████████████████▉                    | 1000/2063 [10:00<09:27,  1.87it/s][INFO|trainer.py:3984] 2025-05-27 22:25:39,386 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:25:39,465 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:25:39,466 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:25:42,402 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:25:42,422 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/special_tokens_map.json\n",
      "[2025-05-27 22:25:42,907] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!\n",
      "[2025-05-27 22:25:42,998] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:25:42,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:25:47,312] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:25:47,355] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:25:57,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:25:57,419] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1000/global_step1000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:25:57,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!\n",
      " 53%|████████████████████▊                  | 1100/2063 [11:15<08:36,  1.87it/s][INFO|2025-05-27 22:26:51] llamafactory.train.callbacks:143 >> {'loss': 2.9436, 'learning_rate': 1.0548e-04, 'epoch': 0.18, 'throughput': 2459.64}\n",
      "{'loss': 2.9436, 'grad_norm': 0.7302427887916565, 'learning_rate': 0.00010547775936301114, 'epoch': 0.18, 'num_input_tokens_seen': 1661248}\n",
      " 58%|██████████████████████▋                | 1200/2063 [12:08<07:35,  1.89it/s][INFO|2025-05-27 22:27:45] llamafactory.train.callbacks:143 >> {'loss': 2.9411, 'learning_rate': 8.8642e-05, 'epoch': 0.19, 'throughput': 2486.81}\n",
      "{'loss': 2.9411, 'grad_norm': 0.6691627502441406, 'learning_rate': 8.86419813949525e-05, 'epoch': 0.19, 'num_input_tokens_seen': 1812800}\n",
      " 61%|███████████████████████▋               | 1250/2063 [12:35<07:13,  1.87it/s][INFO|trainer.py:3984] 2025-05-27 22:28:14,579 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:28:14,644 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:28:14,646 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:28:17,737 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:28:17,746 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/special_tokens_map.json\n",
      "[2025-05-27 22:28:18,218] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1250 is about to be saved!\n",
      "[2025-05-27 22:28:18,305] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:28:18,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:28:22,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:28:22,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:28:32,885] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:28:32,909] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1250/global_step1250/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:28:32,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1250 is ready now!\n",
      " 63%|████████████████████████▌              | 1300/2063 [13:23<06:47,  1.87it/s][INFO|2025-05-27 22:28:59] llamafactory.train.callbacks:143 >> {'loss': 2.8929, 'learning_rate': 7.2128e-05, 'epoch': 0.21, 'throughput': 2444.30}\n",
      "{'loss': 2.8929, 'grad_norm': 0.6825789213180542, 'learning_rate': 7.212841951525097e-05, 'epoch': 0.21, 'num_input_tokens_seen': 1963648}\n",
      " 68%|██████████████████████████▍            | 1400/2063 [14:16<05:53,  1.87it/s][INFO|2025-05-27 22:29:52] llamafactory.train.callbacks:143 >> {'loss': 2.8874, 'learning_rate': 5.6406e-05, 'epoch': 0.22, 'throughput': 2468.57}\n",
      "{'loss': 2.8874, 'grad_norm': 0.6508699655532837, 'learning_rate': 5.640554762756384e-05, 'epoch': 0.22, 'num_input_tokens_seen': 2114368}\n",
      " 73%|████████████████████████████▎          | 1500/2063 [15:11<05:00,  1.87it/s][INFO|2025-05-27 22:30:47] llamafactory.train.callbacks:143 >> {'loss': 2.8598, 'learning_rate': 4.1919e-05, 'epoch': 0.24, 'throughput': 2486.45}\n",
      "{'loss': 2.8598, 'grad_norm': 0.6903864145278931, 'learning_rate': 4.191940851924291e-05, 'epoch': 0.24, 'num_input_tokens_seen': 2265408}\n",
      " 73%|████████████████████████████▎          | 1500/2063 [15:11<05:00,  1.87it/s][INFO|trainer.py:3984] 2025-05-27 22:30:50,187 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:30:50,258 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:30:50,259 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:30:53,427 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:30:53,435 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/special_tokens_map.json\n",
      "[2025-05-27 22:30:53,900] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1500 is about to be saved!\n",
      "[2025-05-27 22:30:54,020] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:30:54,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:30:58,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:30:58,315] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:31:08,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:31:08,520] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1500/global_step1500/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:31:08,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1500 is ready now!\n",
      " 78%|██████████████████████████████▏        | 1600/2063 [16:26<04:06,  1.88it/s][INFO|2025-05-27 22:32:02] llamafactory.train.callbacks:143 >> {'loss': 2.8625, 'learning_rate': 2.9081e-05, 'epoch': 0.26, 'throughput': 2450.31}\n",
      "{'loss': 2.8625, 'grad_norm': 0.6511245369911194, 'learning_rate': 2.9080960055492356e-05, 'epoch': 0.26, 'num_input_tokens_seen': 2416384}\n",
      " 82%|████████████████████████████████▏      | 1700/2063 [17:19<03:14,  1.86it/s][INFO|2025-05-27 22:32:56] llamafactory.train.callbacks:143 >> {'loss': 2.8387, 'learning_rate': 1.8254e-05, 'epoch': 0.27, 'throughput': 2468.81}\n",
      "{'loss': 2.8387, 'grad_norm': 0.7006964683532715, 'learning_rate': 1.8254416699099276e-05, 'epoch': 0.27, 'num_input_tokens_seen': 2566848}\n",
      " 85%|█████████████████████████████████      | 1750/2063 [17:46<02:46,  1.87it/s][INFO|trainer.py:3984] 2025-05-27 22:33:25,485 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:33:25,534 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:33:25,535 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:33:28,650 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:33:28,676 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/special_tokens_map.json\n",
      "[2025-05-27 22:33:29,168] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step1750 is about to be saved!\n",
      "[2025-05-27 22:33:29,285] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:33:29,285] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:33:33,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:33:33,712] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:33:43,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:33:43,556] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-1750/global_step1750/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:33:43,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1750 is ready now!\n",
      " 87%|██████████████████████████████████     | 1800/2063 [18:34<02:20,  1.88it/s][INFO|2025-05-27 22:34:10] llamafactory.train.callbacks:143 >> {'loss': 2.8380, 'learning_rate': 9.7469e-06, 'epoch': 0.29, 'throughput': 2438.78}\n",
      "{'loss': 2.838, 'grad_norm': 0.6984342932701111, 'learning_rate': 9.7469170956347e-06, 'epoch': 0.29, 'num_input_tokens_seen': 2717504}\n",
      " 92%|███████████████████████████████████▉   | 1900/2063 [19:27<01:27,  1.87it/s][INFO|2025-05-27 22:35:04] llamafactory.train.callbacks:143 >> {'loss': 2.8316, 'learning_rate': 3.7998e-06, 'epoch': 0.30, 'throughput': 2456.86}\n",
      "{'loss': 2.8316, 'grad_norm': 0.675422728061676, 'learning_rate': 3.7998108448174973e-06, 'epoch': 0.3, 'num_input_tokens_seen': 2868544}\n",
      " 97%|█████████████████████████████████████▊ | 2000/2063 [20:20<00:33,  1.90it/s][INFO|2025-05-27 22:35:57] llamafactory.train.callbacks:143 >> {'loss': 2.8355, 'learning_rate': 5.8181e-07, 'epoch': 0.32, 'throughput': 2473.37}\n",
      "{'loss': 2.8355, 'grad_norm': 0.7429365515708923, 'learning_rate': 5.818116439760157e-07, 'epoch': 0.32, 'num_input_tokens_seen': 3019456}\n",
      " 97%|█████████████████████████████████████▊ | 2000/2063 [20:20<00:33,  1.90it/s][INFO|trainer.py:3984] 2025-05-27 22:36:01,070 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:36:01,150 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:36:01,152 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:36:04,258 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:36:04,267 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/special_tokens_map.json\n",
      "[2025-05-27 22:36:04,743] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!\n",
      "[2025-05-27 22:36:04,858] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:36:04,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:36:09,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:36:09,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:36:18,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:36:18,943] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2000/global_step2000/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:36:18,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!\n",
      "100%|███████████████████████████████████████| 2063/2063 [21:16<00:00,  1.89it/s][INFO|trainer.py:3984] 2025-05-27 22:36:54,835 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:36:54,879 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:36:54,880 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:36:58,037 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:36:58,043 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/special_tokens_map.json\n",
      "[2025-05-27 22:36:58,529] [INFO] [logging.py:107:log_dist] [Rank 0] [Torch] Checkpoint global_step2063 is about to be saved!\n",
      "[2025-05-27 22:36:58,624] [INFO] [logging.py:107:log_dist] [Rank 0] Saving model checkpoint: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt\n",
      "[2025-05-27 22:36:58,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt...\n",
      "[2025-05-27 22:37:02,848] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/mp_rank_00_model_states.pt.\n",
      "[2025-05-27 22:37:02,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\n",
      "[2025-05-27 22:37:12,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\n",
      "[2025-05-27 22:37:12,692] [INFO] [engine.py:3701:_save_zero_checkpoint] zero checkpoint saved saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/checkpoint-2063/global_step2063/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\n",
      "[2025-05-27 22:37:12,692] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2063 is ready now!\n",
      "[INFO|trainer.py:2681] 2025-05-27 22:37:12,928 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "{'train_runtime': 1296.4596, 'train_samples_per_second': 25.454, 'train_steps_per_second': 1.591, 'train_loss': 3.0318601518594925, 'epoch': 0.33, 'num_input_tokens_seen': 3113792}\n",
      "100%|███████████████████████████████████████| 2063/2063 [21:36<00:00,  1.59it/s]\n",
      "[INFO|trainer.py:3984] 2025-05-27 22:37:14,726 >> Saving model checkpoint to saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 22:37:14,777 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 22:37:14,778 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 22:37:18,027 >> tokenizer config file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 22:37:18,038 >> Special tokens file saved in saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/special_tokens_map.json\n",
      "***** train metrics *****\n",
      "  epoch                    =      0.3301\n",
      "  num_input_tokens_seen    =     3113792\n",
      "  total_flos               = 133502384GF\n",
      "  train_loss               =      3.0319\n",
      "  train_runtime            =  0:21:36.45\n",
      "  train_samples_per_second =      25.454\n",
      "  train_steps_per_second   =       1.591\n",
      "Figure saved at: saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00/training_loss.png\n",
      "[WARNING|2025-05-27 22:37:18] llamafactory.extras.ploting:148 >> No metric eval_loss to plot.\n",
      "[WARNING|2025-05-27 22:37:18] llamafactory.extras.ploting:148 >> No metric eval_accuracy to plot.\n",
      "[INFO|modelcard.py:450] 2025-05-27 22:37:18,734 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2997, in block_thread\n",
      "    time.sleep(0.1)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/llamafactory-cli\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/cli.py\", line 115, in main\n",
      "    COMMAND_MAP[command]()\n",
      "  File \"/gfshome/LLaMA-Factory/src/llamafactory/webui/interface.py\", line 97, in run_web_ui\n",
      "    create_ui().queue().launch(share=gradio_share, server_name=server_name, inbrowser=True)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2903, in launch\n",
      "    self.block_thread()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 3001, in block_thread\n",
      "    self.server.close()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/gradio/http_server.py\", line 69, in close\n",
      "    self.thread.join(timeout=5)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1100, in join\n",
      "    self._wait_for_tstate_lock(timeout=max(timeout, 0))\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "!cd LLaMA-Factory && llamafactory-cli webui"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the training parameters we selected in the file `Llama3-8B-Instruct-sft.yaml`, or refer to the screenshot in the slides. After you fullfill the parameters, click `Start` and wait for the SFT process to complete.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training runs to complete, please paste your loss change chat below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_2025-05-27-22-03-00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# You can now terminate the training process by stopping the previous cell.\n",
    "# The resulting LoRA is saved in LLaMA-Factory/saves/Llama-3-8B-Instruct/lora \n",
    "# (who is automatically named with a date as suffix)\n",
    "!ls LLaMA-Factory/saves/Llama-3-8B-Instruct/lora "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging the LoRA into the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-27 23:00:14,455] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /root/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n",
      "INFO 05-27 23:00:19 [__init__.py:239] Automatically detected platform cuda.\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,279 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,279 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,280 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,280 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,280 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,280 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-27 23:00:23,680 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 23:00:23,687 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 23:00:23,688 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2058] 2025-05-27 23:00:23,695 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2323] 2025-05-27 23:00:24,080 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|2025-05-27 23:00:24] llamafactory.data.template:143 >> Replace eos token: <|eot_id|>.\n",
      "[INFO|2025-05-27 23:00:24] llamafactory.data.template:143 >> Add pad token: <|eot_id|>\n",
      "[INFO|2025-05-27 23:00:24] llamafactory.data.template:143 >> Add <|eom_id|> to stop words.\n",
      "[WARNING|2025-05-27 23:00:24] llamafactory.data.template:148 >> New tokens have been added, make sure `resize_vocab` is True.\n",
      "[INFO|configuration_utils.py:691] 2025-05-27 23:00:24,100 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/config.json\n",
      "[INFO|configuration_utils.py:765] 2025-05-27 23:00:24,101 >> Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|2025-05-27 23:00:24] llamafactory.model.model_utils.kv_cache:143 >> KV cache is enabled for faster generation.\n",
      "[INFO|modeling_utils.py:1121] 2025-05-27 23:00:24,168 >> loading weights file /ssdshare/share/Meta-Llama-3-8B-Instruct/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:2167] 2025-05-27 23:00:24,168 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1142] 2025-05-27 23:00:24,190 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:00<00:00, 46.85it/s]\n",
      "[INFO|modeling_utils.py:4926] 2025-05-27 23:00:24,314 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4934] 2025-05-27 23:00:24,314 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /ssdshare/share/Meta-Llama-3-8B-Instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1095] 2025-05-27 23:00:24,316 >> loading configuration file /ssdshare/share/Meta-Llama-3-8B-Instruct/generation_config.json\n",
      "[INFO|configuration_utils.py:1142] 2025-05-27 23:00:24,317 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 4096,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "[INFO|2025-05-27 23:00:24] llamafactory.model.model_utils.attention:143 >> Using torch SDPA for faster training and inference.\n",
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "[INFO|2025-05-27 23:01:52] llamafactory.model.adapter:143 >> Merged 1 adapter(s).\n",
      "[INFO|2025-05-27 23:01:52] llamafactory.model.adapter:143 >> Loaded adapter(s): /gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-27-22-03-00\n",
      "[INFO|2025-05-27 23:01:52] llamafactory.model.loader:143 >> all params: 8,030,261,248\n",
      "[INFO|2025-05-27 23:01:52] llamafactory.train.tuner:143 >> Convert model dtype to: torch.bfloat16.\n",
      "[INFO|configuration_utils.py:419] 2025-05-27 23:01:53,232 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/config.json\n",
      "[INFO|configuration_utils.py:911] 2025-05-27 23:01:53,241 >> Configuration saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/generation_config.json\n",
      "[INFO|modeling_utils.py:3580] 2025-05-27 23:04:23,012 >> The model is bigger than the maximum size per checkpoint (4GB) and is going to be split in 5 checkpoint shards. You can find where each parameters has been saved in the index located at /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/model.safetensors.index.json.\n",
      "[INFO|tokenization_utils_base.py:2510] 2025-05-27 23:04:23,033 >> tokenizer config file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2519] 2025-05-27 23:04:23,058 >> Special tokens file saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/special_tokens_map.json\n",
      "[INFO|2025-05-27 23:04:23] llamafactory.train.tuner:143 >> Ollama modelfile saved in /gfshome/merged_model/Llama-3-8B-Instruct-sft-poet/Modelfile\n"
     ]
    }
   ],
   "source": [
    "# Merge Lora_model with Base model and save the merged model\n",
    "# ***Update the Lora-Merge.yaml configuration file and fullfill the Lora Path***\n",
    "# For more options in export, please refer to the [Llama-Factory Documentation](https://github.com/hiyouga/LLaMA-Factory/blob/main/docs/export.md)\n",
    "\n",
    "!llamafactory-cli export Lora_Merge.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Testing the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose your Finetuneed model for test\n",
    "#Dont't forget to change the model name to your export_dir\n",
    "model_name = \"/gfshome/merged_model/Llama-3-8B-Instruct-sft-poet\"  # your new model \n",
    "device_map = \"auto\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=125 environment variable detected; loading libbitsandbytes_cuda125.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a328776f6de041fe8cd79283653860cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "    bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "    bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# Load base model with bnb config\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you don't want to merge your lora to get a new model, you can just using the lora when inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import transformers\n",
    "# import torch\n",
    "# from transformers import BitsAndBytesConfig\n",
    "\n",
    "# model_name = \"/ssdshare/share/Meta-Llama-3-8B-Instruct\"\n",
    "# device_map = \"auto\"\n",
    "# adapter_name_or_path = \"/gfshome/LLaMA-Factory/saves/Llama-3-8B-Instruct/lora/train_2025-05-15-09-25-23\"\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit= True,    # use 4-bit precision for base model loading\n",
    "#     bnb_4bit_quant_type= \"nf4\",  # Quantization type (fp4 or nf4)\n",
    "#     bnb_4bit_compute_dtype= torch.bfloat16,   # Compute dtype for 4-bit base models  \"float16\" or torch.bfloat16\n",
    "#     bnb_4bit_use_double_quant= False,  # Activate nested quantization for 4-bit base models (double quantization)\n",
    "# )\n",
    "\n",
    "# from transformers import (\n",
    "#     AutoModelForCausalLM,\n",
    "#     AutoTokenizer,\n",
    "#     TrainingArguments,\n",
    "#     pipeline,\n",
    "# )\n",
    "# from peft import PeftModel\n",
    "\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_name,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=device_map\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1\n",
    "\n",
    "# model = PeftModel.from_pretrained(\n",
    "#     model,\n",
    "#     adapter_name_or_path, \n",
    "#     device_map=device_map\n",
    "# )\n",
    "\n",
    "# os.environ['BNB_CUDA_VERSION'] = '125'\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人? [/INST]旅人不见家，\n",
      "风雨夜深时。 </s>\n",
      "旅人不见家，\n",
      "风雨夜深时。\n",
      "何事故乡路，\n",
      "几人过此溪。\n",
      "当年一杯酒，\n",
      "相约今日谁。 </s>\n",
      "旅人不见家，\n",
      "风雨夜深时\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, you are a Chinese ancient poet, can you write a 2 sentence, 5-character poem about the theme of 风雨，旅人?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源. [/INST] <s>桃源水上花开时,</s>\n",
      "<s>桃树花开是故知。</s>\n",
      "<s>故知桃树花开处,</s>\n",
      "<s>不必桃源水上归。</s>\n",
      "</INST>\n",
      "\n",
      "<s>[INST] Hi, please draft a 7-character\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, please draft a 7-character 4-line chinese ancient poem based on the themes: 花开, 桃源.\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税? [/INST] <s>\n",
      "\n",
      "美国关税不关税，\n",
      "中国关税关美国。\n",
      "一分一分都要税，\n",
      "可笑天公不识数。</s>[/INST]\n",
      "<s>美国关税不关税，\n",
      "中国关税关美国。\n",
      "一分一分都要税，\n",
      "可笑天\n"
     ]
    }
   ],
   "source": [
    "# Run text generation pipeline with our next model\n",
    "prompt = \"Hi, as a Chinese ancient poet, can you help me to create a 7-character 4-line poem that incorporates the themes of 美国，关税?\" \n",
    "eos_ids = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")  # 如果 tokenizer 支持这个 token\n",
    "]\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64, eos_token_id=eos_ids, num_return_sequences=1)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

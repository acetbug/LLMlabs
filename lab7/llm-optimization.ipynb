{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## Optimizing LLMs for Speed and Memory"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Ref: https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization\n", "\n", "To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.\n", "\n", "The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.\n", "\n", "In this guide, we will go over the effective techniques for efficient LLM deployment:\n", "\n", "1. **Parallelization**\n", "\n", "2. **Lower Precision (Quantization)**\n", "\n", "3. **Flash Attention**"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Measuring VRAM Usage"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Memory requirements of LLMs can be best understood by seeing the LLM as a set of weight matrices and vectors and the text inputs as a sequence of vectors. In the following, the definition weights will be used to signify all model weight matrices and vectors.\n", "\n", "At the time of 2025, LLMs consist of at least a couple billion parameters. Each parameter thereby is made of a decimal number, e.g. 4.5689 which is usually stored in either float32, bfloat16, or float16 format. This allows us to easily compute the memory requirement to load the LLM into memory:\n", "\n", "Loading the weights of a model having X billion parameters requires roughly 4 X GB of VRAM in float32 precision. Loading the weights of a model having X billion parameters requires roughly 2 X GB of VRAM in bfloat16/float16 precision.\n", "\n", "To give some examples of how much VRAM it roughly takes to load a model in bfloat16:\n", "\n", "- GPT3 requires 2 * 175 GB = 350 GB VRAM\n", "- Llama-2-70b requires 2 * 70 GB = 140 GB VRAM\n", "\n", "However, the current largest GPU chip on the market is the NVIDIA GB200 offering 192GB of VRAM. Our 4090 only has 24GB of VRAM. 4090D has 48GB of VRAM. Most of the models listed before require more than 24GB just to be loaded and therefore necessarily require some form of optimization.\n", "\n", "Moreover, we sometimes cache the hiden representations of the input tokens and generated tokens to speed up the generation process. This technique, called KV caching, is an example of space-time trade-off in Computer Science. It is especially useful for long sequences and commonly intregrated into LLM inference frameworks like vllm, sglang.\n", "\n", "https://lmcache.ai/kv_cache_calculator.html is a good tool to calculate the memory requirements of the KV cache.\n", "\n", "Here we only input and output a short sequence of tokens, so the KV cache is negligible. In the following parts, we only focus on the memory requirements of the model weights."]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# %pip install transformers accelerate bitsandbytes optimum"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["model_path = \"/ssdshare/share/Meta-Llama-3-8B-Instruct/\""]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n", "import torch\n", "\n", "model = AutoModelForCausalLM.from_pretrained(\n", "    model_path,\n", "    torch_dtype=torch.bfloat16, # note here\n", "    attn_implementation=\"flash_attention_2\",\n", "    device_map=\"cuda:0\"\n", ")\n", "tokenizer = AutoTokenizer.from_pretrained(model_path)\n", "\n", "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["prompt = \"Question: Please write a function in Python that transforms bytes to Giga bytes.\\n\\nAnswer:\"\n", "\n", "result = pipe(prompt, max_new_tokens=300, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"][len(prompt):]\n", "result"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["def bytes_to_giga_bytes(bytes):\n", "    \"\"\"\n", "    Convert bytes to gigabytes.\n", "\n", "    Parameters:\n", "        bytes (int): The number of bytes to convert.\n", "\n", "    Returns:\n", "        float: The equivalent value in gigabytes.\n", "    \"\"\"\n", "    # 1 Gigabyte = 1024^3 bytes\n", "    gigabytes = bytes / (1024**3)\n", "    return gigabytes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let\u2019s call torch.cuda.max_memory_allocated to measure the peak GPU memory allocation."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Close enough to our back-of-the-envelope computation! We can see the number is not exactly correct as going from bytes to kilobytes requires a multiplication of 1024 instead of 1000. \n", "\n", "Note that if we had tried to run the model in full float32 precision, a whopping 32 GB of VRAM would have been required.\n", "\n", "Almost all models are trained in bfloat16 nowadays, there is no reason to run the model in full float32 precision if your GPU supports bfloat16. Float32 won\u2019t give better inference results than the precision that was used to train the model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let\u2019s define a flush(...) function to free all allocated memory so that we can accurately measure the peak allocated GPU memory."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import gc\n", "import torch\n", "\n", "del pipe\n", "del model\n", "\n", "def flush():\n", "  gc.collect()\n", "  torch.cuda.empty_cache()\n", "  torch.cuda.reset_peak_memory_stats()"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["flush()"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# In the recent version of the accelerate library, you can also use a utility method called release_memory()\n", "\n", "# from accelerate.utils import release_memory\n", "# release_memory(model)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Lower Precision\n", "\n", "What if we could reduce the memory requirements even further?\n", "It has been found that model weights can be quantized to 8-bit or 4-bits without a significant loss in performance (see [Dettmers et al.](https://arxiv.org/abs/2208.07339))."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.1 Intro to Quantization\n", "\n", "Without going into too many details, quantization schemes aim at reducing the precision of weights while trying to keep the model\u2019s inference results as accurate as possible (a.k.a as close as possible to bfloat16). Note that quantization works especially well for text generation since all we care about is choosing the set of most likely next tokens and don\u2019t really care about the exact values of the next token logit distribution. All that matters is that the next token logit distribution stays roughly the same so that an argmax or topk operation gives the same results.\n", "\n", "There are various quantization techniques, which we won\u2019t discuss in detail here, but in general, all quantization techniques work as follows:\n", "\n", "1. Quantize all weights to the target precision\n", "2. Load the quantized weights, and pass the input sequence of vectors in bfloat16 precision\n", "3. Dynamically dequantize weights to bfloat16 to perform the computation with their input vectors in bfloat16 precision\n", "\n", "In a nutshell, this means that inputs-weight matrix multiplications, with $X$ being the inputs, $W$ being a weight matrix and $Y$ being the output:\n", "$$\n", "Y=X\u2217W\n", "$$\n", "are changed to\n", "$$\n", "Y=X\u2217\\text{dequantize}(W)\n", "$$\n", "for every matrix multiplication. Dequantization and re-quantization is performed sequentially for all weight matrices as the inputs run through the network graph.\n", "\n", "Therefore, inference time is often not reduced when using quantized weights, but rather increases. Enough theory, let\u2019s give it a try! To quantize the weights with Transformers, you need to make sure that the bitsandbytes library is installed."]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# !pip install bitsandbytes"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["from transformers import BitsAndBytesConfig\n", "\n", "config = BitsAndBytesConfig(\n", "    load_in_8bit=True,\n", ")\n", "\n", "quantized_model = AutoModelForCausalLM.from_pretrained(\n", "    model_path, torch_dtype=\"auto\",\n", "    quantization_config=config,\n", "    attn_implementation=\"flash_attention_2\",\n", "    device_map=\"cuda:0\",\n", ")"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["pipe = pipeline(\"text-generation\", model=quantized_model, tokenizer=tokenizer)\n", "\n", "result = pipe(prompt, max_new_tokens=300, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"][len(prompt):]\n", "result"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Amazing! Significant less! We\u2019re down to around 8 GBs and could therefore run this model on consumer GPUs like the 4070. We\u2019re seeing a very nice gain in memory efficiency and more or less no degradation to the model\u2019s output. However, we can also notice a slight slow-down during inference."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 2.2 Benchmarking the Quantized Model\n", "\n", "In this section, we will benchmark the full precision and quantized models to see the precision loss and the inference performance trade-off.\n", "\n", "The benchmarking codes are from Lab6, so we will not go into too much detail here."]}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["import os\n", "import gc\n", "import torch\n", "import time\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, BitsAndBytesConfig\n", "from tqdm.notebook import tqdm\n", "import re\n", "from datasets import load_dataset\n", "\n", "model_path = \"/ssdshare/share/Meta-Llama-3-8B-Instruct/\"\n", "\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["def chat_resp(model, tokenizer, question_list):\n", "    pipe = pipeline(\n", "        \"text-generation\",\n", "        model=model,\n", "        tokenizer=tokenizer,\n", "    )   \n", "    generation_args = {\n", "        \"pad_token_id\": tokenizer.eos_token_id,\n", "        \"max_new_tokens\": 1024,\n", "        \"return_full_text\": False,\n", "        \"do_sample\": True,\n", "        \"temperature\": 0.7,\n", "    }\n", "\n", "    output = pipe(question_list, **generation_args)\n", "    return output\n", "\n", "def chat_resp_batched(model, tokenizer, question_list, batch_size=4):\n", "    batches = [question_list[i:i + batch_size] for i in range(0, len(question_list), batch_size)]\n", "    all_responses = []\n", "    for batch in tqdm(batches, desc=\"Processing batches\"):\n", "        responses = chat_resp(model, tokenizer, batch)\n", "        all_responses.extend(responses)\n", "    return all_responses\n", "\n", "def gsm8k_prompt(question):\n", "    chat = [\n", "        {\"role\": \"system\", \"content\": \"\"\"Please solve the given math problem by providing a detailed, step-by-step explanation. Begin by outlining each step involved in your solution, ensuring clarity and precision in your calculations. After you have worked through the problem, conclude your response by summarizing the solution and stating the final answer as a single exact numerical value on the last line. \"\"\"},\n", "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n", "    ]\n", "    return chat\n", "\n", "dataset = load_dataset(\"gsm8k\", \"main\")  # read directly from huggingface\n", "\n", "# to save time, we only use a small subset, you can use larger subset\n", "subset = dataset['test'][0:40]\n", "questions = subset['question']\n", "answers = subset['answer']"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["def get_exact_answer(x):\n", "    i = x.index('####')\n", "    return x[i+5:].strip('\\n')\n", "\n", "num_answers = list(map(get_exact_answer, answers))\n", "print(num_answers)\n", "\n", "def get_numbers(s):\n", "    number =[]\n", "    lines = s.split('\\n')\n", "    for i in range(-1, -len(lines), -1):\n", "        number = re.findall(r'\\d+(?:\\.\\d+)?', lines[i])\n", "        if len(number) > 0:\n", "            break\n", "    if (len(number) == 0):\n", "        return '-9999'\n", "    return number[-1]  # the last number is the answer"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["# test running time for the quantized model\n", "\n", "question_prompts = [gsm8k_prompt(q) for q in questions]\n", "\n", "start_time_quantized_model = time.time()\n", "quantized_model_answers = chat_resp_batched(quantized_model, tokenizer, question_prompts, batch_size=4)\n", "quantized_model_answers = [get_numbers(resp[0]['generated_text']) for resp in quantized_model_answers]\n", "end_time_quantized_model = time.time()\n", "\n", "print(\"Quantized model time: %s seconds\" % (end_time_quantized_model - start_time_quantized_model))\n", "\n", "# clean up\n", "del quantized_model, pipe\n", "gc.collect()\n", "with torch.no_grad():\n", "    torch.cuda.empty_cache()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# test the full model\n", "full_model = AutoModelForCausalLM.from_pretrained(\n", "    model_path,\n", "    torch_dtype=\"auto\",\n", "    attn_implementation=\"flash_attention_2\",\n", "    device_map=\"cuda:0\"\n", ")\n", "\n", "start_time_full_model = time.time()\n", "full_model_answers = chat_resp_batched(full_model, tokenizer, question_prompts, batch_size=4)\n", "full_model_answers = [get_numbers(resp[0]['generated_text']) for resp in full_model_answers]\n", "end_time_full_model = time.time()\n", "\n", "print(\"Full model time: %s seconds\" % (end_time_full_model - start_time_full_model))"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# compare the results between the full model and the quantized model\n", "\n", "full_model_answers = [float(x) for x in full_model_answers]\n", "quantized_model_answers = [float(x) for x in quantized_model_answers]\n", "num_answers = [float(x) for x in num_answers]\n", "\n", "full_model_error_cnt, quantized_model_error_cnt = 0, 0\n", "for i in range(0, len(full_model_answers)):\n", "    if full_model_answers[i] != quantized_model_answers[i] or full_model_answers[i] != num_answers[i]:\n", "        print(\"For question %s\" % questions[i])\n", "        print(\"full model answer: %s quantized model answer: %s correct answer: %s\\n\" % (full_model_answers[i], quantized_model_answers[i], num_answers[i]))\n", "        if full_model_answers[i] != num_answers[i]:\n", "            full_model_error_cnt += 1\n", "        if quantized_model_answers[i] != num_answers[i]:\n", "            quantized_model_error_cnt += 1\n", "\n", "print(f\"Full model error count: {full_model_error_cnt} out of {len(full_model_answers)}\")\n", "print(f\"Quantized model error count: {quantized_model_error_cnt} out of {len(quantized_model_answers)}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We observe little accuracy loss and noticeable inference performance degradation.\n", "\n", "That's acceptable because we are trading off a bit of accuracy for a lot of memory(50\\%). Novel quantization techniques other than `bitsandbytes` have been developed to patch up the performance degradation, you can try them yourself."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Observe the experiments above, and summarize the difference between the full model and the quantized model here\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# find a model on huggingface and try to optimize it to fit it into 24GB of VRAM \n", "# using 4 bit quantization and observe the performance and accuracy loss\n", "# you can reuse the same benchmark code above\n", "# Note:\n", "# If you encounter CUDA out of memory error,\n", "# you can assign None to the variables which were bound to the models and pipes,\n", "# then call `gc.collect()` and `torch.cuda.empty_cache()`.\n", "# Python garbage collector will handle the freeing of the GPU memory.\n", "# See the codes above.\n", "# You can also restart the jupyter notebook kernel."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}
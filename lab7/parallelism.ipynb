{"cells": [{"cell_type": "markdown", "id": "2eb0f1de", "metadata": {}, "source": ["## Running on multiple GPUs using Hugging Face Transformers\n", "\n", "Naive pipeline parallelism is supported out of the box. For this, simply load the model with device=\"auto\" which will automatically place the different layers on the available GPUs.\n", "\n", "Your task:\n", "\n", "1. Create a pod with two 24GB GPUs.\n", "\n", "2. Try to run the model with device=\"auto\" and see how much VRAM is used. You can also try to run the model with device_map=\"auto\" which will automatically place the different layers on the available GPUs. This is a more advanced version of pipeline parallelism that allows for more flexibility in how the model is distributed across GPUs."]}, {"cell_type": "code", "execution_count": null, "id": "e04b442f", "metadata": {}, "outputs": [], "source": ["model_path = \"/ssdshare/share/Meta-Llama-3-8B-Instruct/\"\n", "# TODO(Your Task): Load the model to multiple GPUs and check the GPU memory usage"]}, {"cell_type": "markdown", "id": "19d168c7", "metadata": {}, "source": ["The GPU memory usage of loading the model to only one GPU is \\_\\_\\_\\_\\_\\_\\_\\_.\n", "\n", "The GPU memory usage of loading the model with device=\"auto\" is \\_\\_\\_\\_\\_\\_\\_\\_. The GPU memory usage of loading the model with device_map=\"auto\" is \\_\\_\\_\\_\\_\\_\\_\\_.\n", "\n", "The number of GPUs you used is \\_\\_\\_\\_\\_\\_\\_\\_.\n", "\n", "Does the numbers above make sense?"]}, {"cell_type": "code", "execution_count": null, "id": "1d20f298", "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 5}
{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 4 Using LLMs as Judges\n", "\n", "Previous sections in this lab all focus on easy to evaluate outputs: single word, multiple choice, true/false etc.  The evaluation of longer text (summary) only uses NLP metrics which might not be too related to its semantics.  In order to better understand the semantics, and other \"quality\" of the answer, we can use a (more powerful) LLM as a judge. \n", "\n", "In this subsection, we'll explore how to use LLMs as judges to evaluate model outputs. This approach leverages the capabilities of large language models to assess the quality of responses from other models, providing a flexible evaluation method that can be applied across various tasks. And llm-as-judge is also a well-used method in industry and LLM research field now. For more detailed knowledge, I recommand you to read this survey: https://arxiv.org/pdf/2411.15594?\n", "\n", "Here, we provide a simplified process and discusses several common challenges to llm-as-judge."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.1 Setting up\n", "\n", "We use the same task as 01_evaluation, i.e., CNN_DAILYMAIL here."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["#Setting up the environment\n", "#Continue to use OpenAI API and INFINI Platform\n", "from dotenv import load_dotenv\n", "import os\n", "import threading\n", "from openai import OpenAI\n", "import time\n", "import random\n", "from openai import RateLimitError\n"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# we use API in this section\n", "load_dotenv()\n", "openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n", "openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n", "client = OpenAI(api_key=openai_api_key, base_url=openai_base_url)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# these are just duplicates from section 2.  You can skip them if you have already read section 2.\n", "\n", "def get_llm_output(model_name, question_content, question_id, output, semaphore=None):\n", "    try:\n", "        retries = 5\n", "        for attempt in range(retries):\n", "            try:\n", "                if isinstance(question_content, str):\n", "                    messages = [{\"role\": \"user\", \"content\": question_content}]\n", "                else:\n", "                    messages = [{\"role\": \"user\", \"content\": q} for q in question_content]\n", "\n", "                chat_response = client.chat.completions.create(\n", "                    model=model_name,\n", "                    messages=messages,\n", "                    max_tokens=2048,\n", "                    temperature=0,\n", "                )\n", "\n", "                llm_answer = chat_response.choices[0].message.content.strip()\n", "                output[question_id] = llm_answer\n", "                break  # \u6210\u529f\u5c31\u9000\u51fa retry \u5faa\u73af\n", "\n", "            except RateLimitError as e:\n", "                wait_time = random.uniform(1, 3) * (2 ** attempt)\n", "                print(f\"RateLimitError on Q{question_id}, retry {attempt+1}/{retries}, wait {wait_time:.1f}s\")\n", "                time.sleep(wait_time)\n", "\n", "            except Exception as e:\n", "                print(f\"Error on Q{question_id}: {e}\")\n", "                break \n", "\n", "        else:\n", "            output[question_id] = \"\"\n", "\n", "    finally:\n", "        if semaphore:\n", "            semaphore.release()\n", "\n", "def get_llm_output_parallel(model_name, question_contents, max_threads=5):\n", "    # Create threads for each question\n", "    output = {}\n", "    threads = []\n", "    semaphore = threading.Semaphore(max_threads)\n", "    for question_id, question_content in tqdm(enumerate(question_contents)):\n", "        semaphore.acquire() \n", "        thread = threading.Thread(target=get_llm_output, args=(model_name, question_content, question_id, output, semaphore))\n", "        threads.append(thread)\n", "        thread.start()\n", "\n", "    # Wait for all threads to complete\n", "    for thread in threads:\n", "        thread.join()\n", "\n", "    sorted_keys = sorted(output.keys())\n", "    sorted_outputs = [output[key] for key in sorted_keys]        \n", "    return sorted_outputs\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's load the dataset.  The dataset is described as\n", "\n", "1. Source:\n", "- Created from articles published on CNN and Daily Mail websites\n", "- Articles were collected between 2007 and 2015\n", "2. Each example consists of:\n", "- A news article (document)\n", "- Bullet point highlights/summaries of the article\n", "- The highlights are treated as reference summaries\n", "\n", "The dataset is popular in evaluating summarization tasks. "]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset, load_from_disk\n", "\n", "d = load_from_disk('/ssdshare/share/data/cnn_dailymail/')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def create_prompt(x):\n", "    s = \"Please summarize the following news article in no more than 30 words.\\n %s\" %(x['article'])\n", "    x['question_content'] = s\n", "    return x\n", "\n", "d = d['test'].map(create_prompt) # Use test set to evaluate\n", "d"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.2 The basic judge model (Direct Scoring)\n", "\n", " \n", "We'll first generate summaries using different models, then have a judge model evaluate these summaries, and finally analyze the results to determine which model performs better at the summarization task, generating a radar graph.\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# let's create a judge model first (by prompting it)\n", "\n", "import json\n", "import os\n", "from tqdm import tqdm\n", "import re\n", "\n", "# Here is the prompt for the judge model\n", "def create_judge_prompt(article, reference, model_answer, model_name):\n", "    prompt = f\"\"\"Please evaluate the following summary of a news article. \n", "    \n", "Original Article:\n", "{article}\n", "\n", "# Reference Summary:\n", "# {reference}\n", "\n", "Model ({model_name}) Summary:\n", "{model_answer}\n", "\n", "Please rate the model's summary on the following dimensions on a scale of 1-20 (1 being the not good one, 20 being the perfect one):\n", "1. Elegance: Comparing to the reference summary (which is 10 points), how well-written and stylistically pleasing is the summary?\n", "2. Fluency: Comparing to the reference summary (which is 10 points), how grammatically correct and easy to read is the summary?\n", "3. Conciseness: Comparing to the reference summary (which is 10 points), how efficiently does the summary convey the key information?\n", "4. Creativity: Comparing to the reference summary (which is 10 points), how original and innovative is the summary compared to a standard summary?\n", "\n", "For each dimension, provide a score and a brief explanation. Format your response as follows:\n", "{{\n", "  \"elegance\": {{\n", "    \"score\": <score>,\n", "    \"explanation\": \"<explanation>\"\n", "  }},\n", "  \"fluency\": {{\n", "    \"score\": <score>,\n", "    \"explanation\": \"<explanation>\"\n", "  }},\n", "  \"conciseness\": {{\n", "    \"score\": <score>,\n", "    \"explanation\": \"<explanation>\"\n", "  }},\n", "  \"creativity\": {{\n", "    \"score\": <score>,\n", "    \"explanation\": \"<explanation>\"\n", "  }}\n", "}}\n", "\"\"\"\n", "    return prompt\n", "\n", "def parse_judge_output(judge_output):\n", "    # this inherited function is widely used in research repositories\n", "    # it does some simple fixes to the output to make it a valid JSON\n", "    try:\n", "        # \u5c1d\u8bd5\u63d0\u53d6\u82b1\u62ec\u53f7\u5305\u88f9\u7684\u90e8\u5206\n", "        match = re.search(r\"\\{.*\\}\", judge_output, re.DOTALL)\n", "        if not match:\n", "            raise ValueError(\"No JSON object found in the output.\")\n", "        \n", "        json_str = match.group(0)\n", "\n", "        # \u7b80\u5355\u4fee\u590d\u5e38\u89c1 JSON \u9519\u8bef\uff1a\u7f3a\u9017\u53f7\u3001\u7f3a\u62ec\u53f7\u7b49\n", "        json_str = re.sub(r'(\"explanation\"\\s*:\\s*\"[^\"]+)\"\\s*(\")', r'\\1,\\2', json_str)  # \u8865\u9017\u53f7\n", "        json_str = json_str.strip()\n", "\n", "        # \u5c1d\u8bd5\u89e3\u6790 JSON\n", "        return json.loads(json_str)\n", "\n", "    except Exception as e:\n", "        print(f\"Error parsing judge output: {e}\")\n", "        print(f\"Raw output: {judge_output}\")\n", "        return {\n", "            \"elegance\": {\"score\": 10, \"explanation\": \"Failed to parse\"},\n", "            \"fluency\": {\"score\": 10, \"explanation\": \"Failed to parse\"},\n", "            \"conciseness\": {\"score\": 10, \"explanation\": \"Failed to parse\"},\n", "            \"creativity\": {\"score\": 10, \"explanation\": \"Failed to parse\"}\n", "        }\n", "\n", "# Create output directory if it doesn't exist\n", "# note that because the output is expensive to get, better to save it to a file so we can examine it later\n", "output_dir = \"~/judge_results\"  # avoid putting it into the current directory (poputing your codebase)\n", "os.makedirs(output_dir, exist_ok=True)\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import os\n", "import json\n", "from tqdm import tqdm\n", "\n", "def evaluate_model(\n", "    judge_model: str,\n", "    model_name: str,\n", "    dataset: dict,\n", "    evaluate_n: int = 10,\n", "    output_dir: str = output_dir,\n", "    model_threads: int = 5,\n", ") -> dict:\n", "\n", "\n", "    # 1. take a subset of the dataset\n", "    articles   = dataset['article'][:evaluate_n]\n", "    questions  = dataset['question_content'][:evaluate_n]\n", "    references = dataset['highlights'][:evaluate_n]\n", "\n", "    # 2. ask the model to generate answers\n", "    model_answers = get_llm_output_parallel(\n", "        model_name, questions, max_threads=model_threads\n", "    )\n", "\n", "    results = []\n", "\n", "    # 3. generate a score using the judge model\n", "    for i in tqdm(range(evaluate_n), desc=f\"Judging {model_name}\"):\n", "        judge_prompt  = create_judge_prompt(\n", "            articles[i], references[i], model_answers[i], model_name\n", "        )\n", "        judge_response = get_llm_output_parallel(\n", "            judge_model, [judge_prompt], max_threads=1\n", "        )[0]\n", "\n", "        scores = parse_judge_output(judge_response)\n", "\n", "        results.append({\n", "            \"article_id\"  : dataset['id'][i],\n", "            \"article\"     : articles[i],\n", "            \"reference\"   : references[i],\n", "            \"model_answer\": model_answers[i],\n", "            \"scores\"      : scores\n", "        })\n", "\n", "    # 4. save the results to a file\n", "    os.makedirs(output_dir, exist_ok=True)\n", "    output_file = os.path.join(\n", "        output_dir, f\"{model_name}_evaluations_{judge_model}.jsonl\"\n", "    )\n", "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n", "        for r in results:\n", "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n", "\n", "    # 5. compute the average scores\n", "    dims = [\"elegance\", \"fluency\", \"conciseness\", \"creativity\"]\n", "    avg_scores = {d: sum(r[\"scores\"][d][\"score\"] for r in results) / evaluate_n\n", "                  for d in dims}\n", "\n", "    # 6. print the results (optional)\n", "    print(f\"\\n{model_name} average scores (judged by {judge_model}):\")\n", "    for d, s in avg_scores.items():\n", "        print(f\"  {d:<12}: {s:.2f}\")\n", "    print(f\"\\nResults saved to {output_file}\")\n", "\n", "    return {\n", "        \"output_file\": output_file,\n", "        \"avg_scores\" : avg_scores,\n", "    }\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Use 10 samples for evaluation to save time.\n", "evaluate_n = 10\n", "\n", "judge_model = \"deepseek-v3\"\n", "model_names = [\"qwen2.5-7b-instruct\", \"llama-2-7b-chat\", \"deepseek-v3\"]\n", "\n", "for model_name in model_names:\n", "    evaluate_model(judge_model, model_name, d, evaluate_n)\n"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["import os, json, numpy as np\n", "import matplotlib.pyplot as plt\n", "\n", "# utility function to plot the results\n", "def plot_model_comparison_radar(model_names, judge_model, output_dir=output_dir):\n", "\n", "    all_avg_scores = {}\n", "    for model_name in model_names:\n", "        path = os.path.join(output_dir,\n", "                            f\"{model_name}_evaluations_{judge_model}.jsonl\")\n", "        if not os.path.exists(path):\n", "            raise FileNotFoundError(f\"Result file not found: {path}\")\n", "        \n", "        with open(path, \"r\", encoding=\"utf-8\") as f:\n", "            results = [json.loads(line) for line in f]\n", "        \n", "        # Initialize the dimensions\n", "        dims = [\"elegance\", \"fluency\", \"conciseness\", \"creativity\"]\n", "        avg = {d: 0.0 for d in dims}\n", "        for r in results:\n", "            for d in dims:\n", "                avg[d] += r[\"scores\"][d][\"score\"]\n", "        for d in dims:\n", "            avg[d] /= len(results)\n", "        all_avg_scores[model_name] = avg\n", "    \n", "    dimensions = list(next(iter(all_avg_scores.values())).keys())\n", "    N = len(dimensions)\n", "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n", "    angles += angles[:1]  # close the circle\n", "    \n", "    fig, ax = plt.subplots(figsize=(8, 6), subplot_kw=dict(polar=True))\n", "    \n", "    plt.xticks(angles[:-1], dimensions, size=12)\n", "    \n", "    ax.set_rlabel_position(0)\n", "    plt.yticks([4, 8, 12, 16, 20], [\"4\", \"8\", \"12\", \"16\", \"20\"], color=\"grey\", size=10)\n", "    plt.ylim(0, 20)\n", "    \n", "    for model_name in model_names:\n", "        vals = [all_avg_scores[model_name][d] for d in dimensions]\n", "        vals += vals[:1]\n", "        ax.plot(angles, vals, linewidth=2, linestyle='solid', label=model_name)\n", "        ax.fill(angles, vals, alpha=0.1)\n", "    \n", "    plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n", "    plt.title(f\"LLM Judge Evaluation (Judge: {judge_model})\", size=14, y=1.08)\n", "    plt.tight_layout()\n", "    plt.show()\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plot_model_comparison_radar(\n", "    model_names=model_names,\n", "    judge_model=judge_model\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.3 Bias1: Same family models\n", "\n", "The researchers found that llm as judge had very some problems, such as the model with lower ability sometimes cannot be a good judge, and the model's tendency to give higher scores to the answers of its own or model from same 'famliy'. We will discuss and investigate this in this section."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# let's test model's bias here\n", "# let's first use qwen2.5 model as judge\n", "judge_model = \"qwen2.5-7b-instruct\"\n", "model_names = [\"qwen2.5-72b-instruct\", \"llama-3.3-70b-instruct\", \"deepseek-v3\"]\n", "for model_name in model_names:\n", "    evaluate_model(judge_model, model_name, d, evaluate_n)\n", "\n", "plot_model_comparison_radar(\n", "    model_names=model_names,\n", "    judge_model=judge_model\n", ")\n", "\n", "\n", "# What will happen if we choose the range of socre to be 1-5. 1-10. 1-100?\n", "# Let's test it here"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["judge_model = \"deepseek-v3\"\n", "# using the same models as before\n", "for model_name in model_names:\n", "    evaluate_model(judge_model, model_name, d, evaluate_n)\n", "\n", "\n", "plot_model_comparison_radar(\n", "    model_names=model_names,\n", "    judge_model=judge_model\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.4 Pairwise Comparison judge, and Bias2: positions.\n", "\n", "Another problem is llm's judgement is depend on position."]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["# here is the prompt for pairwise comparison judge\n", "def compare_model_summaries(model1_name, model2_name, judge_model, num_samples):\n", "    \"\"\"\n", "    Compare summaries from two different models using a judge model.\n", "    \"\"\"\n", "    import json, os, random\n", "    from tqdm import tqdm\n", "\n", "    model1_file = f\"judge_results/{model1_name}_evaluations_{judge_model}.jsonl\"\n", "    model2_file = f\"judge_results/{model2_name}_evaluations_{judge_model}.jsonl\"\n", "\n", "    with open(model1_file, \"r\") as f:\n", "        model1_data = [json.loads(l) for l in f]\n", "\n", "    with open(model2_file, \"r\") as f:\n", "        model2_data = [json.loads(l) for l in f]\n", "\n", "    common_ids = list(\n", "        {d[\"article_id\"] for d in model1_data}.intersection(\n", "            {d[\"article_id\"] for d in model2_data}\n", "        )\n", "    )\n", "    if len(common_ids) < num_samples:\n", "        print(f\"Warning: only {len(common_ids)} common articles, using all.\")\n", "        num_samples = len(common_ids)\n", "\n", "    selected_ids = random.sample(common_ids, num_samples)\n", "    m1 = {d[\"article_id\"]: d for d in model1_data}\n", "    m2 = {d[\"article_id\"]: d for d in model2_data}\n", "\n", "    wins = {model1_name: 0, model2_name: 0, \"tie\": 0}\n", "\n", "    for aid in tqdm(selected_ids, desc=f\"{model1_name} vs {model2_name}\"):\n", "        art = m1[aid][\"article\"]\n", "        ref = m1[aid][\"reference\"]\n", "        s1  = m1[aid][\"model_answer\"]\n", "        s2  = m2[aid][\"model_answer\"]\n", "\n", "        prompt = f\"\"\"You are an expert judge evaluating two summaries of a news article.\n", "\n", "Article: {art}\n", "\n", "Reference Summary: {ref}\n", "\n", "Summary A: {s1}\n", "\n", "Summary B: {s2}\n", "\n", "Compare the two summaries on elegance, fluency, conciseness, and creativity.\n", "Which summary is better overall? Respond with **only** \"A\", \"B\", or \"Tie\" on the first line,\n", "then optionally give a short explanation on the next line.\n", "\"\"\"\n", "\n", "        resp = client.chat.completions.create(\n", "            model=judge_model,\n", "            messages=[{\"role\": \"user\", \"content\": prompt}],\n", "        )\n", "\n", "        judge_text = resp.choices[0].message.content.strip()\n", "        first_line = judge_text.splitlines()[0].strip().upper()\n", "\n", "        if first_line.startswith(\"A\"):\n", "            wins[model1_name] += 1\n", "        elif first_line.startswith(\"B\"):\n", "            wins[model2_name] += 1\n", "        else:\n", "            wins[\"tie\"] += 1\n", "\n", "    print(f\"\\nResults ({judge_model} as judge)\")\n", "    print(f\"{model1_name} wins: {wins[model1_name]}\")\n", "    print(f\"{model2_name} wins: {wins[model2_name]}\")\n", "    print(f\"Ties: {wins['tie']}\")\n", "    return wins"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["compare_results = compare_model_summaries(\n", "    \"qwen2.5-72b-instruct\", \n", "    \"llama-3.3-70b-instruct\", \n", "    \"deepseek-v3\", \n", "    evaluate_n\n", ")\n", "\n", "compare_results = compare_model_summaries(\n", "    \"llama-3.3-70b-instruct\",\n", "    \"qwen2.5-72b-instruct\", \n", "    \"deepseek-v3\", \n", "    evaluate_n\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["See the differences?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.5 Further Questions about LLM-as-Judge\n", "\n", "There are several interesting questions about using LLMs as judges that you might want to explore:\n", "\n", "1. **Judge Bias**: Does the choice of judge model significantly impact the results? \n", "\n", "2. **Prompt Sensitivity**: How sensitive are the judgments to the specific wording of the prompt? \n", "\n", "3. **Consistency**: How consistent are LLM judges across multiple evaluations of the same content? \n", "\n", "4. **Ability Limitation**: How do models perform when judging the tasks they cannot achieve perfectly?\n", "\n", "5. **Correlation with Human Judgment**: How well do LLM judgments align with human preferences? \n", "\n", "6. **Metric Weighting**: Do certain metrics (like fluency or creativity) have more influence on the overall judgment than others?\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#### your task ####\n", "# find a dataset that you are interested in, and use llm-as-judge to evaluate the model.\n", "# plot the radar graph of the results (with dimensions of your choice, but as least 4 dimensions)\n", "# you should find two models with as much performance difference as possible.\n", "# to do so, you need choose a good evaluation set, as well as a good judge model.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}
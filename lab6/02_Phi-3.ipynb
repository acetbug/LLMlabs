{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["## 3 Evaluating Locally deployed models"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 Load the (Quantized) model to a single GPU"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["!pip install flash-attn"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import accelerate, bitsandbytes\n", "import torch, os\n", "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n", "\n", "from transformers import pipeline\n", "\n", "os.environ[\"BNB_CUDA_VERSION\"]=\"125\"\n", "\n", "## choose one of the local models.  Let's use a small one for faster evaluation.\n", "model_path = '/ssdshare/share/Phi-3-mini-128k-instruct/'\n", "\n", "# here is how you load a local model using haggingface api\n", "tokenizer = AutoTokenizer.from_pretrained(model_path)\n", "model = AutoModelForCausalLM.from_pretrained(model_path, \n", "                                             device_map=\"cuda:0\", \n", "                                             torch_dtype=\"auto\", \n", "                                             trust_remote_code=True) \n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Verify that the model is loaded to GPU (look at the memory utilization)."]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["# let's check the GPU memory utilization after loading the model\n", "!nvidia-smi"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 Generate responses locally"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def chat_resp(model, tokenizer, question_list):\n", "    # here is how you use the pipeline to generate local responses\n", "    pipe = pipeline(\n", "        \"text-generation\",\n", "        model=model,\n", "        tokenizer=tokenizer,\n", "    )   \n", "    generation_args = {\n", "        \"max_new_tokens\": 500,\n", "        \"return_full_text\": False,\n", "        \"temperature\": 0.6,\n", "        \"do_sample\": True,\n", "    }\n", "\n", "    output = pipe(question_list, **generation_args)  # note that you send in a list of questions (faster)\n", "    # here is how you get the response               # however if you send too many questions, it will run out of memory\n", "    return output\n", "\n", "def chat_resp_batched(model, tokenizer, question_list, batch_size=4):\n", "    # Split a large question list into batches of the specified size, to avoid running out of memory\n", "    batches = [question_list[i:i + batch_size] for i in range(0, len(question_list), batch_size)]\n", "    all_responses = []\n", "    for batch in batches:\n", "        print(f\"processing batch: %s \" % batch)\n", "        responses = chat_resp(model, tokenizer, batch)\n", "        all_responses.extend(responses)\n", "    return all_responses\n", "\n", "def gsm8k_prompt(question):\n", "    # add system prompt to the question\n", "    chat = [\n", "        {\"role\": \"system\", \"content\": \"\"\"Please solve the given math problem by providing a detailed, step-by-step explanation. Begin by outlining each step involved in your solution, ensuring clarity and precision in your calculations. After you have worked through the problem, conclude your response by summarizing the solution and stating the final answer as a single exact numerical value on the last line. \"\"\"},\n", "        {\"role\": \"user\", \"content\": \"Question: \" + question},\n", "    ]\n", "    return chat"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["## Test the model with a sample question\n", "p = gsm8k_prompt(\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\")\n", "p = [p]  # remember to send in a list of questions\n", "\n", "chat_resp(model, tokenizer, p)  # p is the list of questions\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 Prepare the evaluation datasets"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# add proxy to access huggingface ...\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset\n", "dataset = load_dataset(\"gsm8k\", \"main\")  # read directly from huggingface\n", "\n", "# if you want to use a local dataset, you can use the following code\n", "# from datasets import load_dataset, load_from_disk\n", "# dataset = load_from_disk(\"/ssdshare/share/gsm8k\")\n", "\n", "# to save time, we only use a small subset\n", "subset = dataset['test'][5:12]\n", "questions = subset['question']\n", "answers = subset['answer']\n", "\n", "dataset"]}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# We only want the numeric answers from the dataset for evalation (maybe a bad choice?)\n", "\n", "def get_exact_answer(x):\n", "    i = x.index('####')\n", "    return x[i+5:].strip('\\n')\n", "\n", "num_answers = list(map(get_exact_answer, answers))\n", "print(num_answers)\n"]}, {"cell_type": "code", "execution_count": 12, "metadata": {}, "outputs": [], "source": ["# this is very tentative and bad way to find the exact answer, consider fixing it. \n", "\n", "import re\n", "def get_numbers(s):\n", "    number =[]\n", "    lines = s.split('\\n')\n", "    for i in range(-1, -len(lines), -1):\n", "        number = re.findall(r'\\d+(?:\\.\\d+)?', lines[i])\n", "        if len(number) > 0:\n", "            break\n", "    if (len(number) == 0):\n", "        return '-9999'\n", "    return number[-1]  # the last number is the answer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 Evaluate!"]}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["question_prompts = [gsm8k_prompt(q) for q in questions]\n", "resps = chat_resp_batched(model, tokenizer, question_prompts, batch_size=10)\n"]}, {"cell_type": "code", "execution_count": 17, "metadata": {}, "outputs": [], "source": ["llm_answers = []\n", "\n", "for resp in resps:\n", "    gen_text = resp[0]['generated_text']\n", "    print(\"--------\")\n", "    print(gen_text)\n", "    print(\"--------\")\n", "    num = get_numbers(gen_text)\n", "    print(num)\n", "    llm_answers.append(num)\n", "    print(\"---------\" )\n", "    print(llm_answers)"]}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["print(llm_answers)\n", "print(num_answers)"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["## manual way to compute the correct rate\n", "\n", "error = 0\n", "for i in range(0, len(llm_answers)):\n", "    if llm_answers[i] != num_answers[i]:\n", "        error += 1\n", "print(f\"number of errors: %s \\n correct rate: %s\" % (error, 1 - error / len(llm_answers))) "]}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["## the way of using HuggingFace evaluate functions\n", "\n", "import evaluate\n", "exact_match = evaluate.load(\"exact_match\")\n", "results = exact_match.compute(predictions=llm_answers, references=num_answers)\n", "print(results)"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# do not forget to clean the gpu memory\n", "import torch\n", "torch.cuda.empty_cache()\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# check the GPU memory utilization\n", "!nvidia-smi\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 2}
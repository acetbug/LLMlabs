{"cells": [{"cell_type": "markdown", "id": "324be893", "metadata": {}, "source": ["# Lab 6 Performance Evaluation"]}, {"cell_type": "markdown", "id": "d2b05339", "metadata": {}, "source": ["First, we need to install a few more dependencies. "]}, {"cell_type": "code", "execution_count": 1, "id": "91bb8bcf-229f-43ad-844f-f5114178be25", "metadata": {}, "outputs": [], "source": ["!pip3 install -r requirements.txt\n", "# !pip3 install -r requirements.txt -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com"]}, {"cell_type": "code", "execution_count": 2, "id": "0771d566", "metadata": {}, "outputs": [], "source": ["import os\n", "import requests\n", "import threading\n", "import evaluate\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import re\n", "import json\n", "import warnings\n", "import threading\n", "import pandas as pd\n", "import random as rd\n", "from tqdm import tqdm"]}, {"cell_type": "code", "execution_count": 3, "id": "44595b1d", "metadata": {}, "outputs": [], "source": ["# add proxy to access openai ...\n", "import os\n", "os.environ['HTTP_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['HTTPS_PROXY']=\"http://Clash:QOAF8Rmd@10.1.0.213:7890\"\n", "os.environ['ALL_PROXY']=\"socks5://Clash:QOAF8Rmd@10.1.0.213:7893\""]}, {"cell_type": "markdown", "id": "fd59362b", "metadata": {}, "source": ["# 1. NLP related Metrics\n", "You can try to change the `prediction` and `reference` in the following sample code and see the range of the metrics."]}, {"cell_type": "markdown", "id": "59a76847-dfab-47cf-903b-3d427a41019c", "metadata": {}, "source": ["## 1.1 Accuracy\n", "Accuracy is the proportion of correct predictions among the total number of cases processed. \n", "\n", "It can be computed with: Accuracy = (TP + TN) / (TP + TN + FP + FN) Where: TP: True positive TN: True negative FP: False positive FN: False negative\n", "\n", "The metric ranges from 0 to 1 and a higher score is better."]}, {"cell_type": "code", "execution_count": 4, "id": "cafd11de", "metadata": {}, "outputs": [], "source": ["accuracy = evaluate.load('accuracy')"]}, {"cell_type": "code", "execution_count": 5, "id": "3d17ece6", "metadata": {}, "outputs": [], "source": ["accuracy.compute(references=[0,1,0,1], predictions=[0,1,0,0])"]}, {"cell_type": "code", "execution_count": 6, "id": "3546dbb9", "metadata": {}, "outputs": [], "source": ["accuracy.compute(references=[0,1,2,1,1], predictions=[1,2,2,1,1])"]}, {"cell_type": "markdown", "id": "f612208e", "metadata": {}, "source": ["## 1.2 BLEU\n", "BLEU, or the Bilingual Evaluation Understudy, is a score for comparing a candidate translation of text to one or more reference translations. The higher the BLEU score, the more similar the generated text is to the reference text. Its value ranges from 0 to 1.\n", "\n", "This metric compares the n-gram overlap between the machine translation result and the reference translation where an n-gram is a sequence of consecutive n words. \n", "\n", "The metric ranges from 0 to 1 and a higher score is better.\n"]}, {"cell_type": "markdown", "id": "911f7fd7-f85b-43bc-96f2-d40d7349608a", "metadata": {}, "source": ["### Single sentence score\n", "Observe how the score varies as the input changes."]}, {"cell_type": "code", "execution_count": 7, "id": "c447038e-bca2-4fee-ae43-6eb2d2221fd3", "metadata": {}, "outputs": [], "source": ["bleu = evaluate.load(\"bleu\")"]}, {"cell_type": "code", "execution_count": 8, "id": "3d2bc561", "metadata": {}, "outputs": [], "source": ["prediction1 = 'the cat is on the yoga mat'\n", "reference1 = 'the cat sat on the yoga mat'\n", "bleu_score = bleu.compute(predictions=[prediction1], references=[reference1])\n", "bleu_score['bleu']"]}, {"cell_type": "code", "execution_count": 9, "id": "1f3d5178", "metadata": {}, "outputs": [], "source": ["prediction2 = 'the value of life lies in what you create for others not in what you possess'\n", "reference2 = 'the meaning of life lies in what you give to others not in what you receive'\n", "bleu_score = bleu.compute(predictions=[prediction2], references=[reference2])\n", "bleu_score"]}, {"cell_type": "code", "execution_count": 10, "id": "1027d5c6", "metadata": {}, "outputs": [], "source": ["prediction3 = 'the adversary abusing Sybil accounts imposes a critical threat to establishing trust and integrity in web services'\n", "reference3 = 'establishing trust in web services is threatened by the adversary'\n", "bleu_score = bleu.compute(predictions=[prediction3], references=[reference3])\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "6e575af7-86df-4c14-b87b-044b2faec668", "metadata": {}, "source": ["### Multiple sentence score"]}, {"cell_type": "code", "execution_count": 11, "id": "e2ea7b7a-f649-4b65-aae5-0c128bc087b6", "metadata": {}, "outputs": [], "source": ["bleu_score = bleu.compute(predictions=[prediction1, prediction2, prediction3], \n", "                          references=[reference1, reference2, reference3])\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "6109f18a-8acd-42f2-9abc-15463736581d", "metadata": {}, "source": ["### Incremental adding predictions"]}, {"cell_type": "code", "execution_count": 12, "id": "109b6386-2f04-495f-8651-7c2b2966c311", "metadata": {}, "outputs": [], "source": ["bleu.add(predictions=prediction1, references=reference1)\n", "bleu.add(predictions=prediction2, references=reference2)\n", "bleu.add(predictions=prediction3, references=reference3)\n", "bleu_score = bleu.compute()\n", "bleu_score['bleu']"]}, {"cell_type": "markdown", "id": "ee4281fb", "metadata": {}, "source": ["## 1.3 ROUGE\n", "\n", "ROUGE, or Recall-Oriented Understudy for Gisting Evaluation, is a set of metrics and a software package used for evaluating automatic summarization and machine translation software in natural language processing. \n", "\n", "The metrics compare an automatically produced summary or translation against a reference or a set of references (human-produced) summary or translation. Rouge-1 considers 1-gram, Rouge-2 2-gram and so on. Rouge-L considers the longest common subsequence.\n", "\n", "Note that ROUGE is case insensitive, meaning that upper case letters are treated the same way as lower case letters.\n", "\n", "The metric ranges from 0 to 1 and a higher score is better.\n"]}, {"cell_type": "code", "execution_count": 13, "id": "601bfbd1-f536-43df-872d-07471ed64bc1", "metadata": {}, "outputs": [], "source": ["rouge = evaluate.load(\"rouge\")"]}, {"cell_type": "code", "execution_count": 14, "id": "7e5b2dda", "metadata": {}, "outputs": [], "source": ["prediction1 = 'the cat is on the yoga mat'\n", "reference1 = 'the cat sat on the yoga mat'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction1], references=[reference1])\n", "rouge_scores"]}, {"cell_type": "code", "execution_count": 15, "id": "abe9028c", "metadata": {}, "outputs": [], "source": ["prediction2 = 'the value of life lies in what you create for others not in what you possess'\n", "reference2 = 'the meaning of life lies in what you give to others not in what you receive'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction2], references=[reference2])\n", "rouge_scores"]}, {"cell_type": "code", "execution_count": 16, "id": "6193791d", "metadata": {}, "outputs": [], "source": ["prediction3 = 'the adversary abusing Sybil accounts imposes a critical threat to establishing trust and integrity in web services'\n", "reference3 = 'establishing trust in web services is threatened by the adversary'\n", "\n", "rouge_scores = rouge.compute(predictions=[prediction3], references=[reference3])\n", "rouge_scores"]}, {"cell_type": "markdown", "id": "8324be0b-8e1b-4aec-87be-9dec5e071c59", "metadata": {}, "source": ["Rouge also supports multiple sentence score and incremental computing. You could try below if interested."]}, {"cell_type": "markdown", "id": "b6673456-2cd7-4562-ad40-f02bea867afd", "metadata": {}, "source": ["## 1.4 Perplexity\n", "\n", "Perplexity measures the uncertainty of a language model's predictions. \n", "\n", "Given a model and an input text sequence, perplexity measures how likely the model is to generate the input text sequence.  \n", "\n", "- Lower perplexity is better - it means the model is more confident and accurate in its predictions\n", "- A perplexity of 1 would be perfect (but unrealistic), meaning the model perfectly predicts every token\n", "- The higher the perplexity, the more \"surprised\" or \"confused\" the model is by the text\n", "\n", "The range of this metric is [0, inf). A lower score is better."]}, {"cell_type": "code", "execution_count": 40, "id": "b4f25a53-3709-423f-9dc5-3385477ebf03", "metadata": {}, "outputs": [], "source": ["perplexity = evaluate.load(\"perplexity\", module_type=\"metric\")"]}, {"cell_type": "code", "execution_count": 47, "id": "c76fb396-ac1e-4f10-8024-de2ddabace7d", "metadata": {}, "outputs": [], "source": ["input_texts = [\"Perplexity measures the uncertainty of a language model's predictions.\",\n", "               \"Higher the perplexity, the more surprised or confused the model is by the text.\", \n", "               \"X8nP7qLz3 RtYvA5cE2 mD9fGh JkUbW s6i\"]\n", "results = perplexity.compute(model_id='gpt2',\n", "                             add_start_token=False,\n", "                             predictions=input_texts) \n", "results"]}, {"cell_type": "markdown", "id": "375aaac6-df65-4be2-b773-fc0758cca44d", "metadata": {}, "source": ["## 1.5 Combining multiple metrics"]}, {"cell_type": "code", "execution_count": 48, "id": "0dfcce8f-1949-4ea8-bcbd-fefe28263655", "metadata": {}, "outputs": [], "source": ["metrics = evaluate.combine(['bleu', 'rouge'])\n", "scores = metrics.compute(predictions=[prediction1, prediction2, prediction3], \n", "                          references=[reference1, reference2, reference3])\n", "scores"]}, {"cell_type": "markdown", "id": "94ebdc82", "metadata": {}, "source": ["# 2 Evaluatings Models over REST API"]}, {"cell_type": "markdown", "id": "e2a3ad9c", "metadata": {}, "source": ["## 2.1 Some utility functions"]}, {"cell_type": "code", "execution_count": 51, "id": "61a0f44a", "metadata": {}, "outputs": [], "source": ["from dotenv import load_dotenv\n", "import os\n", "load_dotenv()\n", "openai_api_key = os.environ.get(\"INFINI_API_KEY\")\n", "openai_base_url = os.environ.get(\"INFINI_BASE_URL\")\n"]}, {"cell_type": "code", "execution_count": 67, "id": "a0acc338", "metadata": {}, "outputs": [], "source": ["import time\n", "import random\n", "from openai import RateLimitError\n", "from openai import OpenAI\n", "\n", "client = OpenAI(\n", "    api_key=openai_api_key,\n", "    base_url=openai_base_url,\n", ")\n", "\n", "# a single thread version of get_llm_output\n", "def get_llm_output(model_name, question_content, question_id, output, semaphore=None):\n", "    # the last semaphore is used for parallel execution only\n", "    try:\n", "        retries = 5\n", "        for attempt in range(retries):\n", "            try:\n", "                if isinstance(question_content, str):\n", "                    messages = [{\"role\": \"user\", \"content\": question_content}]\n", "                else:\n", "                    messages = [{\"role\": \"user\", \"content\": q} for q in question_content]\n", "\n", "                chat_response = client.chat.completions.create(\n", "                    model=model_name,\n", "                    messages=messages,\n", "                    max_tokens=2048,\n", "                    temperature=0,\n", "                    seed=42\n", "                )\n", "\n", "                llm_answer = chat_response.choices[0].message.content.strip()\n", "                output[question_id] = llm_answer\n", "                break  # \u6210\u529f\u5c31\u9000\u51fa retry \u5faa\u73af\n", "\n", "            except RateLimitError as e:\n", "                wait_time = random.uniform(1, 3) * (2 ** attempt)\n", "                print(f\"RateLimitError on Q{question_id}, retry {attempt+1}/{retries}, wait {wait_time:.1f}s\")\n", "                time.sleep(wait_time)\n", "\n", "            except Exception as e:\n", "                print(f\"Error on Q{question_id}: {e}\")\n", "                break  # \u5176\u4ed6\u5f02\u5e38\u4e0d\u91cd\u8bd5\n", "\n", "        else:\n", "            # \u6240\u6709\u5c1d\u8bd5\u5931\u8d25\uff0c\u586b\u7a7a\u9632\u6b62\u540e\u7eed\u5d29\n", "            output[question_id] = \"\"\n", "\n", "    finally:\n", "        if semaphore:\n", "            semaphore.release() # release the semaphore\n"]}, {"cell_type": "markdown", "id": "21c11c19", "metadata": {}, "source": ["Evaluation requires running lots of Q/A's.  How to run them fast enough is the key.  Revisit my favourite topic: thread synchronization."]}, {"cell_type": "code", "execution_count": 55, "id": "4d519267-d72c-4dfb-b0f6-ba4f52608dc1", "metadata": {}, "outputs": [], "source": ["# extending the single thread version to parallel execution\n", "def get_llm_output_parallel(model_name, question_contents, max_threads=5):\n", "    # Create threads for each question\n", "    output = {}\n", "    threads = []\n", "    semaphore = threading.Semaphore(max_threads)\n", "    for question_id, question_content in tqdm(enumerate(question_contents)):\n", "        semaphore.acquire() \n", "        thread = threading.Thread(target=get_llm_output, args=(model_name, question_content, question_id, output, semaphore))\n", "        threads.append(thread)\n", "        thread.start()\n", "        # semaphore is released when the thread ends, in the single thread version\n", "\n", "    # Wait for all threads to complete\n", "    for thread in threads:\n", "        thread.join()\n", "\n", "    sorted_keys = sorted(output.keys())\n", "    sorted_outputs = [output[key] for key in sorted_keys]        \n", "    return sorted_outputs"]}, {"cell_type": "code", "execution_count": 56, "id": "c333379c-d4cb-4fdd-94ef-209bcddffa97", "metadata": {}, "outputs": [], "source": ["def print_llm_outputs(model_name, question_contents, llm_answers, references):\n", "    for i, (question, answer, reference) in enumerate(zip(question_contents, llm_answers, references)):\n", "        print('Question %d: %s'%(i, question))\n", "        print('Answer from Model %s: %s'%(model_name, answer))\n", "        print('Reference Answer: %s\\n'%(reference))"]}, {"cell_type": "markdown", "id": "27002a74-55ee-48df-9850-845265ac95c1", "metadata": {}, "source": ["## 2.2 Example: Evaluate News Summarization Results"]}, {"cell_type": "markdown", "id": "54000bb7-0726-4c8c-b5de-2f5c074ff4d9", "metadata": {}, "source": ["### Load the data and preprocess\n", "\n", "The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. \n", "\n", "For each instance, there is a string for the article, a string for the highlights, and a string for the id.\n", "\n", "Data source: https://huggingface.co/datasets/cnn_dailymail"]}, {"cell_type": "code", "execution_count": 57, "id": "60884967-ad63-49d1-9a03-8df35dac5c21", "metadata": {}, "outputs": [], "source": ["from datasets import load_dataset, load_from_disk\n", "\n", "# d=load_dataset(r\"ccdv/cnn_dailymail\", '3.0.0')  # reading from huggingface\n", "# d.save_to_disk('/share/data/cnn_dailymail/')  # saving to disk for later use\n", "d = load_from_disk('/ssdshare/share/data/cnn_dailymail/')  # loading from disk"]}, {"cell_type": "code", "execution_count": 58, "id": "bb1b9395-6e17-447f-9cf8-ef54c65a5b65", "metadata": {}, "outputs": [], "source": ["def create_prompt(x):\n", "    s = \"Please summarize the following news article in no more than 30 words.\\n %s\" %(x['article'])\n", "    x['question_content'] = s\n", "    return x\n", "\n", "d = d['test'].map(create_prompt) # Use test set to evaluate\n", "d"]}, {"cell_type": "markdown", "id": "deffa166-871b-4e14-b931-e82840318fab", "metadata": {}, "source": ["### Evaluate the models\n", "It is a summarization task, so we can use both BLEU and ROUGE as evaluation metrics."]}, {"cell_type": "code", "execution_count": 59, "id": "69feb89f-93e6-4746-9c5d-020793d09ab4", "metadata": {"scrolled": true}, "outputs": [], "source": ["EVALUATE_N= 10  # To save time, we evaluate the first 10 articles only.\n", "metrics = evaluate.combine(['bleu', 'rouge'])  # metrics to evaluate\n", "overall_scores = {}\n", "\n", "for model_name in ['qwen2.5-72b-instruct', 'llama-3.3-70b-instruct',]:\n", "    print(f'============== {model_name}  ==============')\n", "    question_contents = d['question_content'][:EVALUATE_N]\n", "    references = d['highlights'][:EVALUATE_N]\n", "    llm_answers = get_llm_output_parallel(model_name, question_contents, max_threads=5)\n", "    # print(\"Predictions:\", llm_answers)\n", "    # print(\"References:\", references)\n", "    # print(\"Lengths:\", len(llm_answers), len(references))\n", "    scores = metrics.compute(predictions=llm_answers, references=references)\n", "    overall_scores[model_name] = [scores['bleu'], scores['rouge1'], scores['rouge2'], scores['rougeL']]    \n", "    print_llm_outputs(model_name, question_contents, llm_answers, references)"]}, {"cell_type": "code", "execution_count": 60, "id": "8f1cb464-eb88-423f-80e0-6eb8d4f88f9b", "metadata": {}, "outputs": [], "source": ["# nice print the results using pandas\n", "import pandas as pd\n", "performance_df = pd.DataFrame(overall_scores)\n", "performance_df.index = ['BLEU', 'ROUGE-1', 'ROUGE-2', 'ROUGE-L']\n", "performance_df"]}, {"cell_type": "code", "execution_count": 61, "id": "4d89b7aa-b410-41c2-814c-c51c56e77e4f", "metadata": {}, "outputs": [], "source": ["performance_df.plot.bar()\n", "plt.ylabel('Evaluation metric score')\n", "plt.title('LLM performance in news summarization')\n", "plt.show()"]}, {"cell_type": "markdown", "id": "ef9ff7cf", "metadata": {}, "source": ["## 2.3 Multiple choice question answering"]}, {"cell_type": "markdown", "id": "074d8a57-06db-493b-9377-5979bc2865e9", "metadata": {}, "source": ["### Load the data from huggingface\n", "C-Eval is a comprehensive Chinese evaluation suite for foundation models. It consists of 13948 multi-choice questions spanning 52 diverse disciplines and four difficulty levels. Here we use two disciplines: art studies and operating system.\n", "\n", "Data source: https://cevalbenchmark.com/index_zh.html"]}, {"cell_type": "code", "execution_count": 63, "id": "8103106e", "metadata": {}, "outputs": [], "source": ["def get_question_content(x):\n", "    s = \"\u4ee5\u4e0b\u662f\u5355\u9879\u9009\u62e9\u9898,\u8bf7\u76f4\u63a5\u7ed9\u51fa\u5176\u4e2d\u7684\u6b63\u786e\u7b54\u6848\u3002\u8bf7\u53ea\u8f93\u51faABCD\u5f53\u4e2d\u7684\u4e00\u4e2a,\u4e0d\u9700\u8981\u4f5c\u89e3\u91ca\u3002\\n%s\\nA. %s\\nB. %s\\nC. %s\\nD. %s\" %(x['question'],x['A'], x['B'], x['C'], x['D'])\n", "    x['question_content'] = s\n", "    return x"]}, {"cell_type": "code", "execution_count": 64, "id": "488f1ece-ca2b-4e0b-9f3c-5559dbcbec4f", "metadata": {}, "outputs": [], "source": ["data_names = ['art_studies', 'operating_system']\n", "ds = []\n", "\n", "# Please try this code if you want to load the data from huggingface (sometimes it will be slow or even failed)\n", "# for data_name in data_names:\n", "#     d=load_dataset(r\"ceval/ceval-exam\", name=data_name)\n", "#     d_updated = d['val'].map(get_question_content) # Use validation set to evaluate\n", "#     print(data_name)\n", "#     print(d_updated)\n", "#     ds.append(d_updated)\n", "\n", "\n", "for data_name in data_names:\n", "    file_path = f\"/ssdshare/share/data/ceval-exam/val/{data_name}_val.csv\"\n", "    d = load_dataset(\"csv\", data_files={ \"val\": file_path })[\"val\"]\n", "    d_updated = d.map(get_question_content)\n", "    print(data_name)\n", "    print(d_updated)\n", "    ds.append(d_updated)\n"]}, {"cell_type": "markdown", "id": "da2d6db1-f419-4212-affd-4f33f69f92ff", "metadata": {}, "source": ["### Evaluate the models\n", "Accuracy is used to evaluate the model."]}, {"cell_type": "code", "execution_count": 65, "id": "250b569d-1598-4d07-b0d6-d1a1b70e3a39", "metadata": {}, "outputs": [], "source": ["def get_options(llm_answers):\n", "    # Select the option that occurs most times in the model output as the final answer.\n", "    options = []\n", "    for llm_answer in llm_answers:\n", "        option_frequencies = [llm_answer.count(option) for option in 'ABCD']\n", "        most_frequent = np.argmax(option_frequencies)\n", "        most_frequent_option = 'ABCD'[most_frequent]\n", "        options.append(most_frequent_option)\n", "    return options\n", "\n", "def option2num(options):\n", "    # Transform the ABCD options to numbers for accuracy evaluation.\n", "    option2num_dict = {'A':0 ,'B':1, 'C':2, 'D':3}\n", "    nums = list(map(lambda x:option2num_dict[x], options))\n", "    return nums"]}, {"cell_type": "code", "execution_count": 66, "id": "a3f65cb3-1220-41a6-bf6c-e1a3916cf145", "metadata": {"scrolled": true}, "outputs": [], "source": ["overall_scores = {} # Evaluation results for all models\n", "\n", "for model_name in ['qwen2.5-72b-instruct', 'llama-3.3-70b-instruct',]:\n", "    scores = []\n", "    print(f'============== {model_name}  ==============')\n", "    for i, d in enumerate(ds):\n", "        print('Data %s has %d questions'%(data_names[i], d.num_rows))\n", "        question_contents = d['question_content']\n", "        llm_answers = get_llm_output_parallel(model_name, question_contents, max_threads=5)\n", "        print_llm_outputs(model_name, question_contents, llm_answers, d['answer'])        \n", "        llm_answers = get_options(llm_answers)\n", "        acc = accuracy.compute(references=option2num(d['answer']), predictions=option2num(llm_answers)) \n", "        scores.append(acc['accuracy'])\n", "    overall_scores[model_name] = scores"]}, {"cell_type": "code", "execution_count": 35, "id": "57278fe2", "metadata": {}, "outputs": [], "source": ["accuracy_df =  pd.DataFrame(overall_scores)\n", "accuracy_df.index = data_names\n", "accuracy_df"]}, {"cell_type": "code", "execution_count": 36, "id": "86077a04", "metadata": {}, "outputs": [], "source": ["accuracy_df.plot.barh()\n", "plt.xlabel('Accuracy')\n", "plt.title('LLM performance in C-Eval benchmark')\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "id": "3802a023", "metadata": {}, "outputs": [], "source": ["#### Your Task ####\n", "# Find another dataset to evaluate two other models of your choice.\n", "# You should choose two models that have different sizes and different performance.\n", "# For this notebook, use questions with easy to judge answers only (e.g. multiple choice questions), \n", "# or the answer is a simple yes/no question, or a single word answer.\n", "# Report at least two metrics to evaluate the models.\n"]}, {"cell_type": "code", "execution_count": 1, "id": "26af3cb7", "metadata": {}, "outputs": [], "source": ["# do not forget to clean the gpu memory\n", "import torch\n", "torch.cuda.empty_cache()"]}, {"cell_type": "code", "execution_count": 2, "id": "c81cafc2", "metadata": {}, "outputs": [], "source": ["# check the GPU memory utilization\n", "!nvidia-smi"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.12"}}, "nbformat": 4, "nbformat_minor": 5}
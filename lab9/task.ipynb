{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance before training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 A case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = '/ssdshare/qzh/Qwen/Qwen2.5-7B-Instruct-1M'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The model may require a specific input format, adjust as necessary\n",
    "prompt = \"\"\"<|im_start|>system\\nYou are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve a logical reasoning problem. After thinking, when you finally reach a conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is a knight\\n(2) ... </answer>.\\n<|im_end|>\\n<|im_start|>user\\nA very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 3 inhabitants: Michael, Evelyn, and Victoria. Michael told you that If Victoria is a knave then Evelyn is a knight. \"Michael is a knight,\" Evelyn mentioned. Victoria said that Michael is a knave or Evelyn is a knave. So who is a knight and who is a knave?\\n<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "# The gold answer is: (1) Michael is a knight (2) Evelyn is a knight (3) Victoria is a knave\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=1000,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.split('assistant\\n')[1]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Performance after training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 A case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'The path to your trained model'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: The model may require a specific input format, adjust as necessary\n",
    "prompt = \"\"\"<|im_start|>system\\nYou are a helpful assistant. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and<answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>. Now the user asks you to solve a logical reasoning problem. After thinking, when you finally reach a conclusion, clearly state the identity of each character within <answer> </answer> tags. i.e., <answer> (1) Zoey is a knight\\n(2) ... </answer>.\\n<|im_end|>\\n<|im_start|>user\\nA very special island is inhabited only by knights and knaves. Knights always tell the truth, and knaves always lie. You meet 3 inhabitants: Michael, Evelyn, and Victoria. Michael told you that If Victoria is a knave then Evelyn is a knight. \"Michael is a knight,\" Evelyn mentioned. Victoria said that Michael is a knave or Evelyn is a knave. So who is a knight and who is a knave?\\n<|im_end|>\\n<|im_start|>assistant\\n\"\"\"\n",
    "# The gold answer is: (1) Michael is a knight (2) Evelyn is a knight (3) Victoria is a knave\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "# Generate text\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=1000,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "# Decode the generated text\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.split('assistant\\n')[1]\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Key results record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# Record the following metrics (post screenshots in cell):\n",
    "# 1. the change of the response length during training\n",
    "# 2. the change in accuracy of the model on the training dataset during training\n",
    "# 3. the change in accuracy of the model on the test dataset during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### YOUR TASK ####\n",
    "# With the saved log file, complete the following tasks:\n",
    "# 1. Select a model response, which shows at least one of the following behavioral characteristics:\n",
    "#   - Multipath exploration (Les't test both possibilities)\n",
    "#   - Analysis of previous conclusions (Analyze .. statement again)\n",
    "#   - Phase summary (Let's summarize, Now we have determined)\n",
    "#   - Verify the conclusion before answering (Let's verify all statements)\n",
    "#   - Mix language\n",
    "# 2. Pick three responses in the training process with different response lengths and analyse what different behaviors the model exhibits in these responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
